"Celery – a python task queue for long jobs – AndyPiSubscribeServicesWeb Design and HostingPython app developmentPro PortfolioCountly Two Factor Authentication PluginAutomating Global HTTPS Deployment using Fly’s API and AnsibleVideo Analytics for Cloudflare Stream WordPress PluginGithub PageCategoriesRaspberry PiArduinoCloudDisplays & LightsWeb AppsPythonPython: beginner to pro in 5 stepsPHPJavaScriptRandomContactAbout中文SubscribeServicesWeb Design and HostingPython app developmentPro PortfolioCountly Two Factor Authentication PluginAutomating Global HTTPS Deployment using Fly’s API and AnsibleVideo Analytics for Cloudflare Stream WordPress PluginGithub PageCategoriesRaspberry PiArduinoCloudDisplays & LightsWeb AppsPythonPython: beginner to pro in 5 stepsPHPJavaScriptRandomContactAbout中文Celery – a python task queue for long jobsRunning python tasks in the backgroundIn the past few months I’ve built a couple of web apps using python and flask, and the main task of the programs takes significant time to run. The web app for my client requires a customer to enter their email address and click a button, and the app generates some data files and sends them to a customer. It will take 30 seconds or so to do it, but in my flask app that means the customer clicks the button and has to wait that whole time for the app to process before loading the next page – which is far too long. In other cases, it’s not a user entering the details but a webhook passing some data to the web application – which requires a response to be given within 5 seconds. So we need to either load the next page immediately, telling the customer they will get an email shortly, or immediately respond to the webhook, and then run the actual task in the background.In python you can do this with threading, where each ‘thread’ is a piece of code that can run at the same time, i.e. asynchronously. (See The Raspberry Pi Education Manual page 108 for a demo). However, there is more powerful way to do this using a task queue. This brings the added bonus of being able to run the task queue and customer facing web app on separate servers (so the task queue can chug away doing its thing, without affecting the performance serving the website to the end customer. There are a few options to do this but it seems like the most popular python package is Celery. Like all good python packages, the name tells you nothing about what it does, but basically Celery lets you call a standard python function, but it will be added to a task queue to run in the background, thus enabling us to return the web page request quickly. You’ll also need to run a task queue such as Redis. it’s a little confusing at first but at the end, you’ll actually have 3 programs running, all of which could be on the same, or different servers:1. The Flask web application, which runs the Celery client allowing you to add a background task to the task queue.2. The task queue itself, such as Redis. This does not do anything except hold the details of the task to be executed.3. The Celery Worker, which is continuously grabbing tasks from the task queue, and actually executing them.Basic ExampleThis is a one file basic example of using celery with flask.Installation:You’ll need install redis and the flask and celery python packages (pip install flask celery).Python Code: (app.py)# Import required modulesfrom flask import Flask, requestfrom celery import Celery# Create the Flask instancesapp = Flask(__name__)# Create the Celery instance, referring to the task queue (or broker) as rediscelery = Celery(app.name, broker='redis://localhost:6379/0')# This is the celery task that will be run by the worker in the background# We need to give it the celery decorator to denote this@celery.taskdef my_background_task(arg1, arg2):    # some long running task here (this simple example has no output)    result = arg1*arg2    # Create a flask route - this is a simpl get request@app.route('/', methods=['GET'])def index():    if request.method == 'GET':        # add the background task to the task queue,        # arguments for the task: arg1=10, arg2=20        # optionally countdown specifies a 60 second delay        task = my_background_task.apply_async(args=[10, 20], countdown=60)    # Flask returns this message to the browser    return {'task started'}Run the three programs:$ redis-server$ celery worker -A app.celery --loglevel=info$ python app.pyThat’s it. You can now point your browser to wherever you’ve set the flask app up, and it will return the ‘task started’ message to you, and 60 seconds later, the celery worker will complete the task.Miguel Grinberg has a more detailed tutorial on Celery showing how you can get updates on the progress of your background task, which is well worth a read.andyAuthor archive15th February 2016Python, Web Appspython, web appsPrevious Post Next PostComments are closed.Recent PostsCloudflare Stream Video Analytics WordPress PluginWordPress Plugin repos with SVNGet your eBay sales statistics summary using Python & PandasRemote Weather MonitoringPython for Mechanical Engineers – Rail Brake distance calculationsEmail*Categories3D Printing (2)Arduino (5)Cloud (10)Cryptocurreny (2)Displays & Lights (14)JavaScript (3)PHP (4)Portfolio (3)Products (9)Python (38)Python for Mechanical Engineers (2)Python: beginner to pro in 5 steps (5)Random (4)Raspberry Pi (26)Web Apps (10)Follow AndyPiCategories3D PrintingArduinoCloudCryptocurrenyDisplays & LightsJavaScriptPHPPortfolioProductsPythonPython for Mechanical EngineersPython: beginner to pro in 5 stepsRandomRaspberry PiWeb AppsFollow AndyPiPrivacy PolicyThis site uses cookies and by using the site you are consenting to this. Find out why we use cookies and how to opt out by reading our Privacy Policy.© 2021 AndyPi — Powered by WordPressTheme by Anders Noren — Up ↑"
