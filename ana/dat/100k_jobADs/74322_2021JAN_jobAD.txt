"FAIR jobs scheduling in Apache Spark on waitingforcode.com - articles about Apache SparkHomeAboutTips#TagsData engneeringApache AirflowBig Data algorithmsBig Data problems - solutionsData engineering patternsGeneral Big DataGeneral data engineeringGraphsSQLData processingApache BeamApache SparkApache Spark GraphFramesApache Spark GraphXApache Spark SQLApache Spark StreamingApache Spark Structured StreamingPySparkStorageApache AvroApache CassandraApache ParquetApache ZooKeeperElasticsearchEmbedded databasesHDFSMySQLPostgreSQLTime seriesMessagingApache KafkaApache PulsarRabbitMQCloudData engineering on AWSData engineering on GCPData engineering on the cloudJVMJavaScalaSoftware engineeringProgrammingTestingWeb securityFAIR jobs scheduling in Apache SparkApril 4, 2019 • Apache Spark • Bartosz KoniecznyHome Apache Spark FAIR jobs scheduling in Apache SparkVersions: Apache Spark 2.4.0During my exploration of Apache Spark configuration options, I found an entry called spark.scheduler.mode. After looking for its possible values, I ended up with a pretty intriguing concept called FAIR scheduling that I will detail in this post.The post has 3 sections. The first one introduces the default scheduler mode in Apache Spark called FIFO. The second section focuses on the FAIR scheduler whereas the last part compares both of them through 2 simple test cases.FIFO scheduling in Apache SparkIn Apache Spark, a job is the unit of work represented by the transformation(s) ending by an action. By default, the framework allocates the resources in FIFO manner. This means that the first defined job will get the priority for all available resources. If this first job doesn't need all resources, that's fine because other jobs can use them too. But if it's not the case, the remaining jobs must wait until the first job frees them. It can be problematic especially when the first job is a long-running one and the remaining execute much faster. The following image shows the problem:As you can see, despite the fact of submitting the jobs from 2 different threads, the first triggered job starts and reserves all resources. To mitigate that issue, Apache Spark proposes a scheduling mode called FAIR.FAIR scheduling in Apache SparkFAIR scheduling mode works in round-robin manner, like in the following schema:As you can see, the engine schedules tasks of different jobs. That's why the job number 2 doesn't need to wait the long job number 1 to terminate and it can start as soon as possible. Thus, the final goal of the FAIR scheduling mode is to share the resources between different jobs and thanks to that, not penalize the execution time of the shorter ones.FAIR scheduling method brings also the concept of pools. The pool is a concept used to group different jobs inside the same logical unit. Each pool can have different properties, like weight which is a kind of importance notion, minShare to define the minimum reserved capacity and schedulingMode to say whether the jobs within given pool are scheduled in FIFO or FAIR manner.Hence, pools are a great way to separate the resources between different clients. If one of the executed jobs is more important than the others, you can increase its weight and minimum capacity in order to guarantee its quick termination.Both concepts, FAIR mode and pools, are configurable. The scheduling method is set in spark.scheduler.mode option whereas the pools are defined with sparkContext.setLocalProperty(\"spark.scheduler.pool\", poolName) method inside the thread invoking given job.FIFO and FAIR scheduling comparisonTo see FAIR scheduling mode in action we have different choices. In the local mode, the easiest one though is to see the order of scheduled and executed tasks in the logs. The 2 following tests prove that in FIFO mode, the jobs are scheduled one after another whereas in FAIR mode, the tasks of different jobs are mixed:\"FIFO scheduling mode\" should \"run the tasks of one job in sequential manner\" in {
