[0m2021.02.25 11:12:56 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.25 11:12:56 INFO  time: initialize in 0.43s[0m
[0m2021.02.25 11:12:56 WARN  Build server is not auto-connectable.[0m
[0m2021.02.25 11:13:00 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8125804751256519568/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:13:01 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/metals.sbt[0m
[0m2021.02.25 11:13:01 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/project/metals.sbt[0m
[0m2021.02.25 11:13:01 INFO  skipping build import with status 'Started'[0m
[0m2021.02.25 11:13:02 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:13:03 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.25 11:13:07 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
// DO NOT EDIT! This file is auto-generated.
// This file enables sbt-bloop to create bloop config files.

addSbtPlugin("ch.epfl.scala" % "sbt-bloop" % "1.4.8")

[0m2021.02.25 11:13:08 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:13:10 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:13:10 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:13:12 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:13:12 INFO  [info] compiling 1 Scala source to /home/skyler/project3/s3data/s3dataget/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 11:13:15 INFO  [info] done compiling[0m
[0m2021.02.25 11:13:15 INFO  [success] Total time: 5 s, completed Feb 25, 2021 11:13:15 AM[0m
[0m2021.02.25 11:13:17 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:13:17 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:13:17 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:13:17 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:13:17 INFO  [success] Total time: 0 s, completed Feb 25, 2021 11:13:18 AM[0m
[0m2021.02.25 11:13:19 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:13:19 INFO  time: ran 'sbt bloopInstall' in 18s[0m
[0m2021.02.25 11:13:19 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6728976256959835049/bsp.socket'...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.13.4.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/net/java/dev/jna/jna/5.3.1/jna-5.3.1.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/jline/jline/3.16.0/jline-3.16.0.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.13.4/scala-compiler-2.13.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.13.4/scala-library-2.13.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.13.4/scala-reflect-2.13.4.jar
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root'
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6728976256959835049/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6728976256959835049/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 11:13:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:13:40 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4924750887088379548/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4924750887088379548/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4924750887088379548/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 11:13:40 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:13:40 INFO  time: Connected to build server in 21s[0m
[0m2021.02.25 11:13:40 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:13:41 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:13:41 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:13:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:13:44 INFO  time: indexed workspace in 3.6s[0m
[0m2021.02.25 11:13:44 INFO  compiling root (1 scala source)[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.02.25 11:14:09 INFO  time: compiled root in 25s[0m
[0m2021.02.25 11:16:17 INFO  compiling s3dataget-build (1 scala source)[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.13.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.4"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 11:16:21 INFO  time: code lens generation in 1.6s[0m
[0m2021.02.25 11:16:21 INFO  time: code lens generation in 3.74s[0m
[0m2021.02.25 11:16:21 INFO  time: code lens generation in 2.67s[0m
[0m2021.02.25 11:16:21 INFO  time: code lens generation in 1.31s[0m
[0m2021.02.25 11:16:21 INFO  time: compiled s3dataget-build in 4.13s[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.example"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 11:16:22 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals6848039691854785842/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:16:23 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:16:23 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:16:25 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.reva"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 11:16:26 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:16:26 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "example"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 11:16:30 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:16:30 INFO  [success] Total time: 4 s, completed Feb 25, 2021 11:16:31 AM[0m
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test
  )

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.25 11:16:34 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:16:34 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:16:34 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:16:34 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:16:34 INFO  [success] Total time: 0 s, completed Feb 25, 2021 11:16:35 AM[0m
[0m2021.02.25 11:16:35 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:16:35 INFO  time: ran 'sbt bloopInstall' in 13s[0m
[0m2021.02.25 11:16:35 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:16:35 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:16:35 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.25 11:16:35 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3765731819890637835/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.25 11:16:35 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:16:35 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:16:36 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:16:36 INFO  time: Connected to build server in 0.23s[0m
[0m2021.02.25 11:16:36 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:16:36 INFO  no build target: using presentation compiler with only scala-library: 2.13.4[0m
[0m2021.02.25 11:16:37 INFO  no build target: using presentation compiler with only scala-library: 2.13.4[0m
[0m2021.02.25 11:16:37 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
[0m2021.02.25 11:16:37 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:16:37 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:16:37 INFO  time: indexed workspace in 2.35s[0m
[0m2021.02.25 11:16:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:16:42 INFO  time: compiled root in 3.27s[0m
[0m2021.02.25 11:16:42 INFO  skipping build import with status 'Installed'[0m
[0m2021.02.25 11:17:02 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4050057607226931925/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:17:03 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:17:03 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:17:05 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:17:06 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:17:06 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:17:09 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:17:09 INFO  [success] Total time: 3 s, completed Feb 25, 2021 11:17:09 AM[0m
[0m2021.02.25 11:17:13 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:17:13 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:17:17 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:17:17 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:17:17 INFO  [success] Total time: 4 s, completed Feb 25, 2021 11:17:17 AM[0m
[0m2021.02.25 11:17:17 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:17:18 INFO  time: ran 'sbt bloopInstall' in 16s[0m
[0m2021.02.25 11:17:18 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:17:18 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:17:18 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:17:18 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:17:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:17:18 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:17:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:17:18 INFO  time: Connected to build server in 0.18s[0m
[0m2021.02.25 11:17:18 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:17:19 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:17:19 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:17:21 INFO  time: indexed workspace in 2.63s[0m
[0m2021.02.25 11:17:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:17:21 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 11:17:22 INFO  skipping build import with status 'Installed'[0m
[0m2021.02.25 11:22:18 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals9032963300691067958/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:22:19 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:22:21 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:22:22 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:22:23 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:22:23 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:22:27 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:22:27 INFO  [success] Total time: 4 s, completed Feb 25, 2021 11:22:28 AM[0m
[0m2021.02.25 11:22:34 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:22:34 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:22:42 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:22:44 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:22:44 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:22:44 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:22:44 INFO  [success] Total time: 10 s, completed Feb 25, 2021 11:22:44 AM[0m
[0m2021.02.25 11:22:45 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:22:45 INFO  time: ran 'sbt bloopInstall' in 27s[0m
[0m2021.02.25 11:22:45 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:22:45 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:22:45 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:22:45 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:22:45 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:22:45 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:22:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:22:46 INFO  time: Connected to build server in 0.43s[0m
[0m2021.02.25 11:22:46 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:22:48 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:22:48 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:22:48 INFO  time: indexed workspace in 2.02s[0m
[0m2021.02.25 11:23:16 INFO  skipping build import with status 'Installed'[0m
[0m2021.02.25 11:23:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:23:16 INFO  time: compiled root in 0.5s[0m
[0m2021.02.25 11:23:22 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals3116513021097443026/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:23:23 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:23:25 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:23:25 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:23:27 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:23:27 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:23:29 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:23:29 INFO  [success] Total time: 3 s, completed Feb 25, 2021 11:23:30 AM[0m
[0m2021.02.25 11:23:34 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:23:34 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:24:11 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:24:24 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:24:24 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:24:24 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:24:24 INFO  [success] Total time: 50 s, completed Feb 25, 2021 11:24:24 AM[0m
[0m2021.02.25 11:24:24 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:24:25 INFO  time: ran 'sbt bloopInstall' in 1m2s[0m
[0m2021.02.25 11:24:25 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:24:25 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:24:25 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:24:25 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:24:25 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:24:25 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:24:25 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:24:25 INFO  time: Connected to build server in 0.24s[0m
[0m2021.02.25 11:24:25 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:24:27 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:24:27 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:24:27 INFO  time: indexed workspace in 1.83s[0m
[0m2021.02.25 11:24:39 INFO  skipping build import with status 'Installed'[0m
[0m2021.02.25 11:24:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:24:39 INFO  time: compiled root in 0.4s[0m
[0m2021.02.25 11:28:03 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8263477662535087846/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:28:04 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:28:04 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:28:06 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:28:06 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:28:07 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:28:10 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:28:11 INFO  [success] Total time: 4 s, completed Feb 25, 2021 11:28:11 AM[0m
[0m2021.02.25 11:28:14 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 11:28:14 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 11:28:16 INFO  skipping build import with status 'Started'[0m
[0m2021.02.25 11:28:20 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals7868150058467966958/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:28:21 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:28:23 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:28:24 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:28:26 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:28:26 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:28:29 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:28:29 INFO  [success] Total time: 3 s, completed Feb 25, 2021 11:28:30 AM[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assemblyMergeStrategy[0m
[0m2021.02.25 11:28:31 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:28:31 ERROR ^[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assembly[0m
[0m2021.02.25 11:28:31 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:28:31 ERROR                          ^[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value PathList[0m
[0m2021.02.25 11:28:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:28:31 ERROR        ^[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: star patterns must correspond with varargs parameters[0m
[0m2021.02.25 11:28:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:28:31 ERROR                                   ^[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:28:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:28:31 ERROR                                         ^[0m
[0m2021.02.25 11:28:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:20: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:28:31 ERROR   case x => MergeStrategy.first[0m
[0m2021.02.25 11:28:31 ERROR             ^[0m
[0m2021.02.25 11:28:31 INFO  [error] Type error in expression[0m
[0m2021.02.25 11:28:31 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.02.25 11:28:31 INFO  sbt bloopInstall exit: 1[0m
[0m2021.02.25 11:28:32 INFO  time: ran 'sbt bloopInstall' in 11s[0m
[0m2021.02.25 11:28:32 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:28:32 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals7868150058467966958/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 11:28:32 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:28:32 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:28:32 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:28:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:28:32 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:28:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:28:32 INFO  time: Connected to build server in 0.15s[0m
[0m2021.02.25 11:28:32 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:28:33 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:28:33 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:28:33 INFO  time: indexed workspace in 1.31s[0m
[0m2021.02.25 11:28:47 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:29:05 INFO  [warn] There may be incompatibilities among your library dependencies; run 'evicted' to see detailed eviction warnings.[0m
[0m2021.02.25 11:29:05 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 11:29:05 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 11:29:05 INFO  [success] Total time: 51 s, completed Feb 25, 2021 11:29:06 AM[0m
[0m2021.02.25 11:29:05 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 11:29:06 INFO  time: ran 'sbt bloopInstall' in 1m3s[0m
[0m2021.02.25 11:29:06 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:29:06 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:29:06 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:29:06 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:29:06 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:29:06 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:29:06 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:29:06 INFO  time: Connected to build server in 0.17s[0m
[0m2021.02.25 11:29:06 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:29:08 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:29:08 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:29:08 INFO  time: indexed workspace in 2.04s[0m
[0m2021.02.25 11:29:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:29:11 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 11:29:20 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4842458625260321921/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:29:21 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:29:23 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:29:25 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:29:27 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:29:27 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:29:30 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:29:30 INFO  [success] Total time: 3 s, completed Feb 25, 2021 11:29:31 AM[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assemblyMergeStrategy[0m
[0m2021.02.25 11:29:31 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:29:31 ERROR ^[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assembly[0m
[0m2021.02.25 11:29:31 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:29:31 ERROR                          ^[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value PathList[0m
[0m2021.02.25 11:29:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:29:31 ERROR        ^[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: star patterns must correspond with varargs parameters[0m
[0m2021.02.25 11:29:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:29:31 ERROR                                   ^[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:29:31 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:29:31 ERROR                                         ^[0m
[0m2021.02.25 11:29:31 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:20: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:29:31 ERROR   case x => MergeStrategy.first[0m
[0m2021.02.25 11:29:31 ERROR             ^[0m
[0m2021.02.25 11:29:31 INFO  [error] Type error in expression[0m
[0m2021.02.25 11:29:31 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.02.25 11:29:31 INFO  sbt bloopInstall exit: 1[0m
[0m2021.02.25 11:29:32 INFO  time: ran 'sbt bloopInstall' in 12s[0m
[0m2021.02.25 11:29:32 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals4842458625260321921/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 11:29:32 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:29:32 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:29:32 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:29:32 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:29:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:29:32 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:29:32 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:29:32 INFO  time: Connected to build server in 0.18s[0m
[0m2021.02.25 11:29:32 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:29:34 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:29:34 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:29:34 INFO  time: indexed workspace in 1.38s[0m
[0m2021.02.25 11:29:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:29:34 INFO  time: compiled root in 0.52s[0m
[0m2021.02.25 11:30:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:30:50 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 11:30:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:30:53 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 11:31:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:31:07 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 11:31:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:31:12 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 11:31:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:31:14 INFO  time: compiled root in 0.63s[0m
[0m2021.02.25 11:31:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:31:20 INFO  time: compiled root in 0.54s[0m
[0m2021.02.25 11:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:12 INFO  time: compiled root in 0.54s[0m
[0m2021.02.25 11:37:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:25 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 11:37:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:31 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 11:37:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:32 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:37:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:44 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:37:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:51 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 11:37:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:37:57 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:38:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:24 INFO  time: compiled root in 0.63s[0m
[0m2021.02.25 11:38:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:29 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 11:38:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:34 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 11:38:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:36 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 11:38:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:40 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 11:38:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:51 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 11:38:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:38:54 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:39:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:39:02 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 11:40:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:40:12 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 11:45:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:45:09 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 11:45:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:45:51 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:46:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:46:00 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 11:46:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:46:36 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:46:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:46:40 INFO  time: compiled root in 1s[0m
[0m2021.02.25 11:46:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:46:44 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 11:46:50 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals303425066083298973/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 11:46:51 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 11:46:52 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 11:46:53 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 11:46:54 INFO  [info] loading settings for project s3dataget-build from metals.sbt ...[0m
[0m2021.02.25 11:46:54 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 11:46:57 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 11:46:57 INFO  [success] Total time: 3 s, completed Feb 25, 2021 11:46:58 AM[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assemblyMergeStrategy[0m
[0m2021.02.25 11:46:58 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:46:58 ERROR ^[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:18: error: not found: value assembly[0m
[0m2021.02.25 11:46:58 ERROR assemblyMergeStrategy in assembly := {[0m
[0m2021.02.25 11:46:58 ERROR                          ^[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value PathList[0m
[0m2021.02.25 11:46:58 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:46:58 ERROR        ^[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: star patterns must correspond with varargs parameters[0m
[0m2021.02.25 11:46:58 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:46:58 ERROR                                   ^[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:19: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:46:58 ERROR   case PathList("META-INF", xs @ _*) => MergeStrategy.discard[0m
[0m2021.02.25 11:46:58 ERROR                                         ^[0m
[0m2021.02.25 11:46:58 ERROR /home/skyler/project3/s3data/s3dataget/build.sbt:20: error: not found: value MergeStrategy[0m
[0m2021.02.25 11:46:58 ERROR   case x => MergeStrategy.first[0m
[0m2021.02.25 11:46:58 ERROR             ^[0m
[0m2021.02.25 11:46:58 INFO  [error] Type error in expression[0m
[0m2021.02.25 11:46:58 INFO  [warn] Project loading failed: (r)etry, (q)uit, (l)ast, or (i)gnore? (default: r)[0m
[0m2021.02.25 11:46:58 INFO  sbt bloopInstall exit: 1[0m
[0m2021.02.25 11:46:59 INFO  time: ran 'sbt bloopInstall' in 9.49s[0m
[0m2021.02.25 11:46:59 ERROR sbt command failed: /usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals303425066083298973/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall[0m
[0m2021.02.25 11:46:59 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 11:46:59 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:46:59 INFO  Shut down connection with build server.[0m
[0m2021.02.25 11:46:59 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:46:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:46:59 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 11:46:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 11:46:59 INFO  time: Connected to build server in 0.15s[0m
[0m2021.02.25 11:46:59 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 11:47:00 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 11:47:00 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 11:47:00 INFO  time: indexed workspace in 1.27s[0m
[0m2021.02.25 11:47:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:22 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 11:47:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:38 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 11:47:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:43 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 11:47:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:46 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 11:47:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:49 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 11:47:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:47:57 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 11:48:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:02 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:48:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:06 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 11:48:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:13 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 11:48:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:28 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:38 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 11:48:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:42 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:48:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:48:46 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:49:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:13 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 11:49:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:19 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:49:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:22 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 11:49:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:23 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:49:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:28 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 11:49:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:33 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 11:49:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:36 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 11:49:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:42 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 11:49:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:51 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 11:49:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:53 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:49:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:49:59 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:50:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:07 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 11:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:14 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 11:50:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:19 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 11:50:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:21 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:50:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:28 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 11:50:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:29 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 11:50:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:32 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 11:50:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:35 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:50:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:40 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:50:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:50:44 INFO  time: compiled root in 0.61s[0m
[0m2021.02.25 11:51:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:04 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 11:51:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:08 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:51:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:11 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 11:51:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:17 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 11:51:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:27 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 11:51:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:30 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 11:51:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 11:51:32 INFO  time: compiled root in 0.58s[0m
[0m2021.02.25 12:00:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:00:58 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 12:01:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:02 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 12:01:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:06 INFO  time: compiled root in 0.59s[0m
[0m2021.02.25 12:01:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:08 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 12:01:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:15 INFO  time: compiled root in 0.64s[0m
[0m2021.02.25 12:01:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:41 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 12:01:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:46 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 12:01:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:49 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 12:01:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 12:01:57 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 13:22:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:22:53 INFO  time: compiled root in 3.22s[0m
[0m2021.02.25 13:23:44 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:23:44 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:23:44 INFO  skipping build import with status 'Failed'[0m
[0m2021.02.25 13:23:45 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
[0m2021.02.25 13:23:46 INFO  time: code lens generation in 1.81s[0m
[0m2021.02.25 13:23:49 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:23:54 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:23:55 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:01 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:03 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:05 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:06 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:09 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/project/plugins.sbt[0m
[0m2021.02.25 13:24:18 INFO  running '/usr/lib/jvm/java-8-openjdk-amd64/bin/java -Djline.terminal=jline.UnsupportedTerminal -Dsbt.log.noformat=true -Dfile.encoding=UTF-8 -jar /tmp/metals8099668084859611708/sbt-launch.jar -Dbloop.export-jar-classifiers=sources bloopInstall'[0m
[0m2021.02.25 13:24:20 INFO  [info] welcome to sbt 1.4.7 (Private Build Java 1.8.0_282)[0m
[0m2021.02.25 13:24:24 INFO  [info] loading settings for project s3dataget-build-build-build from metals.sbt ...[0m
[0m2021.02.25 13:24:25 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project/project[0m
[0m2021.02.25 13:24:28 INFO  [info] loading settings for project s3dataget-build-build from metals.sbt ...[0m
[0m2021.02.25 13:24:28 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project/project[0m
[0m2021.02.25 13:24:32 INFO  [success] Generated .bloop/s3dataget-build-build.json[0m
[0m2021.02.25 13:24:32 INFO  [success] Total time: 3 s, completed Feb 25, 2021 1:24:32 PM[0m
[0m2021.02.25 13:24:33 INFO  [info] loading settings for project s3dataget-build from metals.sbt,plugins.sbt ...[0m
[0m2021.02.25 13:24:33 INFO  [info] loading project definition from /home/skyler/project3/s3data/s3dataget/project[0m
[0m2021.02.25 13:24:35 INFO  [success] Generated .bloop/s3dataget-build.json[0m
[0m2021.02.25 13:24:35 INFO  [info] compiling 1 Scala source to /home/skyler/project3/s3data/s3dataget/project/target/scala-2.12/sbt-1.0/classes ...[0m
[0m2021.02.25 13:24:39 INFO  [info] done compiling[0m
[0m2021.02.25 13:24:39 INFO  [success] Total time: 6 s, completed Feb 25, 2021 1:24:40 PM[0m
[0m2021.02.25 13:24:43 INFO  [info] loading settings for project root from build.sbt ...[0m
[0m2021.02.25 13:24:43 INFO  [info] set current project to s3dataget (in build file:/home/skyler/project3/s3data/s3dataget/)[0m
[0m2021.02.25 13:24:45 INFO  [success] Generated .bloop/root.json[0m
[0m2021.02.25 13:24:45 INFO  [success] Generated .bloop/root-test.json[0m
[0m2021.02.25 13:24:45 INFO  [success] Total time: 2 s, completed Feb 25, 2021 1:24:45 PM[0m
[0m2021.02.25 13:24:45 INFO  sbt bloopInstall exit: 0[0m
[0m2021.02.25 13:24:46 INFO  time: ran 'sbt bloopInstall' in 27s[0m
[0m2021.02.25 13:24:46 INFO  Disconnecting from Bloop session...[0m
[0m2021.02.25 13:24:46 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:24:46 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:24:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:24:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:24:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:24:46 INFO  Attempting to connect to the build server...[0m
[0m2021.02.25 13:24:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:24:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:24:46 INFO  time: Connected to build server in 0.37s[0m
[0m2021.02.25 13:24:46 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:24:46 INFO  time: Imported build in 0.13s[0m
[0m2021.02.25 13:24:50 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 13:24:50 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:24:50 INFO  time: indexed workspace in 4.26s[0m
[0m2021.02.25 13:26:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:13 INFO  time: compiled root in 1.84s[0m
[0m2021.02.25 13:26:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:14 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 13:26:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:44 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 13:26:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:52 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 13:26:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:56 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 13:26:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:26:59 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 13:27:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:27:01 INFO  time: compiled root in 0.69s[0m
[0m2021.02.25 13:28:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:28:15 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 13:28:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:28:20 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 13:28:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:28:22 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 13:32:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:32:08 INFO  time: compiled root in 0.84s[0m
[0m2021.02.25 13:33:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:33:15 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 13:33:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:33:19 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 13:33:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:33:35 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 13:34:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:34:31 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 13:34:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:34:35 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 13:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:34:51 INFO  time: compiled root in 4.65s[0m
[0m2021.02.25 13:34:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:34:56 INFO  time: compiled root in 1.17s[0m
[0m2021.02.25 13:35:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:35:47 INFO  time: compiled root in 1.2s[0m
[0m2021.02.25 13:36:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:36:41 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 13:36:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:36:43 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 13:37:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:37:03 INFO  time: compiled root in 0.88s[0m
[0m2021.02.25 13:37:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:37:23 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 13:38:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:38:11 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 13:38:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:38:14 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 13:43:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:43:21 INFO  time: compiled root in 1s[0m
[0m2021.02.25 13:45:53 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.25 13:45:54 INFO  time: initialize in 0.86s[0m
[0m2021.02.25 13:45:54 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4486923298793404473/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.25 13:45:54 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.25 13:45:54 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4486923298793404473/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4486923298793404473/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:45:55 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:45:55 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2434642421721323199/bsp.socket'...
[0mWaiting for the bsp connection to come up...
2021.02.25 13:45:55 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8229434468283979206/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2434642421721323199/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2434642421721323199/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:45:55 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8229434468283979206/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8229434468283979206/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.25 13:45:55 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.25 13:45:55 INFO  time: Connected to build server in 0.85s[0m
[0m2021.02.25 13:45:55 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.25 13:45:58 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession

object GetS3Data {
  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder()
        .appName("Get S3 Data")
        .config("spark.master", "local")
        .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")

    val rdd = spark.sparkContext.textFile("s3a://bigdatasamyers/CC-MAIN-20210128134124-20210128164124-00799.warc.wat")
    rdd.foreach(println)

    spark.close()
  }
}

[0m2021.02.25 13:46:01 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession

object GetS3Data {
  def main(args: Array[String]): Unit = {

    val spark = SparkSession.builder()
        .appName("Get S3 Data")
        .config("spark.master", "local")
        .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.endpoint", "s3.amazonaws.com")

    val rdd = spark.sparkContext.textFile("s3a://bigdatasamyers/CC-MAIN-20210128134124-20210128164124-00799.warc.wat")
    rdd.foreach(println)

    spark.close()
  }
}

[0m2021.02.25 13:46:01 INFO  time: code lens generation in 6.94s[0m
[0m2021.02.25 13:46:04 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.25 13:46:04 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.25 13:46:05 INFO  time: indexed workspace in 7.06s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.02.25 13:47:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:14: stale bloop error: Invalid literal number
    rdd.take(100foreach(println)
             ^[0m
[0m2021.02.25 13:47:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:14: stale bloop error: Invalid literal number
    rdd.take(100foreach(println)
             ^[0m
[0m2021.02.25 13:47:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:27:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 13:47:33 INFO  time: compiled root in 0.37s[0m
[0m2021.02.25 13:47:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:14: stale bloop error: Invalid literal number
    rdd.take(100foreach(println)
             ^[0m
[0m2021.02.25 13:47:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:27:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 13:47:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:36 INFO  time: compiled root in 1.4s[0m
[0m2021.02.25 13:47:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:40 INFO  time: compiled root in 1.21s[0m
[0m2021.02.25 13:47:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:40 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 13:47:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:42 INFO  time: compiled root in 0.28s[0m
[0m2021.02.25 13:47:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:47:44 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 13:50:45 INFO  shutting down Metals[0m
[0m2021.02.25 13:50:45 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:50:45 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:50:45 INFO  Shut down connection with build server.[0m
[0m2021.02.25 13:53:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:53:09 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 13:55:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:55:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 13:55:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 13:55:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:28:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 13:55:59 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 13:56:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 13:56:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:28:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 13:56:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24:1: stale bloop error: unclosed string literal
")
^[0m
[0m2021.02.25 13:56:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:28:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 13:56:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:56:07 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 13:56:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:56:12 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 13:57:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:57:19 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 13:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:27 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 13:58:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:30 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 13:58:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:34 INFO  time: compiled root in 0.36s[0m
[0m2021.02.25 13:58:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 13:58:37 INFO  time: compiled root in 1.23s[0m
[0m2021.02.25 14:00:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:00:34 INFO  time: compiled root in 1.07s[0m
[0m2021.02.25 14:00:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:00:36 INFO  time: compiled root in 1.24s[0m
[0m2021.02.25 14:03:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:13 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 14:03:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:15 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 14:03:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:19 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 14:03:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:23 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 14:03:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:25 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:03:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:03:35 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:06:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:06:57 INFO  time: compiled root in 0.3s[0m
[0m2021.02.25 14:07:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:05 INFO  time: compiled root in 0.44s[0m
[0m2021.02.25 14:07:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:11 INFO  time: compiled root in 0.44s[0m
[0m2021.02.25 14:07:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:18 INFO  time: compiled root in 0.48s[0m
[0m2021.02.25 14:07:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:20 INFO  time: compiled root in 0.34s[0m
[0m2021.02.25 14:07:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:32 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 14:07:32 INFO  time: compiled root in 1.61s[0m
[0m2021.02.25 14:07:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:07:36 INFO  time: compiled root in 1.51s[0m
[0m2021.02.25 14:08:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:08:53 INFO  time: compiled root in 1.42s[0m
[0m2021.02.25 14:08:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:08:56 INFO  time: compiled root in 0.29s[0m
[0m2021.02.25 14:08:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:00 INFO  time: compiled root in 1.53s[0m
[0m2021.02.25 14:09:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:03 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 14:09:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:07 INFO  time: compiled root in 1.6s[0m
[0m2021.02.25 14:09:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:09:10 INFO  time: compiled root in 1.63s[0m
[0m2021.02.25 14:11:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:04 INFO  time: compiled root in 2.17s[0m
[0m2021.02.25 14:11:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:20 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 14:11:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:26 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:11:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:32 INFO  time: compiled root in 0.4s[0m
[0m2021.02.25 14:11:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:39 INFO  time: compiled root in 1.27s[0m
[0m2021.02.25 14:11:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:42 INFO  time: compiled root in 1.4s[0m
[0m2021.02.25 14:11:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:11:47 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:12:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:12:02 INFO  time: compiled root in 0.48s[0m
[0m2021.02.25 14:12:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:12:03 INFO  time: compiled root in 0.46s[0m
[0m2021.02.25 14:12:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:12:03 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 14:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:23 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:14:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:31 INFO  time: compiled root in 0.13s[0m
Feb 25, 2021 2:14:31 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.25 14:14:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:34 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:14:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:37 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:14:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:14:59 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:16:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:16:38 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 14:16:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:16:43 INFO  time: compiled root in 1.27s[0m
[0m2021.02.25 14:17:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:17:04 INFO  time: compiled root in 1.5s[0m
[0m2021.02.25 14:17:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:17:08 INFO  time: compiled root in 0.3s[0m
[0m2021.02.25 14:17:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:17:37 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 14:17:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:17:51 INFO  time: compiled root in 1.05s[0m
[0m2021.02.25 14:17:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:17:59 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 14:18:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:18:01 INFO  time: compiled root in 0.41s[0m
[0m2021.02.25 14:18:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:18:17 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 14:18:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:18:26 INFO  time: compiled root in 1.3s[0m
[0m2021.02.25 14:18:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:18:34 INFO  time: compiled root in 1.75s[0m
[0m2021.02.25 14:22:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:26 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 14:22:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:28 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 14:22:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:39 INFO  time: compiled root in 0.31s[0m
[0m2021.02.25 14:22:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:41 INFO  time: compiled root in 0.32s[0m
[0m2021.02.25 14:22:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:22:46 INFO  time: compiled root in 0.35s[0m
[0m2021.02.25 14:23:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:23 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 14:23:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:27 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 14:23:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:29 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 14:23:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:34 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 14:23:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:39 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 14:23:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:50 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 14:23:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:52 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 14:23:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:54 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 14:23:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:23:59 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 14:24:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:24:18 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 14:43:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:10 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:43:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:19 INFO  time: compiled root in 0.29s[0m
[0m2021.02.25 14:43:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:28 INFO  time: compiled root in 0.3s[0m
[0m2021.02.25 14:43:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:47 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 14:43:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:49 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 14:43:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:54 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:43:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:43:59 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 14:44:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:44:01 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 14:44:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:44:04 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 14:44:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:44:08 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 14:44:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:44:14 INFO  time: compiled root in 3.53s[0m
[0m2021.02.25 14:46:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:46:19 INFO  time: compiled root in 1.52s[0m
[0m2021.02.25 14:53:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:53:25 INFO  time: compiled root in 0.31s[0m
[0m2021.02.25 14:53:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:53:29 INFO  time: compiled root in 1.06s[0m
[0m2021.02.25 14:58:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:32 INFO  time: compiled root in 1.76s[0m
Feb 25, 2021 2:58:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1697
Feb 25, 2021 2:58:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1694
[0m2021.02.25 14:58:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:36 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 14:58:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:49 INFO  time: compiled root in 0.43s[0m
[0m2021.02.25 14:58:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:51 INFO  time: compiled root in 0.32s[0m
[0m2021.02.25 14:58:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 14:58:53 INFO  time: compiled root in 1.28s[0m
[0m2021.02.25 15:00:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:00:58 INFO  time: compiled root in 2.35s[0m
[0m2021.02.25 15:03:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:24 INFO  time: compiled root in 2.14s[0m
[0m2021.02.25 15:03:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:40 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 15:03:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:45 INFO  time: compiled root in 1.44s[0m
[0m2021.02.25 15:03:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:03:47 INFO  time: compiled root in 1.37s[0m
[0m2021.02.25 15:07:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:07:59 INFO  time: compiled root in 1.07s[0m
[0m2021.02.25 15:08:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:08:04 INFO  time: compiled root in 1.51s[0m
[0m2021.02.25 15:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:08:56 INFO  time: compiled root in 1.52s[0m
[0m2021.02.25 15:10:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:10:17 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 15:11:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:11:27 INFO  time: compiled root in 1.14s[0m
[0m2021.02.25 15:14:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:14:12 INFO  time: compiled root in 1.09s[0m
[0m2021.02.25 15:14:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:14:19 INFO  time: compiled root in 1.59s[0m
[0m2021.02.25 15:17:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:17:46 INFO  time: compiled root in 0.97s[0m
[0m2021.02.25 15:23:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:18 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 15:23:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:21 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 15:23:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:26 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 15:23:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:36 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 15:23:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:38 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 15:23:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:42 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 15:23:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:55 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 15:23:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:23:59 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 15:24:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:24:00 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 15:24:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:24:08 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 15:26:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:40 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 15:26:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:43 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 15:26:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:26:46 INFO  time: compiled root in 1.11s[0m
[0m2021.02.25 15:39:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:39:52 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 15:39:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:39:59 INFO  time: compiled root in 1s[0m
[0m2021.02.25 15:40:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:40:01 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 15:40:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:40:03 INFO  time: compiled root in 0.91s[0m
[0m2021.02.25 15:40:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:40:07 INFO  time: compiled root in 1.02s[0m
[0m2021.02.25 15:40:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:40:10 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 15:40:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:40:14 INFO  time: compiled root in 1.41s[0m
[0m2021.02.25 15:41:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:41:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:41:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:41:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:41:44 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 15:41:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:41:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:41:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:41:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:41:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:41:48 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 15:42:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:42:49 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 15:42:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:42:53 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 15:42:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:42:56 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 15:43:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:43:00 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 15:43:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:43:02 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 15:43:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:43:05 INFO  time: compiled root in 0.57s[0m
[0m2021.02.25 15:43:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:43:07 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 15:43:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:43:24 INFO  time: compiled root in 1.03s[0m
[0m2021.02.25 15:48:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:48:43 INFO  time: compiled root in 2.33s[0m
[0m2021.02.25 15:50:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:12 INFO  time: compiled root in 2.33s[0m
[0m2021.02.25 15:50:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:15 INFO  time: compiled root in 2.44s[0m
[0m2021.02.25 15:50:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:16 INFO  time: compiled root in 0.36s[0m
[0m2021.02.25 15:50:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:20 INFO  time: compiled root in 1.82s[0m
[0m2021.02.25 15:50:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:21 INFO  time: compiled root in 1.35s[0m
[0m2021.02.25 15:50:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:26 INFO  time: compiled root in 0.36s[0m
[0m2021.02.25 15:50:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:30 INFO  time: compiled root in 1.33s[0m
[0m2021.02.25 15:50:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:34 INFO  time: compiled root in 1.9s[0m
[0m2021.02.25 15:50:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:50:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:50:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:50:35 INFO  time: compiled root in 0.39s[0m
[0m2021.02.25 15:50:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:50:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:50:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:93: stale bloop error: unclosed string literal
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                                                                            ^[0m
[0m2021.02.25 15:50:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains(a href") && line.contains("/job") && line.contains("/jobs"))
                                    ^[0m
[0m2021.02.25 15:50:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:38 INFO  time: compiled root in 1.2s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package collection

import generic._
import scala.annotation.tailrec

/** A template trait for indexed sequences of type `IndexedSeq[A]` which optimizes
 *  the implementation of several methods under the assumption of fast random access.
 *
 *  $indexedSeqInfo
 *
 *  @define willNotTerminateInf
 *  @define mayNotTerminateInf
 */
trait IndexedSeqOptimized[+A, +Repr] extends Any with IndexedSeqLike[A, Repr] { self =>

  override /*IterableLike*/
  def isEmpty: Boolean = { length == 0 }

  override /*IterableLike*/
  def foreach[U](f: A => U): Unit = {
    var i = 0
    val len = length
    while (i < len) { f(this(i)); i += 1 }
  }

  private def prefixLengthImpl(p: A => Boolean, expectTrue: Boolean): Int = {
    var i = 0
    while (i < length && p(apply(i)) == expectTrue) i += 1
    i
  }

  override /*IterableLike*/
  def forall(p: A => Boolean): Boolean = prefixLengthImpl(p, expectTrue = true) == length

  override /*IterableLike*/
  def exists(p: A => Boolean): Boolean = prefixLengthImpl(p, expectTrue = false) != length

  override /*IterableLike*/
  def find(p: A => Boolean): Option[A] = {
    val i = prefixLength(!p(_))
    if (i < length) Some(this(i)) else None
  }

  @tailrec
  private def foldl[B](start: Int, end: Int, z: B, op: (B, A) => B): B =
    if (start == end) z
    else foldl(start + 1, end, op(z, this(start)), op)

  @tailrec
  private def foldr[B](start: Int, end: Int, z: B, op: (A, B) => B): B =
    if (start == end) z
    else foldr(start, end - 1, op(this(end - 1), z), op)

  override /*TraversableLike*/
  def foldLeft[B](z: B)(op: (B, A) => B): B =
    foldl(0, length, z, op)

  override /*IterableLike*/
  def foldRight[B](z: B)(op: (A, B) => B): B =
    foldr(0, length, z, op)

  override /*TraversableLike*/
  def reduceLeft[B >: A](op: (B, A) => B): B =
    if (length > 0) foldl(1, length, this(0), op) else super.reduceLeft(op)

  override /*IterableLike*/
  def reduceRight[B >: A](op: (A, B) => B): B =
    if (length > 0) foldr(0, length - 1, this(length - 1), op) else super.reduceRight(op)

  override /*IterableLike*/
  def zip[A1 >: A, B, That](that: GenIterable[B])(implicit bf: CanBuildFrom[Repr, (A1, B), That]): That = that match {
    case that: IndexedSeq[_] =>
      val b = bf(repr)
      var i = 0
      val len = this.length min that.length
      b.sizeHint(len)
      while (i < len) {
        b += ((this(i), that(i).asInstanceOf[B]))
        i += 1
      }
      b.result()
    case _ =>
      super.zip[A1, B, That](that)(bf)
  }

  override /*IterableLike*/
  def zipWithIndex[A1 >: A, That](implicit bf: CanBuildFrom[Repr, (A1, Int), That]): That = {
    val b = bf(repr)
    val len = length
    b.sizeHint(len)
    var i = 0
    while (i < len) {
      b += ((this(i), i))
      i += 1
    }
    b.result()
  }

  override /*IterableLike*/
  def slice(from: Int, until: Int): Repr = {
    val lo    = math.max(from, 0)
    val hi    = math.min(math.max(until, 0), length)
    val elems = math.max(hi - lo, 0)
    val b     = newBuilder
    b.sizeHint(elems)

    var i = lo
    while (i < hi) {
      b += self(i)
      i += 1
    }
    b.result()
  }

  override /*IterableLike*/
  def head: A = if (isEmpty) super.head else this(0)

  override /*TraversableLike*/
  def tail: Repr = if (isEmpty) super.tail else slice(1, length)

  override /*TraversableLike*/
  def last: A = if (length > 0) this(length - 1) else super.last

  override /*IterableLike*/
  def init: Repr = if (length > 0) slice(0, length - 1) else super.init

  override /*TraversableLike*/
  def take(n: Int): Repr = slice(0, n)

  override /*TraversableLike*/
  def drop(n: Int): Repr = slice(n, length)

  override /*IterableLike*/
  def takeRight(n: Int): Repr = slice(length - math.max(n, 0), length)

  override /*IterableLike*/
  def dropRight(n: Int): Repr = slice(0, length - math.max(n, 0))

  override /*TraversableLike*/
  def splitAt(n: Int): (Repr, Repr) = (take(n), drop(n))

  override /*IterableLike*/
  def takeWhile(p: A => Boolean): Repr = take(prefixLength(p))

  override /*TraversableLike*/
  def dropWhile(p: A => Boolean): Repr = drop(prefixLength(p))

  override /*TraversableLike*/
  def span(p: A => Boolean): (Repr, Repr) = splitAt(prefixLength(p))

  override /*IterableLike*/
  def sameElements[B >: A](that: GenIterable[B]): Boolean = that match {
    case that: IndexedSeq[_] =>
      val len = length
      len == that.length && {
        var i = 0
        while (i < len && this(i) == that(i)) i += 1
        i == len
      }
    case _ =>
      super.sameElements(that)
  }

  override /*IterableLike*/
  def copyToArray[B >: A](xs: Array[B], start: Int, len: Int) {
    var i = 0
    var j = start
    val end = length min len min (xs.length - start)
    while (i < end) {
      xs(j) = this(i)
      i += 1
      j += 1
    }
  }

  // Overridden methods from Seq

  override /*SeqLike*/
  def lengthCompare(len: Int): Int = length - len

  override /*SeqLike*/
  def segmentLength(p: A => Boolean, from: Int): Int = {
    val len = length
    var i = from
    while (i < len && p(this(i))) i += 1
    i - from
  }

  private def negLength(n: Int) = if (n >= length) -1 else n

  override /*SeqLike*/
  def indexWhere(p: A => Boolean, from: Int): Int = {
    val start = math.max(from, 0)
    negLength(start + segmentLength(!p(_), start))
  }

  override /*SeqLike*/
  def lastIndexWhere(p: A => Boolean, end: Int): Int = {
    var i = math.min(end, length - 1)
    while (i >= 0 && !p(this(i))) i -= 1
    i
  }

  override /*SeqLike*/
  def reverse: Repr = {
    val b = newBuilder
    b.sizeHint(length)
    var i = length
    while (0 < i) {
      i -= 1
      b += this(i)
    }
    b.result()
  }

  override /*SeqLike*/
  def reverseIterator: Iterator[A] = new AbstractIterator[A] {
    private var i = self.length
    def hasNext: Boolean = 0 < i
    def next(): A =
      if (0 < i) {
        i -= 1
        self(i)
      } else Iterator.empty.next()
  }

  override /*SeqLike*/
  def startsWith[B](that: GenSeq[B], offset: Int): Boolean = that match {
    case that: IndexedSeq[_] =>
      var i = offset
      var j = 0
      val thisLen = length
      val thatLen = that.length
      while (i < thisLen && j < thatLen && this(i) == that(j)) {
        i += 1
        j += 1
      }
      j == thatLen
    case _ =>
      var i = offset
      val thisLen = length
      val thatElems = that.iterator
      while (i < thisLen && thatElems.hasNext) {
        if (this(i) != thatElems.next())
          return false

        i += 1
      }
      !thatElems.hasNext
  }

  override /*SeqLike*/
  def endsWith[B](that: GenSeq[B]): Boolean = that match {
    case that: IndexedSeq[_] =>
      var i = length - 1
      var j = that.length - 1

      (j <= i) && {
        while (j >= 0) {
          if (this(i) != that(j))
            return false
          i -= 1
          j -= 1
        }
        true
      }
    case _ =>
      super.endsWith(that)
  }

  override def toList: List[A] = {
    var i = length - 1
    var result: List[A] = Nil
    while (i >= 0) {
      result ::= apply(i)
      i -= 1
    }
    result
  }
}


[0m2021.02.25 15:50:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:50:50 INFO  time: compiled root in 2.6s[0m
[0m2021.02.25 15:51:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:51:58 INFO  time: compiled root in 1.28s[0m
[0m2021.02.25 15:52:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:52:51 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 15:52:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:52:56 INFO  time: compiled root in 1s[0m
[0m2021.02.25 15:53:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:53:08 INFO  time: compiled root in 0.94s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.rdd

import java.util.Random

import scala.collection.{mutable, Map}
import scala.collection.mutable.ArrayBuffer
import scala.io.Codec
import scala.language.implicitConversions
import scala.reflect.{classTag, ClassTag}
import scala.util.hashing

import com.clearspring.analytics.stream.cardinality.HyperLogLogPlus
import org.apache.hadoop.io.{BytesWritable, NullWritable, Text}
import org.apache.hadoop.io.compress.CompressionCodec
import org.apache.hadoop.mapred.TextOutputFormat

import org.apache.spark._
import org.apache.spark.Partitioner._
import org.apache.spark.annotation.{DeveloperApi, Experimental, Since}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.partial.BoundedDouble
import org.apache.spark.partial.CountEvaluator
import org.apache.spark.partial.GroupedCountEvaluator
import org.apache.spark.partial.PartialResult
import org.apache.spark.storage.{RDDBlockId, StorageLevel}
import org.apache.spark.util.{BoundedPriorityQueue, Utils}
import org.apache.spark.util.collection.{OpenHashMap, Utils => collectionUtils}
import org.apache.spark.util.random.{BernoulliCellSampler, BernoulliSampler, PoissonSampler,
  SamplingUtils}

/**
 * A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,
 * partitioned collection of elements that can be operated on in parallel. This class contains the
 * basic operations available on all RDDs, such as `map`, `filter`, and `persist`. In addition,
 * [[org.apache.spark.rdd.PairRDDFunctions]] contains operations available only on RDDs of key-value
 * pairs, such as `groupByKey` and `join`;
 * [[org.apache.spark.rdd.DoubleRDDFunctions]] contains operations available only on RDDs of
 * Doubles; and
 * [[org.apache.spark.rdd.SequenceFileRDDFunctions]] contains operations available on RDDs that
 * can be saved as SequenceFiles.
 * All operations are automatically available on any RDD of the right type (e.g. RDD[(Int, Int)])
 * through implicit.
 *
 * Internally, each RDD is characterized by five main properties:
 *
 *  - A list of partitions
 *  - A function for computing each split
 *  - A list of dependencies on other RDDs
 *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
 *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
 *    an HDFS file)
 *
 * All of the scheduling and execution in Spark is done based on these methods, allowing each RDD
 * to implement its own way of computing itself. Indeed, users can implement custom RDDs (e.g. for
 * reading data from a new storage system) by overriding these functions. Please refer to the
 * <a href="http://people.csail.mit.edu/matei/papers/2012/nsdi_spark.pdf">Spark paper</a>
 * for more details on RDD internals.
 */
abstract class RDD[T: ClassTag](
    @transient private var _sc: SparkContext,
    @transient private var deps: Seq[Dependency[_]]
  ) extends Serializable with Logging {

  if (classOf[RDD[_]].isAssignableFrom(elementClassTag.runtimeClass)) {
    // This is a warning instead of an exception in order to avoid breaking user programs that
    // might have defined nested RDDs without running jobs with them.
    logWarning("Spark does not support nested RDDs (see SPARK-5063)")
  }

  private def sc: SparkContext = {
    if (_sc == null) {
      throw new SparkException(
        "This RDD lacks a SparkContext. It could happen in the following cases: \n(1) RDD " +
        "transformations and actions are NOT invoked by the driver, but inside of other " +
        "transformations; for example, rdd1.map(x => rdd2.values.count() * x) is invalid " +
        "because the values transformation and count action cannot be performed inside of the " +
        "rdd1.map transformation. For more information, see SPARK-5063.\n(2) When a Spark " +
        "Streaming job recovers from checkpoint, this exception will be hit if a reference to " +
        "an RDD not defined by the streaming job is used in DStream operations. For more " +
        "information, See SPARK-13758.")
    }
    _sc
  }

  /** Construct an RDD with just a one-to-one dependency on one parent */
  def this(@transient oneParent: RDD[_]) =
    this(oneParent.context, List(new OneToOneDependency(oneParent)))

  private[spark] def conf = sc.conf
  // =======================================================================
  // Methods that should be implemented by subclasses of RDD
  // =======================================================================

  /**
   * :: DeveloperApi ::
   * Implemented by subclasses to compute a given partition.
   */
  @DeveloperApi
  def compute(split: Partition, context: TaskContext): Iterator[T]

  /**
   * Implemented by subclasses to return the set of partitions in this RDD. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   *
   * The partitions in this array must satisfy the following property:
   *   `rdd.partitions.zipWithIndex.forall { case (partition, index) => partition.index == index }`
   */
  protected def getPartitions: Array[Partition]

  /**
   * Implemented by subclasses to return how this RDD depends on parent RDDs. This method will only
   * be called once, so it is safe to implement a time-consuming computation in it.
   */
  protected def getDependencies: Seq[Dependency[_]] = deps

  /**
   * Optionally overridden by subclasses to specify placement preferences.
   */
  protected def getPreferredLocations(split: Partition): Seq[String] = Nil

  /** Optionally overridden by subclasses to specify how they are partitioned. */
  @transient val partitioner: Option[Partitioner] = None

  // =======================================================================
  // Methods and fields available on all RDDs
  // =======================================================================

  /** The SparkContext that created this RDD. */
  def sparkContext: SparkContext = sc

  /** A unique ID for this RDD (within its SparkContext). */
  val id: Int = sc.newRddId()

  /** A friendly name for this RDD */
  @transient var name: String = _

  /** Assign a name to this RDD */
  def setName(_name: String): this.type = {
    name = _name
    this
  }

  /**
   * Mark this RDD for persisting using the specified level.
   *
   * @param newLevel the target storage level
   * @param allowOverride whether to override any existing level with the new one
   */
  private def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = {
    // TODO: Handle changes of StorageLevel
    if (storageLevel != StorageLevel.NONE && newLevel != storageLevel && !allowOverride) {
      throw new UnsupportedOperationException(
        "Cannot change storage level of an RDD after it was already assigned a level")
    }
    // If this is the first time this RDD is marked for persisting, register it
    // with the SparkContext for cleanups and accounting. Do this only once.
    if (storageLevel == StorageLevel.NONE) {
      sc.cleaner.foreach(_.registerRDDForCleanup(this))
      sc.persistRDD(this)
    }
    storageLevel = newLevel
    this
  }

  /**
   * Set this RDD's storage level to persist its values across operations after the first time
   * it is computed. This can only be used to assign a new storage level if the RDD does not
   * have a storage level set yet. Local checkpointing is an exception.
   */
  def persist(newLevel: StorageLevel): this.type = {
    if (isLocallyCheckpointed) {
      // This means the user previously called localCheckpoint(), which should have already
      // marked this RDD for persisting. Here we should override the old storage level with
      // one that is explicitly requested by the user (after adapting it to use disk).
      persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true)
    } else {
      persist(newLevel, allowOverride = false)
    }
  }

  /**
   * Persist this RDD with the default storage level (`MEMORY_ONLY`).
   */
  def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)

  /**
   * Persist this RDD with the default storage level (`MEMORY_ONLY`).
   */
  def cache(): this.type = persist()

  /**
   * Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.
   *
   * @param blocking Whether to block until all blocks are deleted.
   * @return This RDD.
   */
  def unpersist(blocking: Boolean = true): this.type = {
    logInfo("Removing RDD " + id + " from persistence list")
    sc.unpersistRDD(id, blocking)
    storageLevel = StorageLevel.NONE
    this
  }

  /** Get the RDD's current storage level, or StorageLevel.NONE if none is set. */
  def getStorageLevel: StorageLevel = storageLevel

  /**
   * Lock for all mutable state of this RDD (persistence, partitions, dependencies, etc.).  We do
   * not use `this` because RDDs are user-visible, so users might have added their own locking on
   * RDDs; sharing that could lead to a deadlock.
   *
   * One thread might hold the lock on many of these, for a chain of RDD dependencies; but
   * because DAGs are acyclic, and we only ever hold locks for one path in that DAG, there is no
   * chance of deadlock.
   *
   * The use of Integer is simply so this is serializable -- executors may reference the shared
   * fields (though they should never mutate them, that only happens on the driver).
   */
  private val stateLock = new Integer(0)

  // Our dependencies and partitions will be gotten by calling subclass's methods below, and will
  // be overwritten when we're checkpointed
  @volatile private var dependencies_ : Seq[Dependency[_]] = _
  @volatile @transient private var partitions_ : Array[Partition] = _

  /** An Option holding our checkpoint RDD, if we are checkpointed */
  private def checkpointRDD: Option[CheckpointRDD[T]] = checkpointData.flatMap(_.checkpointRDD)

  /**
   * Get the list of dependencies of this RDD, taking into account whether the
   * RDD is checkpointed or not.
   */
  final def dependencies: Seq[Dependency[_]] = {
    checkpointRDD.map(r => List(new OneToOneDependency(r))).getOrElse {
      if (dependencies_ == null) {
        stateLock.synchronized {
          if (dependencies_ == null) {
            dependencies_ = getDependencies
          }
        }
      }
      dependencies_
    }
  }

  /**
   * Get the array of partitions of this RDD, taking into account whether the
   * RDD is checkpointed or not.
   */
  final def partitions: Array[Partition] = {
    checkpointRDD.map(_.partitions).getOrElse {
      if (partitions_ == null) {
        stateLock.synchronized {
          if (partitions_ == null) {
            partitions_ = getPartitions
            partitions_.zipWithIndex.foreach { case (partition, index) =>
              require(partition.index == index,
                s"partitions($index).partition == ${partition.index}, but it should equal $index")
            }
          }
        }
      }
      partitions_
    }
  }

  /**
   * Returns the number of partitions of this RDD.
   */
  @Since("1.6.0")
  final def getNumPartitions: Int = partitions.length

  /**
   * Get the preferred locations of a partition, taking into account whether the
   * RDD is checkpointed.
   */
  final def preferredLocations(split: Partition): Seq[String] = {
    checkpointRDD.map(_.getPreferredLocations(split)).getOrElse {
      getPreferredLocations(split)
    }
  }

  /**
   * Internal method to this RDD; will read from cache if applicable, or otherwise compute it.
   * This should ''not'' be called by users directly, but is available for implementors of custom
   * subclasses of RDD.
   */
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
      getOrCompute(split, context)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }

  /**
   * Return the ancestors of the given RDD that are related to it only through a sequence of
   * narrow dependencies. This traverses the given RDD's dependency tree using DFS, but maintains
   * no ordering on the RDDs returned.
   */
  private[spark] def getNarrowAncestors: Seq[RDD[_]] = {
    val ancestors = new mutable.HashSet[RDD[_]]

    def visit(rdd: RDD[_]): Unit = {
      val narrowDependencies = rdd.dependencies.filter(_.isInstanceOf[NarrowDependency[_]])
      val narrowParents = narrowDependencies.map(_.rdd)
      val narrowParentsNotVisited = narrowParents.filterNot(ancestors.contains)
      narrowParentsNotVisited.foreach { parent =>
        ancestors.add(parent)
        visit(parent)
      }
    }

    visit(this)

    // In case there is a cycle, do not include the root itself
    ancestors.filterNot(_ == this).toSeq
  }

  /**
   * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.
   */
  private[spark] def computeOrReadCheckpoint(split: Partition, context: TaskContext): Iterator[T] =
  {
    if (isCheckpointedAndMaterialized) {
      firstParent[T].iterator(split, context)
    } else {
      compute(split, context)
    }
  }

  /**
   * Gets or computes an RDD partition. Used by RDD.iterator() when an RDD is cached.
   */
  private[spark] def getOrCompute(partition: Partition, context: TaskContext): Iterator[T] = {
    val blockId = RDDBlockId(id, partition.index)
    var readCachedBlock = true
    // This method is called on executors, so we need call SparkEnv.get instead of sc.env.
    SparkEnv.get.blockManager.getOrElseUpdate(blockId, storageLevel, elementClassTag, () => {
      readCachedBlock = false
      computeOrReadCheckpoint(partition, context)
    }) match {
      case Left(blockResult) =>
        if (readCachedBlock) {
          val existingMetrics = context.taskMetrics().inputMetrics
          existingMetrics.incBytesRead(blockResult.bytes)
          new InterruptibleIterator[T](context, blockResult.data.asInstanceOf[Iterator[T]]) {
            override def next(): T = {
              existingMetrics.incRecordsRead(1)
              delegate.next()
            }
          }
        } else {
          new InterruptibleIterator(context, blockResult.data.asInstanceOf[Iterator[T]])
        }
      case Right(iter) =>
        new InterruptibleIterator(context, iter.asInstanceOf[Iterator[T]])
    }
  }

  /**
   * Execute a block of code in a scope such that all new RDDs created in this body will
   * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}.
   *
   * Note: Return statements are NOT allowed in the given body.
   */
  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](sc)(body)

  // Transformations (return a new RDD)

  /**
   * Return a new RDD by applying a function to all elements of this RDD.
   */
  def map[U: ClassTag](f: T => U): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.map(cleanF))
  }

  /**
   *  Return a new RDD by first applying a function to all elements of this
   *  RDD, and then flattening the results.
   */
  def flatMap[U: ClassTag](f: T => TraversableOnce[U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[U, T](this, (context, pid, iter) => iter.flatMap(cleanF))
  }

  /**
   * Return a new RDD containing only the elements that satisfy a predicate.
   */
  def filter(f: T => Boolean): RDD[T] = withScope {
    val cleanF = sc.clean(f)
    new MapPartitionsRDD[T, T](
      this,
      (context, pid, iter) => iter.filter(cleanF),
      preservesPartitioning = true)
  }

  /**
   * Return a new RDD containing the distinct elements in this RDD.
   */
  def distinct(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    map(x => (x, null)).reduceByKey((x, y) => x, numPartitions).map(_._1)
  }

  /**
   * Return a new RDD containing the distinct elements in this RDD.
   */
  def distinct(): RDD[T] = withScope {
    distinct(partitions.length)
  }

  /**
   * Return a new RDD that has exactly numPartitions partitions.
   *
   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses
   * a shuffle to redistribute data.
   *
   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,
   * which can avoid performing a shuffle.
   *
   * TODO Fix the Shuffle+Repartition data loss issue described in SPARK-23207.
   */
  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    coalesce(numPartitions, shuffle = true)
  }

  /**
   * Return a new RDD that is reduced into `numPartitions` partitions.
   *
   * This results in a narrow dependency, e.g. if you go from 1000 partitions
   * to 100 partitions, there will not be a shuffle, instead each of the 100
   * new partitions will claim 10 of the current partitions. If a larger number
   * of partitions is requested, it will stay at the current number of partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can pass shuffle = true. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @note With shuffle = true, you can actually coalesce to a larger number
   * of partitions. This is useful if you have a small number of partitions,
   * say 100, potentially with a few partitions being abnormally large. Calling
   * coalesce(1000, shuffle = true) will result in 1000 partitions with the
   * data distributed using a hash partitioner. The optional partition coalescer
   * passed in must be serializable.
   */
  def coalesce(numPartitions: Int, shuffle: Boolean = false,
               partitionCoalescer: Option[PartitionCoalescer] = Option.empty)
              (implicit ord: Ordering[T] = null)
      : RDD[T] = withScope {
    require(numPartitions > 0, s"Number of partitions ($numPartitions) must be positive.")
    if (shuffle) {
      /** Distributes elements evenly across output partitions, starting from a random partition. */
      val distributePartition = (index: Int, items: Iterator[T]) => {
        var position = new Random(hashing.byteswap32(index)).nextInt(numPartitions)
        items.map { t =>
          // Note that the hash code of the key will just be the key itself. The HashPartitioner
          // will mod it with the number of total partitions.
          position = position + 1
          (position, t)
        }
      } : Iterator[(Int, T)]

      // include a shuffle step so that our upstream tasks are still distributed
      new CoalescedRDD(
        new ShuffledRDD[Int, T, T](
          mapPartitionsWithIndexInternal(distributePartition, isOrderSensitive = true),
          new HashPartitioner(numPartitions)),
        numPartitions,
        partitionCoalescer).values
    } else {
      new CoalescedRDD(this, numPartitions, partitionCoalescer)
    }
  }

  /**
   * Return a sampled subset of this RDD.
   *
   * @param withReplacement can elements be sampled multiple times (replaced when sampled out)
   * @param fraction expected size of the sample as a fraction of this RDD's size
   *  without replacement: probability that each element is chosen; fraction must be [0, 1]
   *  with replacement: expected number of times each element is chosen; fraction must be greater
   *  than or equal to 0
   * @param seed seed for the random number generator
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[RDD]].
   */
  def sample(
      withReplacement: Boolean,
      fraction: Double,
      seed: Long = Utils.random.nextLong): RDD[T] = {
    require(fraction >= 0,
      s"Fraction must be nonnegative, but got ${fraction}")

    withScope {
      require(fraction >= 0.0, "Negative fraction value: " + fraction)
      if (withReplacement) {
        new PartitionwiseSampledRDD[T, T](this, new PoissonSampler[T](fraction), true, seed)
      } else {
        new PartitionwiseSampledRDD[T, T](this, new BernoulliSampler[T](fraction), true, seed)
      }
    }
  }

  /**
   * Randomly splits this RDD with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1
   * @param seed random seed
   *
   * @return split RDDs in an array
   */
  def randomSplit(
      weights: Array[Double],
      seed: Long = Utils.random.nextLong): Array[RDD[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    withScope {
      val sum = weights.sum
      val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
      normalizedCumWeights.sliding(2).map { x =>
        randomSampleWithRange(x(0), x(1), seed)
      }.toArray
    }
  }


  /**
   * Internal method exposed for Random Splits in DataFrames. Samples an RDD given a probability
   * range.
   * @param lb lower bound to use for the Bernoulli sampler
   * @param ub upper bound to use for the Bernoulli sampler
   * @param seed the seed for the Random number generator
   * @return A random sub-sample of the RDD without replacement.
   */
  private[spark] def randomSampleWithRange(lb: Double, ub: Double, seed: Long): RDD[T] = {
    this.mapPartitionsWithIndex( { (index, partition) =>
      val sampler = new BernoulliCellSampler[T](lb, ub)
      sampler.setSeed(seed + index)
      sampler.sample(partition)
    }, isOrderSensitive = true, preservesPartitioning = true)
  }

  /**
   * Return a fixed-size sampled subset of this RDD in an array
   *
   * @param withReplacement whether sampling is done with replacement
   * @param num size of the returned sample
   * @param seed seed for the random number generator
   * @return sample of specified size in an array
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def takeSample(
      withReplacement: Boolean,
      num: Int,
      seed: Long = Utils.random.nextLong): Array[T] = withScope {
    val numStDev = 10.0

    require(num >= 0, "Negative number of elements requested")
    require(num <= (Int.MaxValue - (numStDev * math.sqrt(Int.MaxValue)).toInt),
      "Cannot support a sample size > Int.MaxValue - " +
      s"$numStDev * math.sqrt(Int.MaxValue)")

    if (num == 0) {
      new Array[T](0)
    } else {
      val initialCount = this.count()
      if (initialCount == 0) {
        new Array[T](0)
      } else {
        val rand = new Random(seed)
        if (!withReplacement && num >= initialCount) {
          Utils.randomizeInPlace(this.collect(), rand)
        } else {
          val fraction = SamplingUtils.computeFractionForSampleSize(num, initialCount,
            withReplacement)
          var samples = this.sample(withReplacement, fraction, rand.nextInt()).collect()

          // If the first sample didn't turn out large enough, keep trying to take samples;
          // this shouldn't happen often because we use a big multiplier for the initial size
          var numIters = 0
          while (samples.length < num) {
            logWarning(s"Needed to re-sample due to insufficient sample size. Repeat #$numIters")
            samples = this.sample(withReplacement, fraction, rand.nextInt()).collect()
            numIters += 1
          }
          Utils.randomizeInPlace(samples, rand).take(num)
        }
      }
    }
  }

  /**
   * Return the union of this RDD and another one. Any identical elements will appear multiple
   * times (use `.distinct()` to eliminate them).
   */
  def union(other: RDD[T]): RDD[T] = withScope {
    sc.union(this, other)
  }

  /**
   * Return the union of this RDD and another one. Any identical elements will appear multiple
   * times (use `.distinct()` to eliminate them).
   */
  def ++(other: RDD[T]): RDD[T] = withScope {
    this.union(other)
  }

  /**
   * Return this RDD sorted by the given key function.
   */
  def sortBy[K](
      f: (T) => K,
      ascending: Boolean = true,
      numPartitions: Int = this.partitions.length)
      (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T] = withScope {
    this.keyBy[K](f)
        .sortByKey(ascending, numPartitions)
        .values
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.
   *
   * @note This method performs a shuffle internally.
   */
  def intersection(other: RDD[T]): RDD[T] = withScope {
    this.map(v => (v, null)).cogroup(other.map(v => (v, null)))
        .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty }
        .keys
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.
   *
   * @note This method performs a shuffle internally.
   *
   * @param partitioner Partitioner to use for the resulting RDD
   */
  def intersection(
      other: RDD[T],
      partitioner: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    this.map(v => (v, null)).cogroup(other.map(v => (v, null)), partitioner)
        .filter { case (_, (leftGroup, rightGroup)) => leftGroup.nonEmpty && rightGroup.nonEmpty }
        .keys
  }

  /**
   * Return the intersection of this RDD and another one. The output will not contain any duplicate
   * elements, even if the input RDDs did.  Performs a hash partition across the cluster
   *
   * @note This method performs a shuffle internally.
   *
   * @param numPartitions How many partitions to use in the resulting RDD
   */
  def intersection(other: RDD[T], numPartitions: Int): RDD[T] = withScope {
    intersection(other, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD created by coalescing all elements within each partition into an array.
   */
  def glom(): RDD[Array[T]] = withScope {
    new MapPartitionsRDD[Array[T], T](this, (context, pid, iter) => Iterator(iter.toArray))
  }

  /**
   * Return the Cartesian product of this RDD and another one, that is, the RDD of all pairs of
   * elements (a, b) where a is in `this` and b is in `other`.
   */
  def cartesian[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {
    new CartesianRDD(sc, this, other)
  }

  /**
   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](f: T => K)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
    groupBy[K](f, defaultPartitioner(this))
  }

  /**
   * Return an RDD of grouped elements. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](
      f: T => K,
      numPartitions: Int)(implicit kt: ClassTag[K]): RDD[(K, Iterable[T])] = withScope {
    groupBy(f, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD of grouped items. Each group consists of a key and a sequence of elements
   * mapping to that key. The ordering of elements within each group is not guaranteed, and
   * may even differ each time the resulting RDD is evaluated.
   *
   * @note This operation may be very expensive. If you are grouping in order to perform an
   * aggregation (such as a sum or average) over each key, using `PairRDDFunctions.aggregateByKey`
   * or `PairRDDFunctions.reduceByKey` will provide much better performance.
   */
  def groupBy[K](f: T => K, p: Partitioner)(implicit kt: ClassTag[K], ord: Ordering[K] = null)
      : RDD[(K, Iterable[T])] = withScope {
    val cleanF = sc.clean(f)
    this.map(t => (cleanF(t), t)).groupByKey(p)
  }

  /**
   * Return an RDD created by piping elements to a forked external process.
   */
  def pipe(command: String): RDD[String] = withScope {
    // Similar to Runtime.exec(), if we are given a single string, split it into words
    // using a standard StringTokenizer (i.e. by spaces)
    pipe(PipedRDD.tokenize(command))
  }

  /**
   * Return an RDD created by piping elements to a forked external process.
   */
  def pipe(command: String, env: Map[String, String]): RDD[String] = withScope {
    // Similar to Runtime.exec(), if we are given a single string, split it into words
    // using a standard StringTokenizer (i.e. by spaces)
    pipe(PipedRDD.tokenize(command), env)
  }

  /**
   * Return an RDD created by piping elements to a forked external process. The resulting RDD
   * is computed by executing the given process once per partition. All elements
   * of each input partition are written to a process's stdin as lines of input separated
   * by a newline. The resulting partition consists of the process's stdout output, with
   * each line of stdout resulting in one element of the output partition. A process is invoked
   * even for empty partitions.
   *
   * The print behavior can be customized by providing two functions.
   *
   * @param command command to run in forked process.
   * @param env environment variables to set.
   * @param printPipeContext Before piping elements, this function is called as an opportunity
   *                         to pipe context data. Print line function (like out.println) will be
   *                         passed as printPipeContext's parameter.
   * @param printRDDElement Use this function to customize how to pipe elements. This function
   *                        will be called with each RDD element as the 1st parameter, and the
   *                        print line function (like out.println()) as the 2nd parameter.
   *                        An example of pipe the RDD data of groupBy() in a streaming way,
   *                        instead of constructing a huge String to concat all the elements:
   *                        {{{
   *                        def printRDDElement(record:(String, Seq[String]), f:String=>Unit) =
   *                          for (e <- record._2) {f(e)}
   *                        }}}
   * @param separateWorkingDir Use separate working directories for each task.
   * @param bufferSize Buffer size for the stdin writer for the piped process.
   * @param encoding Char encoding used for interacting (via stdin, stdout and stderr) with
   *                 the piped process
   * @return the result RDD
   */
  def pipe(
      command: Seq[String],
      env: Map[String, String] = Map(),
      printPipeContext: (String => Unit) => Unit = null,
      printRDDElement: (T, String => Unit) => Unit = null,
      separateWorkingDir: Boolean = false,
      bufferSize: Int = 8192,
      encoding: String = Codec.defaultCharsetCodec.name): RDD[String] = withScope {
    new PipedRDD(this, command, env,
      if (printPipeContext ne null) sc.clean(printPipeContext) else null,
      if (printRDDElement ne null) sc.clean(printRDDElement) else null,
      separateWorkingDir,
      bufferSize,
      encoding)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   */
  def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(iter),
      preservesPartitioning)
  }

  /**
   * [performance] Spark's internal mapPartitionsWithIndex method that skips closure cleaning.
   * It is a performance API to be used carefully only if we are sure that the RDD elements are
   * serializable and don't require closure cleaning.
   *
   * @param preservesPartitioning indicates whether the input function preserves the partitioner,
   *                              which should be `false` unless this is a pair RDD and the input
   *                              function doesn't modify the keys.
   * @param isOrderSensitive whether or not the function is order-sensitive. If it's order
   *                         sensitive, it may return totally different result when the input order
   *                         is changed. Mostly stateful functions are order-sensitive.
   */
  private[spark] def mapPartitionsWithIndexInternal[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean = false,
      isOrderSensitive: Boolean = false): RDD[U] = withScope {
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => f(index, iter),
      preservesPartitioning = preservesPartitioning,
      isOrderSensitive = isOrderSensitive)
  }

  /**
   * [performance] Spark's internal mapPartitions method that skips closure cleaning.
   */
  private[spark] def mapPartitionsInternal[U: ClassTag](
      f: Iterator[T] => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => f(iter),
      preservesPartitioning)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
   * of the original partition.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   */
  def mapPartitionsWithIndex[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean = false): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (context: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter),
      preservesPartitioning)
  }

  /**
   * Return a new RDD by applying a function to each partition of this RDD, while tracking the index
   * of the original partition.
   *
   * `preservesPartitioning` indicates whether the input function preserves the partitioner, which
   * should be `false` unless this is a pair RDD and the input function doesn't modify the keys.
   *
   * `isOrderSensitive` indicates whether the function is order-sensitive. If it is order
   * sensitive, it may return totally different result when the input order
   * is changed. Mostly stateful functions are order-sensitive.
   */
  private[spark] def mapPartitionsWithIndex[U: ClassTag](
      f: (Int, Iterator[T]) => Iterator[U],
      preservesPartitioning: Boolean,
      isOrderSensitive: Boolean): RDD[U] = withScope {
    val cleanedF = sc.clean(f)
    new MapPartitionsRDD(
      this,
      (_: TaskContext, index: Int, iter: Iterator[T]) => cleanedF(index, iter),
      preservesPartitioning,
      isOrderSensitive = isOrderSensitive)
  }

  /**
   * Zips this RDD with another one, returning key-value pairs with the first element in each RDD,
   * second element in each RDD, etc. Assumes that the two RDDs have the *same number of
   * partitions* and the *same number of elements in each partition* (e.g. one was made through
   * a map on the other).
   */
  def zip[U: ClassTag](other: RDD[U]): RDD[(T, U)] = withScope {
    zipPartitions(other, preservesPartitioning = false) { (thisIter, otherIter) =>
      new Iterator[(T, U)] {
        def hasNext: Boolean = (thisIter.hasNext, otherIter.hasNext) match {
          case (true, true) => true
          case (false, false) => false
          case _ => throw new SparkException("Can only zip RDDs with " +
            "same number of elements in each partition")
        }
        def next(): (T, U) = (thisIter.next(), otherIter.next())
      }
    }
  }

  /**
   * Zip this RDD's partitions with one (or more) RDD(s) and return a new RDD by
   * applying a function to the zipped partitions. Assumes that all the RDDs have the
   * *same number of partitions*, but does *not* require them to have the same number
   * of elements in each partition.
   */
  def zipPartitions[B: ClassTag, V: ClassTag]
      (rdd2: RDD[B], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD2(sc, sc.clean(f), this, rdd2, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, V: ClassTag]
      (rdd2: RDD[B])
      (f: (Iterator[T], Iterator[B]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, preservesPartitioning = false)(f)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD3(sc, sc.clean(f), this, rdd2, rdd3, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C])
      (f: (Iterator[T], Iterator[B], Iterator[C]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, rdd3, preservesPartitioning = false)(f)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D], preservesPartitioning: Boolean)
      (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope {
    new ZippedPartitionsRDD4(sc, sc.clean(f), this, rdd2, rdd3, rdd4, preservesPartitioning)
  }

  def zipPartitions[B: ClassTag, C: ClassTag, D: ClassTag, V: ClassTag]
      (rdd2: RDD[B], rdd3: RDD[C], rdd4: RDD[D])
      (f: (Iterator[T], Iterator[B], Iterator[C], Iterator[D]) => Iterator[V]): RDD[V] = withScope {
    zipPartitions(rdd2, rdd3, rdd4, preservesPartitioning = false)(f)
  }


  // Actions (launch a job to return a value to the user program)

  /**
   * Applies a function f to all elements of this RDD.
   */
  def foreach(f: T => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => iter.foreach(cleanF))
  }

  /**
   * Applies a function f to each partition of this RDD.
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withScope {
    val cleanF = sc.clean(f)
    sc.runJob(this, (iter: Iterator[T]) => cleanF(iter))
  }

  /**
   * Return an array that contains all of the elements in this RDD.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   */
  def collect(): Array[T] = withScope {
    val results = sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
    Array.concat(results: _*)
  }

  /**
   * Return an iterator that contains all of the elements in this RDD.
   *
   * The iterator will consume as much memory as the largest partition in this RDD.
   *
   * @note This results in multiple Spark jobs, and if the input RDD is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input RDD should be cached first.
   */
  def toLocalIterator: Iterator[T] = withScope {
    def collectPartition(p: Int): Array[T] = {
      sc.runJob(this, (iter: Iterator[T]) => iter.toArray, Seq(p)).head
    }
    partitions.indices.iterator.flatMap(i => collectPartition(i))
  }

  /**
   * Return an RDD that contains all matching values by applying `f`.
   */
  def collect[U: ClassTag](f: PartialFunction[T, U]): RDD[U] = withScope {
    val cleanF = sc.clean(f)
    filter(cleanF.isDefinedAt).map(cleanF)
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   *
   * Uses `this` partitioner/partition size, because even if `other` is huge, the resulting
   * RDD will be &lt;= us.
   */
  def subtract(other: RDD[T]): RDD[T] = withScope {
    subtract(other, partitioner.getOrElse(new HashPartitioner(partitions.length)))
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   */
  def subtract(other: RDD[T], numPartitions: Int): RDD[T] = withScope {
    subtract(other, new HashPartitioner(numPartitions))
  }

  /**
   * Return an RDD with the elements from `this` that are not in `other`.
   */
  def subtract(
      other: RDD[T],
      p: Partitioner)(implicit ord: Ordering[T] = null): RDD[T] = withScope {
    if (partitioner == Some(p)) {
      // Our partitioner knows how to handle T (which, since we have a partitioner, is
      // really (K, V)) so make a new Partitioner that will de-tuple our fake tuples
      val p2 = new Partitioner() {
        override def numPartitions: Int = p.numPartitions
        override def getPartition(k: Any): Int = p.getPartition(k.asInstanceOf[(Any, _)]._1)
      }
      // Unfortunately, since we're making a new p2, we'll get ShuffleDependencies
      // anyway, and when calling .keys, will not have a partitioner set, even though
      // the SubtractedRDD will, thanks to p2's de-tupled partitioning, already be
      // partitioned by the right/real keys (e.g. p).
      this.map(x => (x, null)).subtractByKey(other.map((_, null)), p2).keys
    } else {
      this.map(x => (x, null)).subtractByKey(other.map((_, null)), p).keys
    }
  }

  /**
   * Reduces the elements of this RDD using the specified commutative and
   * associative binary operator.
   */
  def reduce(f: (T, T) => T): T = withScope {
    val cleanF = sc.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    var jobResult: Option[T] = None
    val mergeResult = (index: Int, taskResult: Option[T]) => {
      if (taskResult.isDefined) {
        jobResult = jobResult match {
          case Some(value) => Some(f(value, taskResult.get))
          case None => taskResult
        }
      }
    }
    sc.runJob(this, reducePartition, mergeResult)
    // Get the final result out of our Option, or throw an exception if the RDD was empty
    jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))
  }

  /**
   * Reduces the elements of this RDD in a multi-level tree pattern.
   *
   * @param depth suggested depth of the tree (default: 2)
   * @see [[org.apache.spark.rdd.RDD#reduce]]
   */
  def treeReduce(f: (T, T) => T, depth: Int = 2): T = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    val cleanF = context.clean(f)
    val reducePartition: Iterator[T] => Option[T] = iter => {
      if (iter.hasNext) {
        Some(iter.reduceLeft(cleanF))
      } else {
        None
      }
    }
    val partiallyReduced = mapPartitions(it => Iterator(reducePartition(it)))
    val op: (Option[T], Option[T]) => Option[T] = (c, x) => {
      if (c.isDefined && x.isDefined) {
        Some(cleanF(c.get, x.get))
      } else if (c.isDefined) {
        c
      } else if (x.isDefined) {
        x
      } else {
        None
      }
    }
    partiallyReduced.treeAggregate(Option.empty[T])(op, op, depth)
      .getOrElse(throw new UnsupportedOperationException("empty collection"))
  }

  /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using a
   * given associative function and a neutral "zero value". The function
   * op(t1, t2) is allowed to modify t1 and return it as its result value to avoid object
   * allocation; however, it should not modify t2.
   *
   * This behaves somewhat differently from fold operations implemented for non-distributed
   * collections in functional languages like Scala. This fold operation may be applied to
   * partitions individually, and then fold those results into the final result, rather than
   * apply the fold to each element sequentially in some defined ordering. For functions
   * that are not commutative, the result may differ from that of a fold applied to a
   * non-distributed collection.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the `op`
   *                  operator, and also the initial value for the combine results from different
   *                  partitions for the `op` operator - this will typically be the neutral
   *                  element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param op an operator used to both accumulate results within a partition and combine results
   *                  from different partitions
   */
  def fold(zeroValue: T)(op: (T, T) => T): T = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())
    val cleanOp = sc.clean(op)
    val foldPartition = (iter: Iterator[T]) => iter.fold(zeroValue)(cleanOp)
    val mergeResult = (index: Int, taskResult: T) => jobResult = op(jobResult, taskResult)
    sc.runJob(this, foldPartition, mergeResult)
    jobResult
  }

  /**
   * Aggregate the elements of each partition, and then the results for all the partitions, using
   * given combine functions and a neutral "zero value". This function can return a different result
   * type, U, than the type of this RDD, T. Thus, we need one operation for merging a T into an U
   * and one operation for merging two U's, as in scala.TraversableOnce. Both of these functions are
   * allowed to modify and return their first argument instead of creating a new U to avoid memory
   * allocation.
   *
   * @param zeroValue the initial value for the accumulated result of each partition for the
   *                  `seqOp` operator, and also the initial value for the combine results from
   *                  different partitions for the `combOp` operator - this will typically be the
   *                  neutral element (e.g. `Nil` for list concatenation or `0` for summation)
   * @param seqOp an operator used to accumulate results within a partition
   * @param combOp an associative operator used to combine results from different partitions
   */
  def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U = withScope {
    // Clone the zero value since we will also be serializing it as part of tasks
    var jobResult = Utils.clone(zeroValue, sc.env.serializer.newInstance())
    val cleanSeqOp = sc.clean(seqOp)
    val cleanCombOp = sc.clean(combOp)
    val aggregatePartition = (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
    val mergeResult = (index: Int, taskResult: U) => jobResult = combOp(jobResult, taskResult)
    sc.runJob(this, aggregatePartition, mergeResult)
    jobResult
  }

  /**
   * Aggregates the elements of this RDD in a multi-level tree pattern.
   * This method is semantically identical to [[org.apache.spark.rdd.RDD#aggregate]].
   *
   * @param depth suggested depth of the tree (default: 2)
   */
  def treeAggregate[U: ClassTag](zeroValue: U)(
      seqOp: (U, T) => U,
      combOp: (U, U) => U,
      depth: Int = 2): U = withScope {
    require(depth >= 1, s"Depth must be greater than or equal to 1 but got $depth.")
    if (partitions.length == 0) {
      Utils.clone(zeroValue, context.env.closureSerializer.newInstance())
    } else {
      val cleanSeqOp = context.clean(seqOp)
      val cleanCombOp = context.clean(combOp)
      val aggregatePartition =
        (it: Iterator[T]) => it.aggregate(zeroValue)(cleanSeqOp, cleanCombOp)
      var partiallyAggregated: RDD[U] = mapPartitions(it => Iterator(aggregatePartition(it)))
      var numPartitions = partiallyAggregated.partitions.length
      val scale = math.max(math.ceil(math.pow(numPartitions, 1.0 / depth)).toInt, 2)
      // If creating an extra level doesn't help reduce
      // the wall-clock time, we stop tree aggregation.

      // Don't trigger TreeAggregation when it doesn't save wall-clock time
      while (numPartitions > scale + math.ceil(numPartitions.toDouble / scale)) {
        numPartitions /= scale
        val curNumPartitions = numPartitions
        partiallyAggregated = partiallyAggregated.mapPartitionsWithIndex {
          (i, iter) => iter.map((i % curNumPartitions, _))
        }.foldByKey(zeroValue, new HashPartitioner(curNumPartitions))(cleanCombOp).values
      }
      val copiedZeroValue = Utils.clone(zeroValue, sc.env.closureSerializer.newInstance())
      partiallyAggregated.fold(copiedZeroValue)(cleanCombOp)
    }
  }

  /**
   * Return the number of elements in the RDD.
   */
  def count(): Long = sc.runJob(this, Utils.getIteratorSize _).sum

  /**
   * Approximate version of count() that returns a potentially incomplete result
   * within a timeout, even if not all tasks have finished.
   *
   * The confidence is the probability that the error bounds of the result will
   * contain the true value. That is, if countApprox were called repeatedly
   * with confidence 0.9, we would expect 90% of the results to contain the
   * true count. The confidence must be in the range [0,1] or an exception will
   * be thrown.
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countApprox(
      timeout: Long,
      confidence: Double = 0.95): PartialResult[BoundedDouble] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    val countElements: (TaskContext, Iterator[T]) => Long = { (ctx, iter) =>
      var result = 0L
      while (iter.hasNext) {
        result += 1L
        iter.next()
      }
      result
    }
    val evaluator = new CountEvaluator(partitions.length, confidence)
    sc.runApproximateJob(this, countElements, evaluator, timeout)
  }

  /**
   * Return the count of each unique value in this RDD as a local map of (value, count) pairs.
   *
   * @note This method should only be used if the resulting map is expected to be small, as
   * the whole thing is loaded into the driver's memory.
   * To handle very large results, consider using
   *
   * {{{
   * rdd.map(x => (x, 1L)).reduceByKey(_ + _)
   * }}}
   *
   * , which returns an RDD[T, Long] instead of a map.
   */
  def countByValue()(implicit ord: Ordering[T] = null): Map[T, Long] = withScope {
    map(value => (value, null)).countByKey()
  }

  /**
   * Approximate version of countByValue().
   *
   * @param timeout maximum time to wait for the job, in milliseconds
   * @param confidence the desired statistical confidence in the result
   * @return a potentially incomplete result, with error bounds
   */
  def countByValueApprox(timeout: Long, confidence: Double = 0.95)
      (implicit ord: Ordering[T] = null)
      : PartialResult[Map[T, BoundedDouble]] = withScope {
    require(0.0 <= confidence && confidence <= 1.0, s"confidence ($confidence) must be in [0,1]")
    if (elementClassTag.runtimeClass.isArray) {
      throw new SparkException("countByValueApprox() does not support arrays")
    }
    val countPartition: (TaskContext, Iterator[T]) => OpenHashMap[T, Long] = { (ctx, iter) =>
      val map = new OpenHashMap[T, Long]
      iter.foreach {
        t => map.changeValue(t, 1L, _ + 1L)
      }
      map
    }
    val evaluator = new GroupedCountEvaluator[T](partitions.length, confidence)
    sc.runApproximateJob(this, countPartition, evaluator, timeout)
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater
   * than `p`) would trigger sparse representation of registers, which may reduce the memory
   * consumption and increase accuracy when the cardinality is small.
   *
   * @param p The precision value for the normal set.
   *          `p` must be a value between 4 and `sp` if `sp` is not zero (32 max).
   * @param sp The precision value for the sparse set, between 0 and 32.
   *           If `sp` equals 0, the sparse representation is skipped.
   */
  def countApproxDistinct(p: Int, sp: Int): Long = withScope {
    require(p >= 4, s"p ($p) must be >= 4")
    require(sp <= 32, s"sp ($sp) must be <= 32")
    require(sp == 0 || p <= sp, s"p ($p) cannot be greater than sp ($sp)")
    val zeroCounter = new HyperLogLogPlus(p, sp)
    aggregate(zeroCounter)(
      (hll: HyperLogLogPlus, v: T) => {
        hll.offer(v)
        hll
      },
      (h1: HyperLogLogPlus, h2: HyperLogLogPlus) => {
        h1.addAll(h2)
        h1
      }).cardinality()
  }

  /**
   * Return approximate number of distinct elements in the RDD.
   *
   * The algorithm used is based on streamlib's implementation of "HyperLogLog in Practice:
   * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm", available
   * <a href="http://dx.doi.org/10.1145/2452376.2452456">here</a>.
   *
   * @param relativeSD Relative accuracy. Smaller values create counters that require more space.
   *                   It must be greater than 0.000017.
   */
  def countApproxDistinct(relativeSD: Double = 0.05): Long = withScope {
    require(relativeSD > 0.000017, s"accuracy ($relativeSD) must be greater than 0.000017")
    val p = math.ceil(2.0 * math.log(1.054 / relativeSD) / math.log(2)).toInt
    countApproxDistinct(if (p < 4) 4 else p, 0)
  }

  /**
   * Zips this RDD with its element indices. The ordering is first based on the partition index
   * and then the ordering of items within each partition. So the first item in the first
   * partition gets index 0, and the last item in the last partition receives the largest index.
   *
   * This is similar to Scala's zipWithIndex but it uses Long instead of Int as the index type.
   * This method needs to trigger a spark job when this RDD contains more than one partitions.
   *
   * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of
   * elements in a partition. The index assigned to each element is therefore not guaranteed,
   * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
   * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
   */
  def zipWithIndex(): RDD[(T, Long)] = withScope {
    new ZippedWithIndexRDD(this)
  }

  /**
   * Zips this RDD with generated unique Long ids. Items in the kth partition will get ids k, n+k,
   * 2*n+k, ..., where n is the number of partitions. So there may exist gaps, but this method
   * won't trigger a spark job, which is different from [[org.apache.spark.rdd.RDD#zipWithIndex]].
   *
   * @note Some RDDs, such as those returned by groupBy(), do not guarantee order of
   * elements in a partition. The unique ID assigned to each element is therefore not guaranteed,
   * and may even change if the RDD is reevaluated. If a fixed ordering is required to guarantee
   * the same index assignments, you should sort the RDD with sortByKey() or save it to a file.
   */
  def zipWithUniqueId(): RDD[(T, Long)] = withScope {
    val n = this.partitions.length.toLong
    this.mapPartitionsWithIndex { case (k, iter) =>
      Utils.getIteratorZipWithIndex(iter, 0L).map { case (item, i) =>
        (item, i * n + k)
      }
    }
  }

  /**
   * Take the first num elements of the RDD. It works by first scanning one partition, and use the
   * results from that partition to estimate the number of additional partitions needed to satisfy
   * the limit.
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @note Due to complications in the internal implementation, this method will raise
   * an exception if called on an RDD of `Nothing` or `Null`.
   */
  def take(num: Int): Array[T] = withScope {
    val scaleUpFactor = Math.max(conf.getInt("spark.rdd.limit.scaleUpFactor", 4), 2)
    if (num == 0) {
      new Array[T](0)
    } else {
      val buf = new ArrayBuffer[T]
      val totalParts = this.partitions.length
      var partsScanned = 0
      while (buf.size < num && partsScanned < totalParts) {
        // The number of partitions to try in this iteration. It is ok for this number to be
        // greater than totalParts because we actually cap it at totalParts in runJob.
        var numPartsToTry = 1L
        val left = num - buf.size
        if (partsScanned > 0) {
          // If we didn't find any rows after the previous iteration, quadruple and retry.
          // Otherwise, interpolate the number of partitions we need to try, but overestimate
          // it by 50%. We also cap the estimation in the end.
          if (buf.isEmpty) {
            numPartsToTry = partsScanned * scaleUpFactor
          } else {
            // As left > 0, numPartsToTry is always >= 1
            numPartsToTry = Math.ceil(1.5 * left * partsScanned / buf.size).toInt
            numPartsToTry = Math.min(numPartsToTry, partsScanned * scaleUpFactor)
          }
        }

        val p = partsScanned.until(math.min(partsScanned + numPartsToTry, totalParts).toInt)
        val res = sc.runJob(this, (it: Iterator[T]) => it.take(left).toArray, p)

        res.foreach(buf ++= _.take(num - buf.size))
        partsScanned += p.size
      }

      buf.toArray
    }
  }

  /**
   * Return the first element in this RDD.
   */
  def first(): T = withScope {
    take(1) match {
      case Array(t) => t
      case _ => throw new UnsupportedOperationException("empty collection")
    }
  }

  /**
   * Returns the top k (largest) elements from this RDD as defined by the specified
   * implicit Ordering[T] and maintains the ordering. This does the opposite of
   * [[takeOrdered]]. For example:
   * {{{
   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).top(1)
   *   // returns Array(12)
   *
   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).top(2)
   *   // returns Array(6, 5)
   * }}}
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @param num k, the number of top elements to return
   * @param ord the implicit ordering for T
   * @return an array of top elements
   */
  def top(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
    takeOrdered(num)(ord.reverse)
  }

  /**
   * Returns the first k (smallest) elements from this RDD as defined by the specified
   * implicit Ordering[T] and maintains the ordering. This does the opposite of [[top]].
   * For example:
   * {{{
   *   sc.parallelize(Seq(10, 4, 2, 12, 3)).takeOrdered(1)
   *   // returns Array(2)
   *
   *   sc.parallelize(Seq(2, 3, 4, 5, 6)).takeOrdered(2)
   *   // returns Array(2, 3)
   * }}}
   *
   * @note This method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @param num k, the number of elements to return
   * @param ord the implicit ordering for T
   * @return an array of top elements
   */
  def takeOrdered(num: Int)(implicit ord: Ordering[T]): Array[T] = withScope {
    if (num == 0) {
      Array.empty
    } else {
      val mapRDDs = mapPartitions { items =>
        // Priority keeps the largest elements, so let's reverse the ordering.
        val queue = new BoundedPriorityQueue[T](num)(ord.reverse)
        queue ++= collectionUtils.takeOrdered(items, num)(ord)
        Iterator.single(queue)
      }
      if (mapRDDs.partitions.length == 0) {
        Array.empty
      } else {
        mapRDDs.reduce { (queue1, queue2) =>
          queue1 ++= queue2
          queue1
        }.toArray.sorted(ord)
      }
    }
  }

  /**
   * Returns the max of this RDD as defined by the implicit Ordering[T].
   * @return the maximum element of the RDD
   * */
  def max()(implicit ord: Ordering[T]): T = withScope {
    this.reduce(ord.max)
  }

  /**
   * Returns the min of this RDD as defined by the implicit Ordering[T].
   * @return the minimum element of the RDD
   * */
  def min()(implicit ord: Ordering[T]): T = withScope {
    this.reduce(ord.min)
  }

  /**
   * @note Due to complications in the internal implementation, this method will raise an
   * exception if called on an RDD of `Nothing` or `Null`. This may be come up in practice
   * because, for example, the type of `parallelize(Seq())` is `RDD[Nothing]`.
   * (`parallelize(Seq())` should be avoided anyway in favor of `parallelize(Seq[T]())`.)
   * @return true if and only if the RDD contains no elements at all. Note that an RDD
   *         may be empty even when it has at least 1 partition.
   */
  def isEmpty(): Boolean = withScope {
    partitions.length == 0 || take(1).length == 0
  }

  /**
   * Save this RDD as a text file, using string representations of elements.
   */
  def saveAsTextFile(path: String): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    //
    // NullWritable is a `Comparable` in Hadoop 1.+, so the compiler cannot find an implicit
    // Ordering for it and will use the default `null`. However, it's a `Comparable[NullWritable]`
    // in Hadoop 2.+, so the compiler will call the implicit `Ordering.ordered` method to create an
    // Ordering for `NullWritable`. That's why the compiler will generate different anonymous
    // classes for `saveAsTextFile` in Hadoop 1.+ and Hadoop 2.+.
    //
    // Therefore, here we provide an explicit Ordering `null` to make sure the compiler generate
    // same bytecodes for `saveAsTextFile`.
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =>
      val text = new Text()
      iter.map { x =>
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path)
  }

  /**
   * Save this RDD as a compressed text file, using string representations of elements.
   */
  def saveAsTextFile(path: String, codec: Class[_ <: CompressionCodec]): Unit = withScope {
    // https://issues.apache.org/jira/browse/SPARK-2075
    val nullWritableClassTag = implicitly[ClassTag[NullWritable]]
    val textClassTag = implicitly[ClassTag[Text]]
    val r = this.mapPartitions { iter =>
      val text = new Text()
      iter.map { x =>
        text.set(x.toString)
        (NullWritable.get(), text)
      }
    }
    RDD.rddToPairRDDFunctions(r)(nullWritableClassTag, textClassTag, null)
      .saveAsHadoopFile[TextOutputFormat[NullWritable, Text]](path, codec)
  }

  /**
   * Save this RDD as a SequenceFile of serialized objects.
   */
  def saveAsObjectFile(path: String): Unit = withScope {
    this.mapPartitions(iter => iter.grouped(10).map(_.toArray))
      .map(x => (NullWritable.get(), new BytesWritable(Utils.serialize(x))))
      .saveAsSequenceFile(path)
  }

  /**
   * Creates tuples of the elements in this RDD by applying `f`.
   */
  def keyBy[K](f: T => K): RDD[(K, T)] = withScope {
    val cleanedF = sc.clean(f)
    map(x => (cleanedF(x), x))
  }

  /** A private method for tests, to look at the contents of each partition */
  private[spark] def collectPartitions(): Array[Array[T]] = withScope {
    sc.runJob(this, (iter: Iterator[T]) => iter.toArray)
  }

  /**
   * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir` and all references to its parent
   * RDDs will be removed. This function must be called before any job has been
   * executed on this RDD. It is strongly recommended that this RDD is persisted in
   * memory, otherwise saving it on a file will require recomputation.
   */
  def checkpoint(): Unit = RDDCheckpointData.synchronized {
    // NOTE: we use a global lock here due to complexities downstream with ensuring
    // children RDD partitions point to the correct parent partitions. In the future
    // we should revisit this consideration.
    if (context.checkpointDir.isEmpty) {
      throw new SparkException("Checkpoint directory has not been set in the SparkContext")
    } else if (checkpointData.isEmpty) {
      checkpointData = Some(new ReliableRDDCheckpointData(this))
    }
  }

  /**
   * Mark this RDD for local checkpointing using Spark's existing caching layer.
   *
   * This method is for users who wish to truncate RDD lineages while skipping the expensive
   * step of replicating the materialized data in a reliable distributed file system. This is
   * useful for RDDs with long lineages that need to be truncated periodically (e.g. GraphX).
   *
   * Local checkpointing sacrifices fault-tolerance for performance. In particular, checkpointed
   * data is written to ephemeral local storage in the executors instead of to a reliable,
   * fault-tolerant storage. The effect is that if an executor fails during the computation,
   * the checkpointed data may no longer be accessible, causing an irrecoverable job failure.
   *
   * This is NOT safe to use with dynamic allocation, which removes executors along
   * with their cached blocks. If you must use both features, you are advised to set
   * `spark.dynamicAllocation.cachedExecutorIdleTimeout` to a high value.
   *
   * The checkpoint directory set through `SparkContext#setCheckpointDir` is not used.
   */
  def localCheckpoint(): this.type = RDDCheckpointData.synchronized {
    if (conf.getBoolean("spark.dynamicAllocation.enabled", false) &&
        conf.contains("spark.dynamicAllocation.cachedExecutorIdleTimeout")) {
      logWarning("Local checkpointing is NOT safe to use with dynamic allocation, " +
        "which removes executors along with their cached blocks. If you must use both " +
        "features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` " +
        "to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to " +
        "at least 1 hour.")
    }

    // Note: At this point we do not actually know whether the user will call persist() on
    // this RDD later, so we must explicitly call it here ourselves to ensure the cached
    // blocks are registered for cleanup later in the SparkContext.
    //
    // If, however, the user has already called persist() on this RDD, then we must adapt
    // the storage level he/she specified to one that is appropriate for local checkpointing
    // (i.e. uses disk) to guarantee correctness.

    if (storageLevel == StorageLevel.NONE) {
      persist(LocalRDDCheckpointData.DEFAULT_STORAGE_LEVEL)
    } else {
      persist(LocalRDDCheckpointData.transformStorageLevel(storageLevel), allowOverride = true)
    }

    // If this RDD is already checkpointed and materialized, its lineage is already truncated.
    // We must not override our `checkpointData` in this case because it is needed to recover
    // the checkpointed data. If it is overridden, next time materializing on this RDD will
    // cause error.
    if (isCheckpointedAndMaterialized) {
      logWarning("Not marking RDD for local checkpoint because it was already " +
        "checkpointed and materialized")
    } else {
      // Lineage is not truncated yet, so just override any existing checkpoint data with ours
      checkpointData match {
        case Some(_: ReliableRDDCheckpointData[_]) => logWarning(
          "RDD was already marked for reliable checkpointing: overriding with local checkpoint.")
        case _ =>
      }
      checkpointData = Some(new LocalRDDCheckpointData(this))
    }
    this
  }

  /**
   * Return whether this RDD is checkpointed and materialized, either reliably or locally.
   */
  def isCheckpointed: Boolean = isCheckpointedAndMaterialized

  /**
   * Return whether this RDD is checkpointed and materialized, either reliably or locally.
   * This is introduced as an alias for `isCheckpointed` to clarify the semantics of the
   * return value. Exposed for testing.
   */
  private[spark] def isCheckpointedAndMaterialized: Boolean =
    checkpointData.exists(_.isCheckpointed)

  /**
   * Return whether this RDD is marked for local checkpointing.
   * Exposed for testing.
   */
  private[rdd] def isLocallyCheckpointed: Boolean = {
    checkpointData match {
      case Some(_: LocalRDDCheckpointData[T]) => true
      case _ => false
    }
  }

  /**
   * Return whether this RDD is reliably checkpointed and materialized.
   */
  private[rdd] def isReliablyCheckpointed: Boolean = {
    checkpointData match {
      case Some(reliable: ReliableRDDCheckpointData[_]) if reliable.isCheckpointed => true
      case _ => false
    }
  }

  /**
   * Gets the name of the directory to which this RDD was checkpointed.
   * This is not defined if the RDD is checkpointed locally.
   */
  def getCheckpointFile: Option[String] = {
    checkpointData match {
      case Some(reliable: ReliableRDDCheckpointData[T]) => reliable.getCheckpointDir
      case _ => None
    }
  }

  /**
   * :: Experimental ::
   * Marks the current stage as a barrier stage, where Spark must launch all tasks together.
   * In case of a task failure, instead of only restarting the failed task, Spark will abort the
   * entire stage and re-launch all tasks for this stage.
   * The barrier execution mode feature is experimental and it only handles limited scenarios.
   * Please read the linked SPIP and design docs to understand the limitations and future plans.
   * @return an [[RDDBarrier]] instance that provides actions within a barrier stage
   * @see [[org.apache.spark.BarrierTaskContext]]
   * @see <a href="https://jira.apache.org/jira/browse/SPARK-24374">SPIP: Barrier Execution Mode</a>
   * @see <a href="https://jira.apache.org/jira/browse/SPARK-24582">Design Doc</a>
   */
  @Experimental
  @Since("2.4.0")
  def barrier(): RDDBarrier[T] = withScope(new RDDBarrier[T](this))

  // =======================================================================
  // Other internal methods and fields
  // =======================================================================

  private var storageLevel: StorageLevel = StorageLevel.NONE

  /** User code that created this RDD (e.g. `textFile`, `parallelize`). */
  @transient private[spark] val creationSite = sc.getCallSite()

  /**
   * The scope associated with the operation that created this RDD.
   *
   * This is more flexible than the call site and can be defined hierarchically. For more
   * detail, see the documentation of {{RDDOperationScope}}. This scope is not defined if the
   * user instantiates this RDD himself without using any Spark operations.
   */
  @transient private[spark] val scope: Option[RDDOperationScope] = {
    Option(sc.getLocalProperty(SparkContext.RDD_SCOPE_KEY)).map(RDDOperationScope.fromJson)
  }

  private[spark] def getCreationSite: String = Option(creationSite).map(_.shortForm).getOrElse("")

  private[spark] def elementClassTag: ClassTag[T] = classTag[T]

  private[spark] var checkpointData: Option[RDDCheckpointData[T]] = None

  // Whether to checkpoint all ancestor RDDs that are marked for checkpointing. By default,
  // we stop as soon as we find the first such RDD, an optimization that allows us to write
  // less data but is not safe for all workloads. E.g. in streaming we may checkpoint both
  // an RDD and its parent in every batch, in which case the parent may never be checkpointed
  // and its lineage never truncated, leading to OOMs in the long run (SPARK-6847).
  private val checkpointAllMarkedAncestors =
    Option(sc.getLocalProperty(RDD.CHECKPOINT_ALL_MARKED_ANCESTORS)).exists(_.toBoolean)

  /** Returns the first parent RDD */
  protected[spark] def firstParent[U: ClassTag]: RDD[U] = {
    dependencies.head.rdd.asInstanceOf[RDD[U]]
  }

  /** Returns the jth parent RDD: e.g. rdd.parent[T](0) is equivalent to rdd.firstParent[T] */
  protected[spark] def parent[U: ClassTag](j: Int): RDD[U] = {
    dependencies(j).rdd.asInstanceOf[RDD[U]]
  }

  /** The [[org.apache.spark.SparkContext]] that this RDD was created on. */
  def context: SparkContext = sc

  /**
   * Private API for changing an RDD's ClassTag.
   * Used for internal Java-Scala API compatibility.
   */
  private[spark] def retag(cls: Class[T]): RDD[T] = {
    val classTag: ClassTag[T] = ClassTag.apply(cls)
    this.retag(classTag)
  }

  /**
   * Private API for changing an RDD's ClassTag.
   * Used for internal Java-Scala API compatibility.
   */
  private[spark] def retag(implicit classTag: ClassTag[T]): RDD[T] = {
    this.mapPartitions(identity, preservesPartitioning = true)(classTag)
  }

  // Avoid handling doCheckpoint multiple times to prevent excessive recursion
  @transient private var doCheckpointCalled = false

  /**
   * Performs the checkpointing of this RDD by saving this. It is called after a job using this RDD
   * has completed (therefore the RDD has been materialized and potentially stored in memory).
   * doCheckpoint() is called recursively on the parent RDDs.
   */
  private[spark] def doCheckpoint(): Unit = {
    RDDOperationScope.withScope(sc, "checkpoint", allowNesting = false, ignoreParent = true) {
      if (!doCheckpointCalled) {
        doCheckpointCalled = true
        if (checkpointData.isDefined) {
          if (checkpointAllMarkedAncestors) {
            // TODO We can collect all the RDDs that needs to be checkpointed, and then checkpoint
            // them in parallel.
            // Checkpoint parents first because our lineage will be truncated after we
            // checkpoint ourselves
            dependencies.foreach(_.rdd.doCheckpoint())
          }
          checkpointData.get.checkpoint()
        } else {
          dependencies.foreach(_.rdd.doCheckpoint())
        }
      }
    }
  }

  /**
   * Changes the dependencies of this RDD from its original parents to a new RDD (`newRDD`)
   * created from the checkpoint file, and forget its old dependencies and partitions.
   */
  private[spark] def markCheckpointed(): Unit = stateLock.synchronized {
    clearDependencies()
    partitions_ = null
    deps = null    // Forget the constructor argument for dependencies too
  }

  /**
   * Clears the dependencies of this RDD. This method must ensure that all references
   * to the original parent RDDs are removed to enable the parent RDDs to be garbage
   * collected. Subclasses of RDD may override this method for implementing their own cleaning
   * logic. See [[org.apache.spark.rdd.UnionRDD]] for an example.
   */
  protected def clearDependencies(): Unit = stateLock.synchronized {
    dependencies_ = null
  }

  /** A description of this RDD and its recursive dependencies for debugging. */
  def toDebugString: String = {
    // Get a debug description of an rdd without its children
    def debugSelf(rdd: RDD[_]): Seq[String] = {
      import Utils.bytesToString

      val persistence = if (storageLevel != StorageLevel.NONE) storageLevel.description else ""
      val storageInfo = rdd.context.getRDDStorageInfo(_.id == rdd.id).map(info =>
        "    CachedPartitions: %d; MemorySize: %s; ExternalBlockStoreSize: %s; DiskSize: %s".format(
          info.numCachedPartitions, bytesToString(info.memSize),
          bytesToString(info.externalBlockStoreSize), bytesToString(info.diskSize)))

      s"$rdd [$persistence]" +: storageInfo
    }

    // Apply a different rule to the last child
    def debugChildren(rdd: RDD[_], prefix: String): Seq[String] = {
      val len = rdd.dependencies.length
      len match {
        case 0 => Seq.empty
        case 1 =>
          val d = rdd.dependencies.head
          debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]], true)
        case _ =>
          val frontDeps = rdd.dependencies.take(len - 1)
          val frontDepStrings = frontDeps.flatMap(
            d => debugString(d.rdd, prefix, d.isInstanceOf[ShuffleDependency[_, _, _]]))

          val lastDep = rdd.dependencies.last
          val lastDepStrings =
            debugString(lastDep.rdd, prefix, lastDep.isInstanceOf[ShuffleDependency[_, _, _]], true)

          frontDepStrings ++ lastDepStrings
      }
    }
    // The first RDD in the dependency stack has no parents, so no need for a +-
    def firstDebugString(rdd: RDD[_]): Seq[String] = {
      val partitionStr = "(" + rdd.partitions.length + ")"
      val leftOffset = (partitionStr.length - 1) / 2
      val nextPrefix = (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset))

      debugSelf(rdd).zipWithIndex.map{
        case (desc: String, 0) => s"$partitionStr $desc"
        case (desc: String, _) => s"$nextPrefix $desc"
      } ++ debugChildren(rdd, nextPrefix)
    }
    def shuffleDebugString(rdd: RDD[_], prefix: String = "", isLastChild: Boolean): Seq[String] = {
      val partitionStr = "(" + rdd.partitions.length + ")"
      val leftOffset = (partitionStr.length - 1) / 2
      val thisPrefix = prefix.replaceAll("\\|\\s+$", "")
      val nextPrefix = (
        thisPrefix
        + (if (isLastChild) "  " else "| ")
        + (" " * leftOffset) + "|" + (" " * (partitionStr.length - leftOffset)))

      debugSelf(rdd).zipWithIndex.map{
        case (desc: String, 0) => s"$thisPrefix+-$partitionStr $desc"
        case (desc: String, _) => s"$nextPrefix$desc"
      } ++ debugChildren(rdd, nextPrefix)
    }
    def debugString(
        rdd: RDD[_],
        prefix: String = "",
        isShuffle: Boolean = true,
        isLastChild: Boolean = false): Seq[String] = {
      if (isShuffle) {
        shuffleDebugString(rdd, prefix, isLastChild)
      } else {
        debugSelf(rdd).map(prefix + _) ++ debugChildren(rdd, prefix)
      }
    }
    firstDebugString(this).mkString("\n")
  }

  override def toString: String = "%s%s[%d] at %s".format(
    Option(name).map(_ + " ").getOrElse(""), getClass.getSimpleName, id, getCreationSite)

  def toJavaRDD() : JavaRDD[T] = {
    new JavaRDD(this)(elementClassTag)
  }

  /**
   * Whether the RDD is in a barrier stage. Spark must launch all the tasks at the same time for a
   * barrier stage.
   *
   * An RDD is in a barrier stage, if at least one of its parent RDD(s), or itself, are mapped from
   * an [[RDDBarrier]]. This function always returns false for a [[ShuffledRDD]], since a
   * [[ShuffledRDD]] indicates start of a new stage.
   *
   * A [[MapPartitionsRDD]] can be transformed from an [[RDDBarrier]], under that case the
   * [[MapPartitionsRDD]] shall be marked as barrier.
   */
  private[spark] def isBarrier(): Boolean = isBarrier_

  // From performance concern, cache the value to avoid repeatedly compute `isBarrier()` on a long
  // RDD chain.
  @transient protected lazy val isBarrier_ : Boolean =
    dependencies.filter(!_.isInstanceOf[ShuffleDependency[_, _, _]]).exists(_.rdd.isBarrier())

  /**
   * Returns the deterministic level of this RDD's output. Please refer to [[DeterministicLevel]]
   * for the definition.
   *
   * By default, an reliably checkpointed RDD, or RDD without parents(root RDD) is DETERMINATE. For
   * RDDs with parents, we will generate a deterministic level candidate per parent according to
   * the dependency. The deterministic level of the current RDD is the deterministic level
   * candidate that is deterministic least. Please override [[getOutputDeterministicLevel]] to
   * provide custom logic of calculating output deterministic level.
   */
  // TODO: make it public so users can set deterministic level to their custom RDDs.
  // TODO: this can be per-partition. e.g. UnionRDD can have different deterministic level for
  // different partitions.
  private[spark] final lazy val outputDeterministicLevel: DeterministicLevel.Value = {
    if (isReliablyCheckpointed) {
      DeterministicLevel.DETERMINATE
    } else {
      getOutputDeterministicLevel
    }
  }

  @DeveloperApi
  protected def getOutputDeterministicLevel: DeterministicLevel.Value = {
    val deterministicLevelCandidates = dependencies.map {
      // The shuffle is not really happening, treat it like narrow dependency and assume the output
      // deterministic level of current RDD is same as parent.
      case dep: ShuffleDependency[_, _, _] if dep.rdd.partitioner.exists(_ == dep.partitioner) =>
        dep.rdd.outputDeterministicLevel

      case dep: ShuffleDependency[_, _, _] =>
        if (dep.rdd.outputDeterministicLevel == DeterministicLevel.INDETERMINATE) {
          // If map output was indeterminate, shuffle output will be indeterminate as well
          DeterministicLevel.INDETERMINATE
        } else if (dep.keyOrdering.isDefined && dep.aggregator.isDefined) {
          // if aggregator specified (and so unique keys) and key ordering specified - then
          // consistent ordering.
          DeterministicLevel.DETERMINATE
        } else {
          // In Spark, the reducer fetches multiple remote shuffle blocks at the same time, and
          // the arrival order of these shuffle blocks are totally random. Even if the parent map
          // RDD is DETERMINATE, the reduce RDD is always UNORDERED.
          DeterministicLevel.UNORDERED
        }

      // For narrow dependency, assume the output deterministic level of current RDD is same as
      // parent.
      case dep => dep.rdd.outputDeterministicLevel
    }

    if (deterministicLevelCandidates.isEmpty) {
      // By default we assume the root RDD is determinate.
      DeterministicLevel.DETERMINATE
    } else {
      deterministicLevelCandidates.maxBy(_.id)
    }
  }

}


/**
 * Defines implicit functions that provide extra functionalities on RDDs of specific types.
 *
 * For example, [[RDD.rddToPairRDDFunctions]] converts an RDD into a [[PairRDDFunctions]] for
 * key-value-pair RDDs, and enabling extra functionalities such as `PairRDDFunctions.reduceByKey`.
 */
object RDD {

  private[spark] val CHECKPOINT_ALL_MARKED_ANCESTORS =
    "spark.checkpoint.checkpointAllMarkedAncestors"

  // The following implicit functions were in SparkContext before 1.3 and users had to
  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
  // them automatically. However, we still keep the old functions in SparkContext for backward
  // compatibility and forward to the following functions directly.

  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])
    (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null): PairRDDFunctions[K, V] = {
    new PairRDDFunctions(rdd)
  }

  implicit def rddToAsyncRDDActions[T: ClassTag](rdd: RDD[T]): AsyncRDDActions[T] = {
    new AsyncRDDActions(rdd)
  }

  implicit def rddToSequenceFileRDDFunctions[K, V](rdd: RDD[(K, V)])
      (implicit kt: ClassTag[K], vt: ClassTag[V],
                keyWritableFactory: WritableFactory[K],
                valueWritableFactory: WritableFactory[V])
    : SequenceFileRDDFunctions[K, V] = {
    implicit val keyConverter = keyWritableFactory.convert
    implicit val valueConverter = valueWritableFactory.convert
    new SequenceFileRDDFunctions(rdd,
      keyWritableFactory.writableClass(kt), valueWritableFactory.writableClass(vt))
  }

  implicit def rddToOrderedRDDFunctions[K : Ordering : ClassTag, V: ClassTag](rdd: RDD[(K, V)])
    : OrderedRDDFunctions[K, V, (K, V)] = {
    new OrderedRDDFunctions[K, V, (K, V)](rdd)
  }

  implicit def doubleRDDToDoubleRDDFunctions(rdd: RDD[Double]): DoubleRDDFunctions = {
    new DoubleRDDFunctions(rdd)
  }

  implicit def numericRDDToDoubleRDDFunctions[T](rdd: RDD[T])(implicit num: Numeric[T])
    : DoubleRDDFunctions = {
    new DoubleRDDFunctions(rdd.map(x => num.toDouble(x)))
  }
}

/**
 * The deterministic level of RDD's output (i.e. what `RDD#compute` returns). This explains how
 * the output will diff when Spark reruns the tasks for the RDD. There are 3 deterministic levels:
 * 1. DETERMINATE: The RDD output is always the same data set in the same order after a rerun.
 * 2. UNORDERED: The RDD output is always the same data set but the order can be different
 *               after a rerun.
 * 3. INDETERMINATE. The RDD output can be different after a rerun.
 *
 * Note that, the output of an RDD usually relies on the parent RDDs. When the parent RDD's output
 * is INDETERMINATE, it's very likely the RDD's output is also INDETERMINATE.
 */
private[spark] object DeterministicLevel extends Enumeration {
  val DETERMINATE, UNORDERED, INDETERMINATE = Value
}

[0m2021.02.25 15:54:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:54:36 INFO  time: compiled root in 4.24s[0m
[0m2021.02.25 15:54:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:54:41 INFO  time: compiled root in 1.6s[0m
[0m2021.02.25 15:54:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:54:46 INFO  time: compiled root in 1.18s[0m
[0m2021.02.25 15:55:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:55:52 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 15:55:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:55:55 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 15:55:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:55:58 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 15:56:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 15:56:02 INFO  time: compiled root in 0.97s[0m
[0m2021.02.25 16:08:44 INFO  compiling root (1 scala source)[0m
Feb 25, 2021 4:08:44 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.02.25 16:08:44 INFO  time: compiled root in 0.53s[0m
[0m2021.02.25 16:08:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:08:45 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 16:08:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:08:53 INFO  time: compiled root in 0.6s[0m
Feb 25, 2021 4:11:30 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.02.25 16:11:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:11:31 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:11:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:11:37 INFO  time: compiled root in 0.91s[0m
[0m2021.02.25 16:11:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:11:42 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 16:11:42 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 16:11:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:11:52 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 16:11:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:11:54 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 16:12:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:12:08 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 16:13:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:13:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:03 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:13:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:13:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:15 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:13:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:56: stale bloop error: invalid escape character
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                       ^[0m
[0m2021.02.25 16:13:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:56: stale bloop error: invalid escape character
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                       ^[0m
[0m2021.02.25 16:13:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:167: stale bloop error: unclosed string literal
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                      ^[0m
[0m2021.02.25 16:13:21 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:56: stale bloop error: invalid escape character
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                       ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:167: stale bloop error: unclosed string literal
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                      ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:56: stale bloop error: invalid escape character
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                       ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:167: stale bloop error: unclosed string literal
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                      ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:56: stale bloop error: invalid escape character
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                       ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:167: stale bloop error: unclosed string literal
    .filter(line => line.contains("?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                      ^[0m
[0m2021.02.25 16:13:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:25 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 16:13:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:13:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:13:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:13:33 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:14:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:14:17 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 16:14:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:14:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:168: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*) && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                       ^[0m
[0m2021.02.25 16:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:170: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                         ^[0m
[0m2021.02.25 16:14:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:84: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                   ^[0m
[0m2021.02.25 16:14:23 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:170: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                         ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:84: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                   ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:170: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                         ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:84: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                   ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:170: stale bloop error: unclosed string literal
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                         ^[0m
[0m2021.02.25 16:14:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:84: stale bloop error: ')' expected but string literal found.
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*"") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                   ^[0m
[0m2021.02.25 16:14:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:14:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:14:27 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 16:15:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:15:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:15:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([A-Za-z0-9._%+-]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:15:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:15:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:15:44 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 16:17:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\*)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:33 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 16:17:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\*)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\*)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\*)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:35 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 16:17:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:37 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:17:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:40 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 16:17:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: value r is not a member of StringContext
    .filter(line => line.contains(r"(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.25 16:17:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: value r is not a member of StringContext
    .filter(line => line.contains(r"(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
Feb 25, 2021 4:17:43 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: String index out of range: -1
[0m2021.02.25 16:17:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: value r is not a member of StringContext
    .filter(line => line.contains(r"(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.25 16:17:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:44 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:59: stale bloop error: invalid escape character
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                          ^[0m
[0m2021.02.25 16:17:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:37: stale bloop error: identifier expected but string literal found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                    ^[0m
[0m2021.02.25 16:17:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:162: stale bloop error: ';' expected but ')' found.
    .filter(line => line.contains(r."(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                                                                                                                                 ^[0m
[0m2021.02.25 16:17:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:17:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:17:49 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 16:20:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:20:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:20:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:20:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:20:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:57: stale bloop error: invalid escape character
    .filter(line => line.contains("(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                                        ^[0m
[0m2021.02.25 16:20:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:20:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:54 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:30:35: stale bloop error: unclosed multi-line string literal
    .filter(line => line.contains("""(?:https?://)?(?:www\.)?([job]+)/?.*") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
                                  ^[0m
[0m2021.02.25 16:20:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:37:2: stale bloop error: illegal start of simple expression
}
 ^[0m
[0m2021.02.25 16:20:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:20:58 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 16:21:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:21:41 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:21:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:21:50 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35:10: stale bloop error: ')' expected but '.' found.
    spark.close()
         ^[0m
[0m2021.02.25 16:21:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:36:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 16:21:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:21:53 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 16:21:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:21:58 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 16:22:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:01 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 16:22:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:03 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 16:22:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:08 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 16:22:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:12 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 16:22:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:24 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 16:22:24 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 16:22:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:26 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 16:22:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:32 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 16:22:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:36 INFO  time: compiled root in 0.92s[0m
[0m2021.02.25 16:22:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:37 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 16:22:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:22:44 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 16:26:13 INFO  compiling root (1 scala source)[0m
Feb 25, 2021 4:26:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3565
[0m2021.02.25 16:26:13 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 16:26:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:26:16 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 16:26:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:26:24 INFO  time: compiled root in 1.19s[0m
Feb 25, 2021 4:26:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3598
[0m2021.02.25 16:26:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:26:28 INFO  time: compiled root in 1.16s[0m
[0m2021.02.25 16:30:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:30:33 INFO  time: compiled root in 1.76s[0m
[0m2021.02.25 16:34:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:34:24 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 16:34:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:34:27 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 16:34:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:34:29 INFO  time: compiled root in 1.06s[0m
[0m2021.02.25 16:38:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:32 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 16:38:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:36 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 16:38:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:43 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 16:38:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:49 INFO  time: compiled root in 1.18s[0m
[0m2021.02.25 16:38:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:38:51 INFO  time: compiled root in 1.21s[0m
[0m2021.02.25 16:55:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:55:31 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 16:55:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:55:39 INFO  time: compiled root in 1.85s[0m
[0m2021.02.25 16:55:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:55:48 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 16:55:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:55:58 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 16:56:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:56:00 INFO  time: compiled root in 0.85s[0m
Feb 25, 2021 4:56:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3849
Feb 25, 2021 4:56:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3846
[0m2021.02.25 16:56:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:56:10 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 16:59:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 16:59:28 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 17:00:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:00:20 INFO  time: compiled root in 1.02s[0m
[0m2021.02.25 17:01:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:01:36 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 17:03:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:03:33 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 17:03:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:03:34 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 17:03:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:03:38 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 17:17:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:17:59 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 17:18:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:18:01 INFO  time: compiled root in 0.89s[0m
[0m2021.02.25 17:19:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:19:50 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 17:19:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:19:53 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 17:19:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:19:58 INFO  time: compiled root in 1.18s[0m
[0m2021.02.25 17:25:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:25:49 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 17:25:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:25:52 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 17:25:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:25:54 INFO  time: compiled root in 0.84s[0m
[0m2021.02.25 17:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:30:06 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 17:33:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:33:07 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 17:33:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:33:18 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 17:33:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:33:20 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 17:33:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:33:47 INFO  time: compiled root in 0.68s[0m
[0m2021.02.25 17:33:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:33:55 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 17:34:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:34:20 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 17:34:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:34:22 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 17:34:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:34:28 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 17:34:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:34:34 INFO  time: compiled root in 2.78s[0m
Feb 25, 2021 5:38:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:38:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:08 INFO  time: compiled root in 0.14s[0m
Feb 25, 2021 5:38:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:38:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Feb 25, 2021 5:38:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:15 INFO  time: compiled root in 0.12s[0m
Feb 25, 2021 5:38:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:38:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 78 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 78 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 78 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:38:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Feb 25, 2021 5:38:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:42 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 17:38:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Feb 25, 2021 5:38:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:38:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:44 INFO  time: compiled root in 0.13s[0m
Feb 25, 2021 5:38:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
Feb 25, 2021 5:38:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:38:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:38:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:38:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:38:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:38:46 INFO  time: compiled root in 0.13s[0m
Feb 25, 2021 5:38:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:38:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:41:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 65 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:41:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 25, 2021 5:41:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 74 is not a valid line number, allowed [0..47]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:43:27: stale bloop error: unclosed string literal
    val sqlDF = spark.sql("CREATE EXTERNAL TABLE IF NOT EXISTS ccindex (
                          ^[0m
[0m2021.02.25 17:41:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 17:41:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:41:36 INFO  time: compiled root in 1.45s[0m
[0m2021.02.25 17:41:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:41:39 INFO  time: compiled root in 0.84s[0m
[0m2021.02.25 17:41:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:41:54 INFO  time: compiled root in 0.64s[0m
[0m2021.02.25 17:41:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:41:59 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 17:42:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:11 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 17:42:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:16 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 17:42:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:24 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 17:42:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:30 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 17:42:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:32 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 17:42:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:34 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 17:42:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:39 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 17:42:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:41 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 17:42:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:42:48 INFO  time: compiled root in 0.68s[0m
[0m2021.02.25 17:55:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:31 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 17:55:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:36 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 17:55:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:39 INFO  time: compiled root in 0.65s[0m
[0m2021.02.25 17:55:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:43 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 17:55:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:46 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 17:55:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:50 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 17:55:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:55:54 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 17:57:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:57:51 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 17:57:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:57:55 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 17:58:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:01 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 17:58:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:05 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 17:58:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:10 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 17:58:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:14 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 17:58:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:21 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 17:58:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:25 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 17:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:27 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 17:58:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:58:31 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 17:59:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:59:40 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 17:59:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:59:41 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 17:59:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:59:49 INFO  time: compiled root in 98ms[0m
[0m2021.02.25 17:59:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 17:59:59 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 18:00:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:00:03 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 18:00:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:00:13 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 18:00:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:00:50 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 18:00:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:00:53 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 18:01:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:01:28 INFO  time: compiled root in 0.85s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.02.25 18:01:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:01:55 INFO  time: compiled root in 0.62s[0m
[0m2021.02.25 18:01:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:01:58 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 18:01:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:01:59 INFO  time: compiled root in 0.72s[0m
[0m2021.02.25 18:02:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:02:02 INFO  time: compiled root in 0.67s[0m
[0m2021.02.25 18:02:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:02:10 INFO  time: compiled root in 0.66s[0m
[0m2021.02.25 18:02:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:02:13 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 18:08:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:08:53 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 18:09:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:00 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 18:09:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:15 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 18:09:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:24 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 18:09:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:27 INFO  time: compiled root in 0.12s[0m
Feb 25, 2021 6:09:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5162
Feb 25, 2021 6:09:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5161
Feb 25, 2021 6:09:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5163
[0m2021.02.25 18:09:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:31 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 18:09:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:32 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 18:09:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:35 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 18:09:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:37 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 18:09:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:40 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 18:09:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:47 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 18:09:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:50 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 18:09:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:09:54 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 18:10:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:10:02 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 18:10:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:10:15 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 18:10:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:10:17 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 18:10:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:10:19 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 18:16:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:16:53 INFO  time: compiled root in 1.76s[0m
[0m2021.02.25 18:53:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:18 INFO  time: compiled root in 1.04s[0m
[0m2021.02.25 18:53:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:23 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 18:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:27 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 18:53:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:35 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 18:53:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:37 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 18:53:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:38 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 18:53:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:42 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 18:53:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:53:44 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 18:55:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:55:30 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 18:55:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:55:34 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 18:55:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:55:38 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 18:55:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 18:55:44 INFO  time: compiled root in 0.68s[0m
[0m2021.02.25 19:03:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:03:44 INFO  time: compiled root in 0.92s[0m
[0m2021.02.25 19:03:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:03:52 INFO  time: compiled root in 1.12s[0m
[0m2021.02.25 19:03:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:03:56 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 19:08:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:08:49 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 19:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:08:54 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 19:08:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:08:58 INFO  time: compiled root in 0.6s[0m
[0m2021.02.25 19:09:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:09:02 INFO  time: compiled root in 0.84s[0m
[0m2021.02.25 19:14:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:14:43 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 19:14:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:14:49 INFO  time: compiled root in 0.68s[0m
Feb 25, 2021 7:15:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5671
[0m2021.02.25 19:16:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:21 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 19:16:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:24 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 19:16:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:26 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 19:16:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:49 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 19:16:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:54 INFO  time: compiled root in 0.12s[0m
Feb 25, 2021 7:16:54 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.25 19:16:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:57 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 19:16:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:16:59 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 19:17:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:02 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 19:17:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:08 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:17:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:22 INFO  time: compiled root in 0.96s[0m
[0m2021.02.25 19:17:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:24 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 19:17:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:29 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 19:17:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:36 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 19:17:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:17:43 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 19:18:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:03 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 19:18:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:05 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 19:18:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:14 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 19:18:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:20 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 19:18:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:24 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:18:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:35 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:18:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:47 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 19:18:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:18:52 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:19:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:19:10 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 19:19:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:19:16 INFO  time: compiled root in 1.03s[0m
[0m2021.02.25 19:19:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:19:22 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 19:20:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:12 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 19:20:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:23 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 19:20:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:27 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 19:20:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:45 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 19:20:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:47 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 19:20:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:51 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 19:20:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:20:56 INFO  time: compiled root in 1.66s[0m
[0m2021.02.25 19:21:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:22 INFO  time: compiled root in 1.32s[0m
[0m2021.02.25 19:21:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:24 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 19:21:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:27 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 19:21:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:30 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 19:21:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:34 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 19:21:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:21:42 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 19:22:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:22:16 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:22:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:22:19 INFO  time: compiled root in 0.78s[0m
Feb 25, 2021 7:25:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6333
[0m2021.02.25 19:29:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:29:48 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 19:29:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:29:57 INFO  time: compiled root in 0.85s[0m
Feb 25, 2021 7:31:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6375
[0m2021.02.25 19:37:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:26 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 19:37:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:29 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 19:37:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:31 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 19:37:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:36 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 19:37:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:49 INFO  time: compiled root in 0.76s[0m
[0m2021.02.25 19:37:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:51 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 19:37:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:57 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 19:37:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:37:58 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 19:38:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:38:00 INFO  time: compiled root in 0.88s[0m
Feb 25, 2021 7:38:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6487
[0m2021.02.25 19:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:40:48 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 19:40:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:40:51 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 19:41:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:01 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 19:41:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:03 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 19:41:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:09 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 19:41:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:13 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 19:41:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:17 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 19:41:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:46:15: stale bloop error: unclosed string literal
      .option("inferSchema(
              ^[0m
[0m2021.02.25 19:41:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:46:15: stale bloop error: unclosed string literal
      .option("inferSchema(
              ^[0m
[0m2021.02.25 19:41:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:66:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:41:20 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 19:41:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:46:15: stale bloop error: unclosed string literal
      .option("inferSchema(
              ^[0m
[0m2021.02.25 19:41:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:66:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:41:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:46:15: stale bloop error: unclosed string literal
      .option("inferSchema(
              ^[0m
[0m2021.02.25 19:41:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:66:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:46:15: stale bloop error: unclosed string literal
      .option("inferSchema(
              ^[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:66:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:41:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:23 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:52:5: stale bloop error: ')' expected but 'import' found.
    import spark.implicits._
    ^[0m
[0m2021.02.25 19:41:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:26 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:41:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:28 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:41:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:31 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 19:41:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:41:34 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 19:43:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:43:49 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 19:43:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:43:55 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:43:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:43:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:44:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:63:5: stale bloop error: value select is not a member of org.apache.spark.sql.DataFrameReader
possible cause: maybe a semicolon is missing before `value select'?
> parqDF
>       .select[0m
[0m2021.02.25 19:44:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:44:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:00 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:55:13: stale bloop error: unclosed string literal
    .format("parquet)(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:68:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:08 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:56:13: stale bloop error: unclosed string literal
    .option("inferSchema(
            ^[0m
[0m2021.02.25 19:44:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:69:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.25 19:44:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:44:12 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 19:44:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:44:16 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 19:44:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:44:20 INFO  time: compiled root in 0.73s[0m
[0m2021.02.25 19:45:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:11 INFO  time: compiled root in 0.75s[0m
[0m2021.02.25 19:45:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:12 INFO  time: compiled root in 0.74s[0m
[0m2021.02.25 19:45:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:14 INFO  time: compiled root in 0.83s[0m
[0m2021.02.25 19:45:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:26 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 19:45:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:39 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 19:45:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:45 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 19:45:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:45:58 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 19:46:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:46:32 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:46:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:46:35 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 19:47:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:21 INFO  time: compiled root in 0.84s[0m
[0m2021.02.25 19:47:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:26 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 19:47:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:29 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 19:47:33 INFO  compiling root (1 scala source)[0m
Feb 25, 2021 7:47:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.InterruptedException
java.util.concurrent.CompletionException: java.lang.InterruptedException
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.pc.VirtualFileParams.checkCanceled(VirtualFileParams.java:25)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:76)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.02.25 19:47:34 INFO  time: compiled root in 1.02s[0m
[0m2021.02.25 19:47:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:36 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 19:47:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:38 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 19:47:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:47:48 INFO  time: compiled root in 1s[0m
[0m2021.02.25 19:48:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:48:01 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 19:48:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:48:03 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 19:48:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:48:05 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 19:48:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:48:08 INFO  time: compiled root in 0.88s[0m
[0m2021.02.25 19:48:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:48:11 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 19:49:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:49:04 INFO  time: compiled root in 0.84s[0m
Feb 25, 2021 7:49:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7192
Feb 25, 2021 7:58:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7202
Feb 25, 2021 7:58:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7208
[0m2021.02.25 19:59:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 19:59:15 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 20:00:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:00:48 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 20:00:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:00:49 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 20:00:49 INFO  time: compiled root in 0.91s[0m
[0m2021.02.25 20:00:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:00:59 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 20:01:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:01:00 INFO  time: compiled root in 0.87s[0m
[0m2021.02.25 20:06:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:06:46 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 20:06:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:06:49 INFO  time: compiled root in 1.28s[0m
[0m2021.02.25 20:06:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:06:51 INFO  time: compiled root in 0.94s[0m
[0m2021.02.25 20:06:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:06:55 INFO  time: compiled root in 1.87s[0m
[0m2021.02.25 20:07:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:07:13 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 20:07:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:07:17 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 20:08:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:08:24 INFO  time: compiled root in 0.86s[0m
[0m2021.02.25 20:11:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:11:34 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.02.25 20:11:34 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 20:11:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:11:39 INFO  time: compiled root in 0.71s[0m
[0m2021.02.25 20:11:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:11:49 INFO  time: compiled root in 1.34s[0m
[0m2021.02.25 20:12:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:12:21 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 20:12:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:12:41 INFO  time: compiled root in 0.93s[0m
[0m2021.02.25 20:13:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:13:29 INFO  time: compiled root in 0.77s[0m
[0m2021.02.25 20:13:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:13:34 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 20:14:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:14:36 INFO  time: compiled root in 0.73s[0m
Feb 25, 2021 8:15:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7680
[0m2021.02.25 20:16:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:16:10 INFO  time: compiled root in 0.78s[0m
[0m2021.02.25 20:16:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:16:16 INFO  time: compiled root in 0.1s[0m
[0m2021.02.25 20:16:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:16:17 INFO  time: compiled root in 0.16s[0m
Feb 25, 2021 8:16:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7790
[0m2021.02.25 20:17:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:03 INFO  time: compiled root in 0.8s[0m
[0m2021.02.25 20:17:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:24 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 20:17:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:27 INFO  time: compiled root in 0.13s[0m
[0m2021.02.25 20:17:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:31 INFO  time: compiled root in 0.12s[0m
[0m2021.02.25 20:17:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:33 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 20:17:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:17:56 INFO  time: compiled root in 0.11s[0m
[0m2021.02.25 20:19:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:19:50 INFO  time: compiled root in 0.29s[0m
Feb 25, 2021 8:23:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7949
[0m2021.02.25 20:25:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:14 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 20:25:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:17 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 20:25:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:25 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 20:25:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:37 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 20:25:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:44 INFO  time: compiled root in 0.18s[0m
[0m2021.02.25 20:25:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:48 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 20:25:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:57 INFO  time: compiled root in 0.81s[0m
[0m2021.02.25 20:25:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:25:59 INFO  time: compiled root in 0.82s[0m
[0m2021.02.25 20:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:26:02 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 20:26:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:26:08 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 20:26:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:26:17 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 20:26:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:26:30 INFO  time: compiled root in 0.85s[0m
[0m2021.02.25 20:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:26:33 INFO  time: compiled root in 0.79s[0m
[0m2021.02.25 20:29:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:29:14 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 20:29:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:29:20 INFO  time: compiled root in 0.95s[0m
[0m2021.02.25 20:46:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 20:46:58 INFO  time: compiled root in 0.9s[0m
[0m2021.02.25 21:04:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:04:19 INFO  time: compiled root in 1.47s[0m
[0m2021.02.25 21:09:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:09:55 INFO  time: compiled root in 0.41s[0m
[0m2021.02.25 21:10:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:04 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:10:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:06 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 21:10:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:08 INFO  time: compiled root in 0.36s[0m
[0m2021.02.25 21:10:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:12 INFO  time: compiled root in 0.33s[0m
[0m2021.02.25 21:10:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:16 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:10:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:21 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 21:10:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:23 INFO  time: compiled root in 0.26s[0m
Feb 25, 2021 9:10:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8418
[0m2021.02.25 21:10:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:30 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 21:10:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:54 INFO  time: compiled root in 1.44s[0m
[0m2021.02.25 21:10:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:57 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:10:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:10:59 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:11:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:00 INFO  time: compiled root in 0.34s[0m
[0m2021.02.25 21:11:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:18 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:11:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:19 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 21:11:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:21 INFO  time: compiled root in 0.2s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package collection
package generic

import mutable.{Builder, MapBuilder}
import scala.language.higherKinds

/** A template for companion objects of `Map` and subclasses thereof.
 *
 *  @define coll map
 *  @define Coll `Map`
 *  @define factoryInfo
 *    This object provides a set of operations needed to create `$Coll` values.
 *    @author Martin Odersky
 *    @since 2.8
 *  @define canBuildFromInfo
 *    The standard `CanBuildFrom` instance for `$Coll` objects.
 *    @see CanBuildFrom
 *  @define mapCanBuildFromInfo
 *    The standard `CanBuildFrom` instance for `$Coll` objects.
 *    The created value is an instance of class `MapCanBuildFrom`.
 *    @see CanBuildFrom
 *    @see GenericCanBuildFrom
 */
abstract class GenMapFactory[CC[A, B] <: GenMap[A, B] with GenMapLike[A, B, CC[A, B]]] {

  /** The type constructor of the collection that can be built by this factory */
  type Coll = CC[_, _]

  /** An empty $Coll */
  def empty[A, B]: CC[A, B]

  /** A collection of type $Coll that contains given key/value bindings.
   *  @param elems   the key/value pairs that make up the $coll
   *  @tparam A      the type of the keys
   *  @tparam B      the type of the associated values
   *  @return        a new $coll consisting key/value pairs given by `elems`.
   */
  def apply[A, B](elems: (A, B)*): CC[A, B] = (newBuilder[A, B] ++= elems).result()

  /** The default builder for $Coll objects.
   *  @tparam A      the type of the keys
   *  @tparam B      the type of the associated values
   */
  def newBuilder[A, B]: Builder[(A, B), CC[A, B]] = new MapBuilder[A, B, CC[A, B]](empty[A, B])

  /** The standard `CanBuildFrom` class for maps.
   */
  class MapCanBuildFrom[A, B] extends CanBuildFrom[Coll, (A, B), CC[A, B]] {
    def apply(from: Coll) = newBuilder[A, B]
    def apply() = newBuilder
  }
}

[0m2021.02.25 21:11:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:26 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 21:11:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:28 INFO  time: compiled root in 0.17s[0m
[0m2021.02.25 21:11:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:33 INFO  time: compiled root in 0.43s[0m
[0m2021.02.25 21:11:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:11:55 INFO  time: compiled root in 0.34s[0m
[0m2021.02.25 21:12:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:12:03 INFO  time: compiled root in 1.56s[0m
[0m2021.02.25 21:12:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:12:10 INFO  time: compiled root in 2.46s[0m
[0m2021.02.25 21:12:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:12:51 INFO  time: compiled root in 0.16s[0m
[0m2021.02.25 21:12:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:12:54 INFO  time: compiled root in 1.41s[0m
[0m2021.02.25 21:12:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:12:57 INFO  time: compiled root in 1.16s[0m
[0m2021.02.25 21:13:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:29 INFO  time: compiled root in 1.61s[0m
Feb 25, 2021 9:13:30 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.25 21:13:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:32 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 21:13:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:37 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 21:13:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:44 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 21:13:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:48 INFO  time: compiled root in 1.14s[0m
[0m2021.02.25 21:13:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:57 INFO  time: compiled root in 4.9s[0m
[0m2021.02.25 21:13:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:13:58 INFO  time: compiled root in 0.49s[0m
[0m2021.02.25 21:14:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:00 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 21:14:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:06 INFO  time: compiled root in 0.98s[0m
[0m2021.02.25 21:14:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:09 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 21:14:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:15 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 21:14:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:19 INFO  time: compiled root in 0.24s[0m
[0m2021.02.25 21:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:24 INFO  time: compiled root in 1.16s[0m
[0m2021.02.25 21:14:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:28 INFO  time: compiled root in 1.23s[0m
[0m2021.02.25 21:14:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:34 INFO  time: compiled root in 1s[0m
[0m2021.02.25 21:14:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:14:38 INFO  time: compiled root in 1.31s[0m
[0m2021.02.25 21:20:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:20:49 INFO  time: compiled root in 1.17s[0m
[0m2021.02.25 21:20:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:20:54 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 21:21:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:02 INFO  time: compiled root in 1.43s[0m
[0m2021.02.25 21:21:06 INFO  compiling root (1 scala source)[0m
Feb 25, 2021 9:21:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8942
[0m2021.02.25 21:21:07 INFO  time: compiled root in 1.71s[0m
[0m2021.02.25 21:21:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:08 INFO  time: compiled root in 0.32s[0m
[0m2021.02.25 21:21:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:13 INFO  time: compiled root in 1.46s[0m
[0m2021.02.25 21:21:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:15 INFO  time: compiled root in 0.36s[0m
[0m2021.02.25 21:21:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:28 INFO  time: compiled root in 0.27s[0m
Feb 25, 2021 9:21:43 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9000
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8998
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9003
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9008
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9007
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9009
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9014
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9016
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9015
Feb 25, 2021 9:21:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9017
[0m2021.02.25 21:21:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:47 INFO  time: compiled root in 0.4s[0m
Feb 25, 2021 9:21:48 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.25 21:21:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:47 INFO  time: compiled root in 0.45s[0m
Feb 25, 2021 9:21:48 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.25 21:21:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:52 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 21:21:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:21:56 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 21:22:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:04 INFO  time: compiled root in 1.3s[0m
[0m2021.02.25 21:22:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:10 INFO  time: compiled root in 0.38s[0m
[0m2021.02.25 21:22:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:13 INFO  time: compiled root in 1.53s[0m
[0m2021.02.25 21:22:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:18 INFO  time: compiled root in 1.27s[0m
[0m2021.02.25 21:22:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:20 INFO  time: compiled root in 1.31s[0m
[0m2021.02.25 21:22:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:27 INFO  time: compiled root in 1.29s[0m
[0m2021.02.25 21:22:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:38 INFO  time: compiled root in 1.31s[0m
[0m2021.02.25 21:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:43 INFO  time: compiled root in 1.43s[0m
[0m2021.02.25 21:22:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:22:48 INFO  time: compiled root in 1.66s[0m
[0m2021.02.25 21:27:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:27:02 INFO  time: compiled root in 0.99s[0m
[0m2021.02.25 21:27:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:27:07 INFO  time: compiled root in 1.01s[0m
[0m2021.02.25 21:27:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:27:20 INFO  time: compiled root in 1.11s[0m
[0m2021.02.25 21:27:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:27:22 INFO  time: compiled root in 1.22s[0m
[0m2021.02.25 21:27:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:27:35 INFO  time: compiled root in 1.05s[0m
Feb 25, 2021 9:30:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9210
Feb 25, 2021 9:47:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9216
[0m2021.02.25 21:50:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:50:44 INFO  time: compiled root in 0.15s[0m
[0m2021.02.25 21:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:50:47 INFO  time: compiled root in 0.34s[0m
[0m2021.02.25 21:50:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:50:53 INFO  time: compiled root in 1.56s[0m
[0m2021.02.25 21:51:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:51:00 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 21:51:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:51:12 INFO  time: compiled root in 0.22s[0m
[0m2021.02.25 21:51:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:51:15 INFO  time: compiled root in 1.27s[0m
[0m2021.02.25 21:53:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:09 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 21:53:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:11 INFO  time: compiled root in 0.26s[0m
[0m2021.02.25 21:53:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:18 INFO  time: compiled root in 0.25s[0m
[0m2021.02.25 21:53:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:21 INFO  time: compiled root in 0.29s[0m
[0m2021.02.25 21:53:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:24 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 21:53:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:30 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 21:53:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:34 INFO  time: compiled root in 0.28s[0m
[0m2021.02.25 21:53:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 21:53:57 INFO  time: compiled root in 0.27s[0m
[0m2021.02.25 23:36:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:36:46 INFO  time: compiled root in 2.1s[0m
[0m2021.02.25 23:36:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:36:46 INFO  time: compiled root in 0.7s[0m
[0m2021.02.25 23:36:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:36:50 INFO  time: compiled root in 0.39s[0m
[0m2021.02.25 23:36:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:36:51 INFO  time: compiled root in 0.14s[0m
[0m2021.02.25 23:36:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:36:57 INFO  time: compiled root in 0.46s[0m
[0m2021.02.25 23:36:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:00 INFO  time: compiled root in 1.5s[0m
[0m2021.02.25 23:37:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:05 INFO  time: compiled root in 0.23s[0m
[0m2021.02.25 23:37:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:06 INFO  time: compiled root in 0.2s[0m
[0m2021.02.25 23:37:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:12 INFO  time: compiled root in 1.02s[0m
[0m2021.02.25 23:37:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:15 INFO  time: compiled root in 1.14s[0m
Feb 25, 2021 11:37:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9635
[0m2021.02.25 23:37:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:36 INFO  time: compiled root in 1s[0m
[0m2021.02.25 23:37:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:44 INFO  time: compiled root in 0.19s[0m
[0m2021.02.25 23:37:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:37:45 INFO  time: compiled root in 0.21s[0m
[0m2021.02.25 23:39:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.25 23:39:08 INFO  time: compiled root in 1.1s[0m
[0m2021.02.26 00:19:58 INFO  compiling root (1 scala source)[0m
Feb 26, 2021 12:19:59 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9740
[0m2021.02.26 00:20:01 INFO  time: compiled root in 2.98s[0m
[0m2021.02.26 00:20:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 00:20:05 INFO  time: compiled root in 1.4s[0m
[0m2021.02.26 00:20:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 00:20:07 INFO  time: compiled root in 1.05s[0m
[0m2021.02.26 01:27:41 INFO  shutting down Metals[0m
[0m2021.02.26 01:27:41 INFO  Shut down connection with build server.[0m
[0m2021.02.26 01:27:41 INFO  Shut down connection with build server.[0m
[0m2021.02.26 01:27:41 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.26 19:22:12 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.26 19:22:13 INFO  time: initialize in 0.9s[0m
[0m2021.02.26 19:22:14 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4062278610156304434/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.26 19:22:14 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.26 19:22:14 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.26 19:22:18 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode

object GetS3Data {
  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    val parsedRDD = rdd
      .flatMap(line =>
        line.split("""\s+""") match {
          case Array(href, _) => Some(href)
        }
      )

    // parsedRDD.take(100).foreach(println)

    val jsonDF = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    //df.take(50).foreach(println)

    import spark.implicits._
    val parqDF = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = parqDF
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")

    spark.close()
  }
}

Waiting for the bsp connection to come up...
[0m2021.02.26 19:22:21 INFO  time: code lens generation in 6.82s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4062278610156304434/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4062278610156304434/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 19:22:21 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 19:22:22 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/Runner.scala[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode

object GetS3Data {
  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    val parsedRDD = rdd
      .flatMap(line =>
        line.split("""\s+""") match {
          case Array(href, _) => Some(href)
        }
      )

    // parsedRDD.take(100).foreach(println)

    val jsonDF = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    //df.take(50).foreach(println)

    import spark.implicits._
    val parqDF = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = parqDF
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")

    spark.close()
  }
}

[0m2021.02.26 19:22:23 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5390745163991245258/bsp.socket'...[0m
2021.02.26 19:22:23 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2586198313669226423/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5390745163991245258/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5390745163991245258/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2586198313669226423/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2586198313669226423/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.26 19:22:23 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 19:22:23 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.26 19:22:23 INFO  time: Connected to build server in 8.92s[0m
[0m2021.02.26 19:22:23 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.26 19:22:23 INFO  time: Imported build in 0.55s[0m
[0m2021.02.26 19:22:27 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.26 19:22:27 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.26 19:22:29 INFO  time: indexed workspace in 5.33s[0m
[0m2021.02.26 19:22:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:22:31 INFO  compiling root (1 scala source)[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.02.26 19:22:38 INFO  time: compiled root in 7.53s[0m
[0mFeb 26, 2021 7:22:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala
java.util.concurrent.CompletionException: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsEnrichments$XtensionAbsolutePathBuffers.toInputFromBuffers(MetalsEnrichments.scala:321)
	at scala.meta.internal.metals.Compilers.originInput$1(Compilers.scala:189)
	at scala.meta.internal.metals.Compilers.$anonfun$didChange$2(Compilers.scala:202)
	at scala.Option.getOrElse(Option.scala:189)
	at scala.meta.internal.metals.Compilers.didChange(Compilers.scala:201)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$parseTreesAndPublishDiags$1(MetalsLanguageServer.scala:270)
	at scala.concurrent.Future$.$anonfun$traverse$1(Future.scala:850)
	at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
	at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
	at scala.collection.immutable.List.foldLeft(List.scala:91)
	at scala.concurrent.Future$.traverse(Future.scala:850)
	at scala.meta.internal.metals.MetalsLanguageServer.parseTreesAndPublishDiags(MetalsLanguageServer.scala:267)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$parseTrees$2(MetalsLanguageServer.scala:189)
	at scala.concurrent.Future.$anonfun$flatMap$1(Future.scala:307)
	at scala.concurrent.impl.Promise.$anonfun$transformWith$1(Promise.scala:41)
	... 4 more

Feb 26, 2021 7:22:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 45
2021.02.26 19:22:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:22:42 INFO  time: compiled root in 4.15s[0m
[0m2021.02.26 19:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:22:42 INFO  time: compiled root in 0.57s[0m
[0m2021.02.26 19:22:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:22:50 INFO  time: compiled root in 5.58s[0m
[0m2021.02.26 19:23:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:23:39 INFO  time: compiled root in 0.98s[0m
[0m2021.02.26 19:23:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:23:40 INFO  time: compiled root in 0.61s[0m
Feb 26, 2021 7:23:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 82
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark

import java.io._
import java.net.URI
import java.util.{Arrays, Locale, Properties, ServiceLoader, UUID}
import java.util.concurrent.{ConcurrentHashMap, ConcurrentMap}
import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger, AtomicReference}

import scala.collection.JavaConverters._
import scala.collection.Map
import scala.collection.generic.Growable
import scala.collection.mutable.HashMap
import scala.language.implicitConversions
import scala.reflect.{classTag, ClassTag}
import scala.util.control.NonFatal

import com.google.common.collect.MapMaker
import org.apache.commons.lang3.SerializationUtils
import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FileSystem, Path}
import org.apache.hadoop.io.{ArrayWritable, BooleanWritable, BytesWritable, DoubleWritable, FloatWritable, IntWritable, LongWritable, NullWritable, Text, Writable}
import org.apache.hadoop.mapred.{FileInputFormat, InputFormat, JobConf, SequenceFileInputFormat, TextInputFormat}
import org.apache.hadoop.mapreduce.{InputFormat => NewInputFormat, Job => NewHadoopJob}
import org.apache.hadoop.mapreduce.lib.input.{FileInputFormat => NewFileInputFormat}

import org.apache.spark.annotation.DeveloperApi
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.deploy.{LocalSparkCluster, SparkHadoopUtil}
import org.apache.spark.input.{FixedLengthBinaryInputFormat, PortableDataStream, StreamInputFormat, WholeTextFileInputFormat}
import org.apache.spark.internal.Logging
import org.apache.spark.internal.config._
import org.apache.spark.io.CompressionCodec
import org.apache.spark.partial.{ApproximateEvaluator, PartialResult}
import org.apache.spark.rdd._
import org.apache.spark.rpc.RpcEndpointRef
import org.apache.spark.scheduler._
import org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend, StandaloneSchedulerBackend}
import org.apache.spark.scheduler.local.LocalSchedulerBackend
import org.apache.spark.status.AppStatusStore
import org.apache.spark.status.api.v1.ThreadStackTrace
import org.apache.spark.storage._
import org.apache.spark.storage.BlockManagerMessages.TriggerThreadDump
import org.apache.spark.ui.{ConsoleProgressBar, SparkUI}
import org.apache.spark.util._

/**
 * Main entry point for Spark functionality. A SparkContext represents the connection to a Spark
 * cluster, and can be used to create RDDs, accumulators and broadcast variables on that cluster.
 *
 * Only one SparkContext may be active per JVM.  You must `stop()` the active SparkContext before
 * creating a new one.  This limitation may eventually be removed; see SPARK-2243 for more details.
 *
 * @param config a Spark Config object describing the application configuration. Any settings in
 *   this config overrides the default configs as well as system properties.
 */
class SparkContext(config: SparkConf) extends Logging {

  // The call site where this SparkContext was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  // If true, log warnings instead of throwing exceptions when multiple SparkContexts are active
  private val allowMultipleContexts: Boolean =
    config.getBoolean("spark.driver.allowMultipleContexts", false)

  // In order to prevent multiple SparkContexts from being active at the same time, mark this
  // context as having started construction.
  // NOTE: this must be placed at the beginning of the SparkContext constructor.
  SparkContext.markPartiallyConstructed(this, allowMultipleContexts)

  val startTime = System.currentTimeMillis()

  private[spark] val stopped: AtomicBoolean = new AtomicBoolean(false)

  private[spark] def assertNotStopped(): Unit = {
    if (stopped.get()) {
      val activeContext = SparkContext.activeContext.get()
      val activeCreationSite =
        if (activeContext == null) {
          "(No active SparkContext.)"
        } else {
          activeContext.creationSite.longForm
        }
      throw new IllegalStateException(
        s"""Cannot call methods on a stopped SparkContext.
           |This stopped SparkContext was created at:
           |
           |${creationSite.longForm}
           |
           |The currently active SparkContext was created at:
           |
           |$activeCreationSite
         """.stripMargin)
    }
  }

  /**
   * Create a SparkContext that loads settings from system properties (for instance, when
   * launching with ./bin/spark-submit).
   */
  def this() = this(new SparkConf())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI
   * @param conf a [[org.apache.spark.SparkConf]] object specifying other Spark parameters
   */
  def this(master: String, appName: String, conf: SparkConf) =
    this(SparkContext.updatedConf(conf, master, appName))

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file
   *             system or HDFS, HTTP, HTTPS, or FTP URLs.
   * @param environment Environment variables to set on worker nodes.
   */
  def this(
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()) = {
    this(SparkContext.updatedConf(new SparkConf(), master, appName, sparkHome, jars, environment))
  }

  // The following constructors are required when Java code accesses SparkContext directly.
  // Please see SI-4278

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   */
  private[spark] def this(master: String, appName: String) =
    this(master, appName, null, Nil, Map())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   */
  private[spark] def this(master: String, appName: String, sparkHome: String) =
    this(master, appName, sparkHome, Nil, Map())

  /**
   * Alternative constructor that allows setting common Spark properties directly
   *
   * @param master Cluster URL to connect to (e.g. mesos://host:port, spark://host:port, local[4]).
   * @param appName A name for your application, to display on the cluster web UI.
   * @param sparkHome Location where Spark is installed on cluster nodes.
   * @param jars Collection of JARs to send to the cluster. These can be paths on the local file
   *             system or HDFS, HTTP, HTTPS, or FTP URLs.
   */
  private[spark] def this(master: String, appName: String, sparkHome: String, jars: Seq[String]) =
    this(master, appName, sparkHome, jars, Map())

  // log out Spark Version in Spark driver log
  logInfo(s"Running Spark version $SPARK_VERSION")

  /* ------------------------------------------------------------------------------------- *
   | Private variables. These variables keep the internal state of the context, and are    |
   | not accessible by the outside world. They're mutable since we want to initialize all  |
   | of them to some neutral value ahead of time, so that calling "stop()" while the       |
   | constructor is still running is safe.                                                 |
   * ------------------------------------------------------------------------------------- */

  private var _conf: SparkConf = _
  private var _eventLogDir: Option[URI] = None
  private var _eventLogCodec: Option[String] = None
  private var _listenerBus: LiveListenerBus = _
  private var _env: SparkEnv = _
  private var _statusTracker: SparkStatusTracker = _
  private var _progressBar: Option[ConsoleProgressBar] = None
  private var _ui: Option[SparkUI] = None
  private var _hadoopConfiguration: Configuration = _
  private var _executorMemory: Int = _
  private var _schedulerBackend: SchedulerBackend = _
  private var _taskScheduler: TaskScheduler = _
  private var _heartbeatReceiver: RpcEndpointRef = _
  @volatile private var _dagScheduler: DAGScheduler = _
  private var _applicationId: String = _
  private var _applicationAttemptId: Option[String] = None
  private var _eventLogger: Option[EventLoggingListener] = None
  private var _executorAllocationManager: Option[ExecutorAllocationManager] = None
  private var _cleaner: Option[ContextCleaner] = None
  private var _listenerBusStarted: Boolean = false
  private var _jars: Seq[String] = _
  private var _files: Seq[String] = _
  private var _shutdownHookRef: AnyRef = _
  private var _statusStore: AppStatusStore = _

  /* ------------------------------------------------------------------------------------- *
   | Accessors and public fields. These provide access to the internal state of the        |
   | context.                                                                              |
   * ------------------------------------------------------------------------------------- */

  private[spark] def conf: SparkConf = _conf

  /**
   * Return a copy of this SparkContext's configuration. The configuration ''cannot'' be
   * changed at runtime.
   */
  def getConf: SparkConf = conf.clone()

  def jars: Seq[String] = _jars
  def files: Seq[String] = _files
  def master: String = _conf.get("spark.master")
  def deployMode: String = _conf.getOption("spark.submit.deployMode").getOrElse("client")
  def appName: String = _conf.get("spark.app.name")

  private[spark] def isEventLogEnabled: Boolean = _conf.getBoolean("spark.eventLog.enabled", false)
  private[spark] def eventLogDir: Option[URI] = _eventLogDir
  private[spark] def eventLogCodec: Option[String] = _eventLogCodec

  def isLocal: Boolean = Utils.isLocalMaster(_conf)

  /**
   * @return true if context is stopped or in the midst of stopping.
   */
  def isStopped: Boolean = stopped.get()

  private[spark] def statusStore: AppStatusStore = _statusStore

  // An asynchronous listener bus for Spark events
  private[spark] def listenerBus: LiveListenerBus = _listenerBus

  // This function allows components created by SparkEnv to be mocked in unit tests:
  private[spark] def createSparkEnv(
      conf: SparkConf,
      isLocal: Boolean,
      listenerBus: LiveListenerBus): SparkEnv = {
    SparkEnv.createDriverEnv(conf, isLocal, listenerBus, SparkContext.numDriverCores(master, conf))
  }

  private[spark] def env: SparkEnv = _env

  // Used to store a URL for each static file/jar together with the file's local timestamp
  private[spark] val addedFiles = new ConcurrentHashMap[String, Long]().asScala
  private[spark] val addedJars = new ConcurrentHashMap[String, Long]().asScala

  // Keeps track of all persisted RDDs
  private[spark] val persistentRdds = {
    val map: ConcurrentMap[Int, RDD[_]] = new MapMaker().weakValues().makeMap[Int, RDD[_]]()
    map.asScala
  }
  def statusTracker: SparkStatusTracker = _statusTracker

  private[spark] def progressBar: Option[ConsoleProgressBar] = _progressBar

  private[spark] def ui: Option[SparkUI] = _ui

  def uiWebUrl: Option[String] = _ui.map(_.webUrl)

  /**
   * A default Hadoop Configuration for the Hadoop code (e.g. file systems) that we reuse.
   *
   * @note As it will be reused in all Hadoop RDDs, it's better not to modify it unless you
   * plan to set some global configurations for all Hadoop RDDs.
   */
  def hadoopConfiguration: Configuration = _hadoopConfiguration

  private[spark] def executorMemory: Int = _executorMemory

  // Environment variables to pass to our executors.
  private[spark] val executorEnvs = HashMap[String, String]()

  // Set SPARK_USER for user who is running SparkContext.
  val sparkUser = Utils.getCurrentUserName()

  private[spark] def schedulerBackend: SchedulerBackend = _schedulerBackend

  private[spark] def taskScheduler: TaskScheduler = _taskScheduler
  private[spark] def taskScheduler_=(ts: TaskScheduler): Unit = {
    _taskScheduler = ts
  }

  private[spark] def dagScheduler: DAGScheduler = _dagScheduler
  private[spark] def dagScheduler_=(ds: DAGScheduler): Unit = {
    _dagScheduler = ds
  }

  /**
   * A unique identifier for the Spark application.
   * Its format depends on the scheduler implementation.
   * (i.e.
   *  in case of local spark app something like 'local-1433865536131'
   *  in case of YARN something like 'application_1433865536131_34483'
   *  in case of MESOS something like 'driver-20170926223339-0001'
   * )
   */
  def applicationId: String = _applicationId
  def applicationAttemptId: Option[String] = _applicationAttemptId

  private[spark] def eventLogger: Option[EventLoggingListener] = _eventLogger

  private[spark] def executorAllocationManager: Option[ExecutorAllocationManager] =
    _executorAllocationManager

  private[spark] def cleaner: Option[ContextCleaner] = _cleaner

  private[spark] var checkpointDir: Option[String] = None

  // Thread Local variable that can be used by users to pass information down the stack
  protected[spark] val localProperties = new InheritableThreadLocal[Properties] {
    override protected def childValue(parent: Properties): Properties = {
      // Note: make a clone such that changes in the parent properties aren't reflected in
      // the those of the children threads, which has confusing semantics (SPARK-10563).
      SerializationUtils.clone(parent)
    }
    override protected def initialValue(): Properties = new Properties()
  }

  /* ------------------------------------------------------------------------------------- *
   | Initialization. This code initializes the context in a manner that is exception-safe. |
   | All internal fields holding state are initialized here, and any error prompts the     |
   | stop() method to be called.                                                           |
   * ------------------------------------------------------------------------------------- */

  private def warnSparkMem(value: String): String = {
    logWarning("Using SPARK_MEM to set amount of memory to use per executor process is " +
      "deprecated, please use spark.executor.memory instead.")
    value
  }

  /** Control our logLevel. This overrides any user-defined log settings.
   * @param logLevel The desired log level as a string.
   * Valid log levels include: ALL, DEBUG, ERROR, FATAL, INFO, OFF, TRACE, WARN
   */
  def setLogLevel(logLevel: String) {
    // let's allow lowercase or mixed case too
    val upperCased = logLevel.toUpperCase(Locale.ROOT)
    require(SparkContext.VALID_LOG_LEVELS.contains(upperCased),
      s"Supplied level $logLevel did not match one of:" +
        s" ${SparkContext.VALID_LOG_LEVELS.mkString(",")}")
    Utils.setLogLevel(org.apache.log4j.Level.toLevel(upperCased))
  }

  try {
    _conf = config.clone()
    _conf.validateSettings()

    if (!_conf.contains("spark.master")) {
      throw new SparkException("A master URL must be set in your configuration")
    }
    if (!_conf.contains("spark.app.name")) {
      throw new SparkException("An application name must be set in your configuration")
    }

    // log out spark.app.name in the Spark driver logs
    logInfo(s"Submitted application: $appName")

    // System property spark.yarn.app.id must be set if user code ran by AM on a YARN cluster
    if (master == "yarn" && deployMode == "cluster" && !_conf.contains("spark.yarn.app.id")) {
      throw new SparkException("Detected yarn cluster mode, but isn't running on a cluster. " +
        "Deployment to YARN is not supported directly by SparkContext. Please use spark-submit.")
    }

    if (_conf.getBoolean("spark.logConf", false)) {
      logInfo("Spark configuration:\n" + _conf.toDebugString)
    }

    // Set Spark driver host and port system properties. This explicitly sets the configuration
    // instead of relying on the default value of the config constant.
    _conf.set(DRIVER_HOST_ADDRESS, _conf.get(DRIVER_HOST_ADDRESS))
    _conf.setIfMissing("spark.driver.port", "0")

    _conf.set("spark.executor.id", SparkContext.DRIVER_IDENTIFIER)

    _jars = Utils.getUserJars(_conf)
    _files = _conf.getOption("spark.files").map(_.split(",")).map(_.filter(_.nonEmpty))
      .toSeq.flatten

    _eventLogDir =
      if (isEventLogEnabled) {
        val unresolvedDir = conf.get("spark.eventLog.dir", EventLoggingListener.DEFAULT_LOG_DIR)
          .stripSuffix("/")
        Some(Utils.resolveURI(unresolvedDir))
      } else {
        None
      }

    _eventLogCodec = {
      val compress = _conf.getBoolean("spark.eventLog.compress", false)
      if (compress && isEventLogEnabled) {
        Some(CompressionCodec.getCodecName(_conf)).map(CompressionCodec.getShortName)
      } else {
        None
      }
    }

    _listenerBus = new LiveListenerBus(_conf)

    // Initialize the app status store and listener before SparkEnv is created so that it gets
    // all events.
    _statusStore = AppStatusStore.createLiveStore(conf)
    listenerBus.addToStatusQueue(_statusStore.listener.get)

    // Create the Spark execution environment (cache, map output tracker, etc)
    _env = createSparkEnv(_conf, isLocal, listenerBus)
    SparkEnv.set(_env)

    // If running the REPL, register the repl's output dir with the file server.
    _conf.getOption("spark.repl.class.outputDir").foreach { path =>
      val replUri = _env.rpcEnv.fileServer.addDirectory("/classes", new File(path))
      _conf.set("spark.repl.class.uri", replUri)
    }

    _statusTracker = new SparkStatusTracker(this, _statusStore)

    _progressBar =
      if (_conf.get(UI_SHOW_CONSOLE_PROGRESS) && !log.isInfoEnabled) {
        Some(new ConsoleProgressBar(this))
      } else {
        None
      }

    _ui =
      if (conf.getBoolean("spark.ui.enabled", true)) {
        Some(SparkUI.create(Some(this), _statusStore, _conf, _env.securityManager, appName, "",
          startTime))
      } else {
        // For tests, do not enable the UI
        None
      }
    // Bind the UI before starting the task scheduler to communicate
    // the bound port to the cluster manager properly
    _ui.foreach(_.bind())

    _hadoopConfiguration = SparkHadoopUtil.get.newConfiguration(_conf)

    // Add each JAR given through the constructor
    if (jars != null) {
      jars.foreach(addJar)
    }

    if (files != null) {
      files.foreach(addFile)
    }

    _executorMemory = _conf.getOption("spark.executor.memory")
      .orElse(Option(System.getenv("SPARK_EXECUTOR_MEMORY")))
      .orElse(Option(System.getenv("SPARK_MEM"))
      .map(warnSparkMem))
      .map(Utils.memoryStringToMb)
      .getOrElse(1024)

    // Convert java options to env vars as a work around
    // since we can't set env vars directly in sbt.
    for { (envKey, propKey) <- Seq(("SPARK_TESTING", "spark.testing"))
      value <- Option(System.getenv(envKey)).orElse(Option(System.getProperty(propKey)))} {
      executorEnvs(envKey) = value
    }
    Option(System.getenv("SPARK_PREPEND_CLASSES")).foreach { v =>
      executorEnvs("SPARK_PREPEND_CLASSES") = v
    }
    // The Mesos scheduler backend relies on this environment variable to set executor memory.
    // TODO: Set this only in the Mesos scheduler.
    executorEnvs("SPARK_EXECUTOR_MEMORY") = executorMemory + "m"
    executorEnvs ++= _conf.getExecutorEnv
    executorEnvs("SPARK_USER") = sparkUser

    // We need to register "HeartbeatReceiver" before "createTaskScheduler" because Executor will
    // retrieve "HeartbeatReceiver" in the constructor. (SPARK-6640)
    _heartbeatReceiver = env.rpcEnv.setupEndpoint(
      HeartbeatReceiver.ENDPOINT_NAME, new HeartbeatReceiver(this))

    // Create and start the scheduler
    val (sched, ts) = SparkContext.createTaskScheduler(this, master, deployMode)
    _schedulerBackend = sched
    _taskScheduler = ts
    _dagScheduler = new DAGScheduler(this)
    _heartbeatReceiver.ask[Boolean](TaskSchedulerIsSet)

    // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler's
    // constructor
    _taskScheduler.start()

    _applicationId = _taskScheduler.applicationId()
    _applicationAttemptId = taskScheduler.applicationAttemptId()
    _conf.set("spark.app.id", _applicationId)
    if (_conf.getBoolean("spark.ui.reverseProxy", false)) {
      System.setProperty("spark.ui.proxyBase", "/proxy/" + _applicationId)
    }
    _ui.foreach(_.setAppId(_applicationId))
    _env.blockManager.initialize(_applicationId)

    // The metrics system for Driver need to be set spark.app.id to app ID.
    // So it should start after we get app ID from the task scheduler and set spark.app.id.
    _env.metricsSystem.start()
    // Attach the driver metrics servlet handler to the web ui after the metrics system is started.
    _env.metricsSystem.getServletHandlers.foreach(handler => ui.foreach(_.attachHandler(handler)))

    _eventLogger =
      if (isEventLogEnabled) {
        val logger =
          new EventLoggingListener(_applicationId, _applicationAttemptId, _eventLogDir.get,
            _conf, _hadoopConfiguration)
        logger.start()
        listenerBus.addToEventLogQueue(logger)
        Some(logger)
      } else {
        None
      }

    // Optionally scale number of executors dynamically based on workload. Exposed for testing.
    val dynamicAllocationEnabled = Utils.isDynamicAllocationEnabled(_conf)
    _executorAllocationManager =
      if (dynamicAllocationEnabled) {
        schedulerBackend match {
          case b: ExecutorAllocationClient =>
            Some(new ExecutorAllocationManager(
              schedulerBackend.asInstanceOf[ExecutorAllocationClient], listenerBus, _conf,
              _env.blockManager.master))
          case _ =>
            None
        }
      } else {
        None
      }
    _executorAllocationManager.foreach(_.start())

    _cleaner =
      if (_conf.getBoolean("spark.cleaner.referenceTracking", true)) {
        Some(new ContextCleaner(this))
      } else {
        None
      }
    _cleaner.foreach(_.start())

    setupAndStartListenerBus()
    postEnvironmentUpdate()
    postApplicationStart()

    // Post init
    _taskScheduler.postStartHook()
    _env.metricsSystem.registerSource(_dagScheduler.metricsSource)
    _env.metricsSystem.registerSource(new BlockManagerSource(_env.blockManager))
    _executorAllocationManager.foreach { e =>
      _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
    }

    // Make sure the context is stopped if the user forgets about it. This avoids leaving
    // unfinished event logs around after the JVM exits cleanly. It doesn't help if the JVM
    // is killed, though.
    logDebug("Adding shutdown hook") // force eager creation of logger
    _shutdownHookRef = ShutdownHookManager.addShutdownHook(
      ShutdownHookManager.SPARK_CONTEXT_SHUTDOWN_PRIORITY) { () =>
      logInfo("Invoking stop() from shutdown hook")
      try {
        stop()
      } catch {
        case e: Throwable =>
          logWarning("Ignoring Exception while stopping SparkContext from shutdown hook", e)
      }
    }
  } catch {
    case NonFatal(e) =>
      logError("Error initializing SparkContext.", e)
      try {
        stop()
      } catch {
        case NonFatal(inner) =>
          logError("Error stopping SparkContext after init error.", inner)
      } finally {
        throw e
      }
  }

  /**
   * Called by the web UI to obtain executor thread dumps.  This method may be expensive.
   * Logs an error and returns None if we failed to obtain a thread dump, which could occur due
   * to an executor being dead or unresponsive or due to network issues while sending the thread
   * dump message back to the driver.
   */
  private[spark] def getExecutorThreadDump(executorId: String): Option[Array[ThreadStackTrace]] = {
    try {
      if (executorId == SparkContext.DRIVER_IDENTIFIER) {
        Some(Utils.getThreadDump())
      } else {
        val endpointRef = env.blockManager.master.getExecutorEndpointRef(executorId).get
        Some(endpointRef.askSync[Array[ThreadStackTrace]](TriggerThreadDump))
      }
    } catch {
      case e: Exception =>
        logError(s"Exception getting thread dump from executor $executorId", e)
        None
    }
  }

  private[spark] def getLocalProperties: Properties = localProperties.get()

  private[spark] def setLocalProperties(props: Properties) {
    localProperties.set(props)
  }

  /**
   * Set a local property that affects jobs submitted from this thread, such as the Spark fair
   * scheduler pool. User-defined properties may also be set here. These properties are propagated
   * through to worker tasks and can be accessed there via
   * [[org.apache.spark.TaskContext#getLocalProperty]].
   *
   * These properties are inherited by child threads spawned from this thread. This
   * may have unexpected consequences when working with thread pools. The standard java
   * implementation of thread pools have worker threads spawn other worker threads.
   * As a result, local properties may propagate unpredictably.
   */
  def setLocalProperty(key: String, value: String) {
    if (value == null) {
      localProperties.get.remove(key)
    } else {
      localProperties.get.setProperty(key, value)
    }
  }

  /**
   * Get a local property set in this thread, or null if it is missing. See
   * `org.apache.spark.SparkContext.setLocalProperty`.
   */
  def getLocalProperty(key: String): String =
    Option(localProperties.get).map(_.getProperty(key)).orNull

  /** Set a human readable description of the current job. */
  def setJobDescription(value: String) {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, value)
  }

  /**
   * Assigns a group ID to all the jobs started by this thread until the group ID is set to a
   * different value or cleared.
   *
   * Often, a unit of execution in an application consists of multiple Spark actions or jobs.
   * Application programmers can use this method to group all those jobs together and give a
   * group description. Once set, the Spark web UI will associate such jobs with this group.
   *
   * The application can also use `org.apache.spark.SparkContext.cancelJobGroup` to cancel all
   * running jobs in this group. For example,
   * {{{
   * // In the main thread:
   * sc.setJobGroup("some_job_to_cancel", "some job description")
   * sc.parallelize(1 to 10000, 2).map { i => Thread.sleep(10); i }.count()
   *
   * // In a separate thread:
   * sc.cancelJobGroup("some_job_to_cancel")
   * }}}
   *
   * @param interruptOnCancel If true, then job cancellation will result in `Thread.interrupt()`
   * being called on the job's executor threads. This is useful to help ensure that the tasks
   * are actually stopped in a timely manner, but is off by default due to HDFS-1208, where HDFS
   * may respond to Thread.interrupt() by marking nodes as dead.
   */
  def setJobGroup(groupId: String, description: String, interruptOnCancel: Boolean = false) {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, description)
    setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, groupId)
    // Note: Specifying interruptOnCancel in setJobGroup (rather than cancelJobGroup) avoids
    // changing several public APIs and allows Spark cancellations outside of the cancelJobGroup
    // APIs to also take advantage of this property (e.g., internal job failures or canceling from
    // JobProgressTab UI) on a per-job basis.
    setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, interruptOnCancel.toString)
  }

  /** Clear the current thread's job group ID and its description. */
  def clearJobGroup() {
    setLocalProperty(SparkContext.SPARK_JOB_DESCRIPTION, null)
    setLocalProperty(SparkContext.SPARK_JOB_GROUP_ID, null)
    setLocalProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, null)
  }

  /**
   * Execute a block of code in a scope such that all new RDDs created in this body will
   * be part of the same scope. For more detail, see {{org.apache.spark.rdd.RDDOperationScope}}.
   *
   * @note Return statements are NOT allowed in the given body.
   */
  private[spark] def withScope[U](body: => U): U = RDDOperationScope.withScope[U](this)(body)

  // Methods for creating RDDs

  /** Distribute a local Scala collection to form an RDD.
   *
   * @note Parallelize acts lazily. If `seq` is a mutable collection and is altered after the call
   * to parallelize and before the first action on the RDD, the resultant RDD will reflect the
   * modified collection. Pass a copy of the argument to avoid this.
   * @note avoid using `parallelize(Seq())` to create an empty `RDD`. Consider `emptyRDD` for an
   * RDD with no partitions, or `parallelize(Seq[T]())` for an RDD of `T` with empty partitions.
   * @param seq Scala collection to distribute
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed collection
   */
  def parallelize[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    assertNotStopped()
    new ParallelCollectionRDD[T](this, seq, numSlices, Map[Int, Seq[String]]())
  }

  /**
   * Creates a new RDD[Long] containing elements from `start` to `end`(exclusive), increased by
   * `step` every element.
   *
   * @note if we need to cache this RDD, we should make sure each partition does not exceed limit.
   *
   * @param start the start value.
   * @param end the end value.
   * @param step the incremental step
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed range
   */
  def range(
      start: Long,
      end: Long,
      step: Long = 1,
      numSlices: Int = defaultParallelism): RDD[Long] = withScope {
    assertNotStopped()
    // when step is 0, range will run infinitely
    require(step != 0, "step cannot be 0")
    val numElements: BigInt = {
      val safeStart = BigInt(start)
      val safeEnd = BigInt(end)
      if ((safeEnd - safeStart) % step == 0 || (safeEnd > safeStart) != (step > 0)) {
        (safeEnd - safeStart) / step
      } else {
        // the remainder has the same sign with range, could add 1 more
        (safeEnd - safeStart) / step + 1
      }
    }
    parallelize(0 until numSlices, numSlices).mapPartitionsWithIndex { (i, _) =>
      val partitionStart = (i * numElements) / numSlices * step + start
      val partitionEnd = (((i + 1) * numElements) / numSlices) * step + start
      def getSafeMargin(bi: BigInt): Long =
        if (bi.isValidLong) {
          bi.toLong
        } else if (bi > 0) {
          Long.MaxValue
        } else {
          Long.MinValue
        }
      val safePartitionStart = getSafeMargin(partitionStart)
      val safePartitionEnd = getSafeMargin(partitionEnd)

      new Iterator[Long] {
        private[this] var number: Long = safePartitionStart
        private[this] var overflow: Boolean = false

        override def hasNext =
          if (!overflow) {
            if (step > 0) {
              number < safePartitionEnd
            } else {
              number > safePartitionEnd
            }
          } else false

        override def next() = {
          val ret = number
          number += step
          if (number < ret ^ step < 0) {
            // we have Long.MaxValue + Long.MaxValue < Long.MaxValue
            // and Long.MinValue + Long.MinValue > Long.MinValue, so iff the step causes a step
            // back, we are pretty sure that we have an overflow.
            overflow = true
          }
          ret
        }
      }
    }
  }

  /** Distribute a local Scala collection to form an RDD.
   *
   * This method is identical to `parallelize`.
   * @param seq Scala collection to distribute
   * @param numSlices number of partitions to divide the collection into
   * @return RDD representing distributed collection
   */
  def makeRDD[T: ClassTag](
      seq: Seq[T],
      numSlices: Int = defaultParallelism): RDD[T] = withScope {
    parallelize(seq, numSlices)
  }

  /**
   * Distribute a local Scala collection to form an RDD, with one or more
   * location preferences (hostnames of Spark nodes) for each object.
   * Create a new partition for each collection item.
   * @param seq list of tuples of data and location preferences (hostnames of Spark nodes)
   * @return RDD representing data partitioned according to location preferences
   */
  def makeRDD[T: ClassTag](seq: Seq[(T, Seq[String])]): RDD[T] = withScope {
    assertNotStopped()
    val indexToPrefs = seq.zipWithIndex.map(t => (t._2, t._1._2)).toMap
    new ParallelCollectionRDD[T](this, seq.map(_._1), math.max(seq.size, 1), indexToPrefs)
  }

  /**
   * Read a text file from HDFS, a local file system (available on all nodes), or any
   * Hadoop-supported file system URI, and return it as an RDD of Strings.
   * @param path path to the text file on a supported file system
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of lines of the text file
   */
  def textFile(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[String] = withScope {
    assertNotStopped()
    hadoopFile(path, classOf[TextInputFormat], classOf[LongWritable], classOf[Text],
      minPartitions).map(pair => pair._2.toString).setName(path)
  }

  /**
   * Read a directory of text files from HDFS, a local file system (available on all nodes), or any
   * Hadoop-supported file system URI. Each file is read as a single record and returned in a
   * key-value pair, where the key is the path of each file, the value is the content of each file.
   *
   * <p> For example, if you have the following files:
   * {{{
   *   hdfs://a-hdfs-path/part-00000
   *   hdfs://a-hdfs-path/part-00001
   *   ...
   *   hdfs://a-hdfs-path/part-nnnnn
   * }}}
   *
   * Do `val rdd = sparkContext.wholeTextFile("hdfs://a-hdfs-path")`,
   *
   * <p> then `rdd` contains
   * {{{
   *   (a-hdfs-path/part-00000, its content)
   *   (a-hdfs-path/part-00001, its content)
   *   ...
   *   (a-hdfs-path/part-nnnnn, its content)
   * }}}
   *
   * @note Small files are preferred, large file is also allowable, but may cause bad performance.
   * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files
   *       in a directory rather than `.../path/` or `.../path`
   * @note Partitioning is determined by data locality. This may result in too few partitions
   *       by default.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param minPartitions A suggestion value of the minimal splitting number for input data.
   * @return RDD representing tuples of file path and the corresponding file content
   */
  def wholeTextFiles(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[(String, String)] = withScope {
    assertNotStopped()
    val job = NewHadoopJob.getInstance(hadoopConfiguration)
    // Use setInputPaths so that wholeTextFiles aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updateConf = job.getConfiguration
    new WholeTextFileRDD(
      this,
      classOf[WholeTextFileInputFormat],
      classOf[Text],
      classOf[Text],
      updateConf,
      minPartitions).map(record => (record._1.toString, record._2.toString)).setName(path)
  }

  /**
   * Get an RDD for a Hadoop-readable dataset as PortableDataStream for each file
   * (useful for binary data)
   *
   * For example, if you have the following files:
   * {{{
   *   hdfs://a-hdfs-path/part-00000
   *   hdfs://a-hdfs-path/part-00001
   *   ...
   *   hdfs://a-hdfs-path/part-nnnnn
   * }}}
   *
   * Do
   * `val rdd = sparkContext.binaryFiles("hdfs://a-hdfs-path")`,
   *
   * then `rdd` contains
   * {{{
   *   (a-hdfs-path/part-00000, its content)
   *   (a-hdfs-path/part-00001, its content)
   *   ...
   *   (a-hdfs-path/part-nnnnn, its content)
   * }}}
   *
   * @note Small files are preferred; very large files may cause bad performance.
   * @note On some filesystems, `.../path/&#42;` can be a more efficient way to read all files
   *       in a directory rather than `.../path/` or `.../path`
   * @note Partitioning is determined by data locality. This may result in too few partitions
   *       by default.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param minPartitions A suggestion value of the minimal splitting number for input data.
   * @return RDD representing tuples of file path and corresponding file content
   */
  def binaryFiles(
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[(String, PortableDataStream)] = withScope {
    assertNotStopped()
    val job = NewHadoopJob.getInstance(hadoopConfiguration)
    // Use setInputPaths so that binaryFiles aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updateConf = job.getConfiguration
    new BinaryFileRDD(
      this,
      classOf[StreamInputFormat],
      classOf[String],
      classOf[PortableDataStream],
      updateConf,
      minPartitions).setName(path)
  }

  /**
   * Load data from a flat binary file, assuming the length of each record is constant.
   *
   * @note We ensure that the byte array for each record in the resulting RDD
   * has the provided record length.
   *
   * @param path Directory to the input data files, the path can be comma separated paths as the
   *             list of inputs.
   * @param recordLength The length at which to split the records
   * @param conf Configuration for setting up the dataset.
   *
   * @return An RDD of data with values, represented as byte arrays
   */
  def binaryRecords(
      path: String,
      recordLength: Int,
      conf: Configuration = hadoopConfiguration): RDD[Array[Byte]] = withScope {
    assertNotStopped()
    conf.setInt(FixedLengthBinaryInputFormat.RECORD_LENGTH_PROPERTY, recordLength)
    val br = newAPIHadoopFile[LongWritable, BytesWritable, FixedLengthBinaryInputFormat](path,
      classOf[FixedLengthBinaryInputFormat],
      classOf[LongWritable],
      classOf[BytesWritable],
      conf = conf)
    br.map { case (k, v) =>
      val bytes = v.copyBytes()
      assert(bytes.length == recordLength, "Byte array does not have correct length")
      bytes
    }
  }

  /**
   * Get an RDD for a Hadoop-readable dataset from a Hadoop JobConf given its InputFormat and other
   * necessary info (e.g. file name for a filesystem-based dataset, table name for HyperTable),
   * using the older MapReduce API (`org.apache.hadoop.mapred`).
   *
   * @param conf JobConf for setting up the dataset. Note: This will be put into a Broadcast.
   *             Therefore if you plan to reuse this conf to create multiple RDDs, you need to make
   *             sure you won't modify the conf. A safe approach is always creating a new conf for
   *             a new RDD.
   * @param inputFormatClass storage format of the data to be read
   * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter
   * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter
   * @param minPartitions Minimum number of Hadoop Splits to generate.
   * @return RDD of tuples of key and corresponding value
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   */
  def hadoopRDD[K, V](
      conf: JobConf,
      inputFormatClass: Class[_ <: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(conf)

    // Add necessary security credentials to the JobConf before broadcasting it.
    SparkHadoopUtil.get.addCredentials(conf)
    new HadoopRDD(this, conf, inputFormatClass, keyClass, valueClass, minPartitions)
  }

  /** Get an RDD for a Hadoop file with an arbitrary InputFormat
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param inputFormatClass storage format of the data to be read
   * @param keyClass `Class` of the key associated with the `inputFormatClass` parameter
   * @param valueClass `Class` of the value associated with the `inputFormatClass` parameter
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V](
      path: String,
      inputFormatClass: Class[_ <: InputFormat[K, V]],
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int = defaultMinPartitions): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)

    // A Hadoop configuration can be about 10 KB, which is pretty big, so broadcast it.
    val confBroadcast = broadcast(new SerializableConfiguration(hadoopConfiguration))
    val setInputPathsFunc = (jobConf: JobConf) => FileInputFormat.setInputPaths(jobConf, path)
    new HadoopRDD(
      this,
      confBroadcast,
      Some(setInputPathsFunc),
      inputFormatClass,
      keyClass,
      valueClass,
      minPartitions).setName(path)
  }

  /**
   * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys,
   * values and the InputFormat so that users don't need to pass them directly. Instead, callers
   * can just write, for example,
   * {{{
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path, minPartitions)
   * }}}
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V, F <: InputFormat[K, V]]
      (path: String, minPartitions: Int)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    hadoopFile(path,
      fm.runtimeClass.asInstanceOf[Class[F]],
      km.runtimeClass.asInstanceOf[Class[K]],
      vm.runtimeClass.asInstanceOf[Class[V]],
      minPartitions)
  }

  /**
   * Smarter version of hadoopFile() that uses class tags to figure out the classes of keys,
   * values and the InputFormat so that users don't need to pass them directly. Instead, callers
   * can just write, for example,
   * {{{
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path)
   * }}}
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths as
   * a list of inputs
   * @return RDD of tuples of key and corresponding value
   */
  def hadoopFile[K, V, F <: InputFormat[K, V]](path: String)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    hadoopFile[K, V, F](path, defaultMinPartitions)
  }

  /**
   * Smarter version of `newApiHadoopFile` that uses class tags to figure out the classes of keys,
   * values and the `org.apache.hadoop.mapreduce.InputFormat` (new MapReduce API) so that user
   * don't need to pass them directly. Instead, callers can just write, for example:
   * ```
   * val file = sparkContext.hadoopFile[LongWritable, Text, TextInputFormat](path)
   * ```
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @return RDD of tuples of key and corresponding value
   */
  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]]
      (path: String)
      (implicit km: ClassTag[K], vm: ClassTag[V], fm: ClassTag[F]): RDD[(K, V)] = withScope {
    newAPIHadoopFile(
      path,
      fm.runtimeClass.asInstanceOf[Class[F]],
      km.runtimeClass.asInstanceOf[Class[K]],
      vm.runtimeClass.asInstanceOf[Class[V]])
  }

  /**
   * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat
   * and extra configuration options to pass to the input format.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param fClass storage format of the data to be read
   * @param kClass `Class` of the key associated with the `fClass` parameter
   * @param vClass `Class` of the value associated with the `fClass` parameter
   * @param conf Hadoop configuration
   * @return RDD of tuples of key and corresponding value
   */
  def newAPIHadoopFile[K, V, F <: NewInputFormat[K, V]](
      path: String,
      fClass: Class[F],
      kClass: Class[K],
      vClass: Class[V],
      conf: Configuration = hadoopConfiguration): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(hadoopConfiguration)

    // The call to NewHadoopJob automatically adds security credentials to conf,
    // so we don't need to explicitly add them ourselves
    val job = NewHadoopJob.getInstance(conf)
    // Use setInputPaths so that newAPIHadoopFile aligns with hadoopFile/textFile in taking
    // comma separated files as input. (see SPARK-7155)
    NewFileInputFormat.setInputPaths(job, path)
    val updatedConf = job.getConfiguration
    new NewHadoopRDD(this, fClass, kClass, vClass, updatedConf).setName(path)
  }

  /**
   * Get an RDD for a given Hadoop file with an arbitrary new API InputFormat
   * and extra configuration options to pass to the input format.
   *
   * @param conf Configuration for setting up the dataset. Note: This will be put into a Broadcast.
   *             Therefore if you plan to reuse this conf to create multiple RDDs, you need to make
   *             sure you won't modify the conf. A safe approach is always creating a new conf for
   *             a new RDD.
   * @param fClass storage format of the data to be read
   * @param kClass `Class` of the key associated with the `fClass` parameter
   * @param vClass `Class` of the value associated with the `fClass` parameter
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   */
  def newAPIHadoopRDD[K, V, F <: NewInputFormat[K, V]](
      conf: Configuration = hadoopConfiguration,
      fClass: Class[F],
      kClass: Class[K],
      vClass: Class[V]): RDD[(K, V)] = withScope {
    assertNotStopped()

    // This is a hack to enforce loading hdfs-site.xml.
    // See SPARK-11227 for details.
    FileSystem.getLocal(conf)

    // Add necessary security credentials to the JobConf. Required to access secure HDFS.
    val jconf = new JobConf(conf)
    SparkHadoopUtil.get.addCredentials(jconf)
    new NewHadoopRDD(this, fClass, kClass, vClass, jconf)
  }

  /**
   * Get an RDD for a Hadoop SequenceFile with given key and value types.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param keyClass `Class` of the key associated with `SequenceFileInputFormat`
   * @param valueClass `Class` of the value associated with `SequenceFileInputFormat`
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
  def sequenceFile[K, V](path: String,
      keyClass: Class[K],
      valueClass: Class[V],
      minPartitions: Int
      ): RDD[(K, V)] = withScope {
    assertNotStopped()
    val inputFormatClass = classOf[SequenceFileInputFormat[K, V]]
    hadoopFile(path, inputFormatClass, keyClass, valueClass, minPartitions)
  }

  /**
   * Get an RDD for a Hadoop SequenceFile with given key and value types.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param keyClass `Class` of the key associated with `SequenceFileInputFormat`
   * @param valueClass `Class` of the value associated with `SequenceFileInputFormat`
   * @return RDD of tuples of key and corresponding value
   */
  def sequenceFile[K, V](
      path: String,
      keyClass: Class[K],
      valueClass: Class[V]): RDD[(K, V)] = withScope {
    assertNotStopped()
    sequenceFile(path, keyClass, valueClass, defaultMinPartitions)
  }

  /**
   * Version of sequenceFile() for types implicitly convertible to Writables through a
   * WritableConverter. For example, to access a SequenceFile where the keys are Text and the
   * values are IntWritable, you could simply write
   * {{{
   * sparkContext.sequenceFile[String, Int](path, ...)
   * }}}
   *
   * WritableConverters are provided in a somewhat strange way (by an implicit function) to support
   * both subclasses of Writable and types for which we define a converter (e.g. Int to
   * IntWritable). The most natural thing would've been to have implicit objects for the
   * converters, but then we couldn't have an object for every subclass of Writable (you can't
   * have a parameterized singleton object). We use functions instead to create a new converter
   * for the appropriate type. In addition, we pass the converter a ClassTag of its type to
   * allow it to figure out the Writable class to use in the subclass case.
   *
   * @note Because Hadoop's RecordReader class re-uses the same Writable object for each
   * record, directly caching the returned RDD or directly passing it to an aggregation or shuffle
   * operation will create many references to the same object.
   * If you plan to directly cache, sort, or aggregate Hadoop writable objects, you should first
   * copy them using a `map` function.
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD of tuples of key and corresponding value
   */
   def sequenceFile[K, V]
       (path: String, minPartitions: Int = defaultMinPartitions)
       (implicit km: ClassTag[K], vm: ClassTag[V],
        kcf: () => WritableConverter[K], vcf: () => WritableConverter[V]): RDD[(K, V)] = {
    withScope {
      assertNotStopped()
      val kc = clean(kcf)()
      val vc = clean(vcf)()
      val format = classOf[SequenceFileInputFormat[Writable, Writable]]
      val writables = hadoopFile(path, format,
        kc.writableClass(km).asInstanceOf[Class[Writable]],
        vc.writableClass(vm).asInstanceOf[Class[Writable]], minPartitions)
      writables.map { case (k, v) => (kc.convert(k), vc.convert(v)) }
    }
  }

  /**
   * Load an RDD saved as a SequenceFile containing serialized objects, with NullWritable keys and
   * BytesWritable values that contain a serialized partition. This is still an experimental
   * storage format and may not be supported exactly as is in future Spark releases. It will also
   * be pretty slow if you use the default serializer (Java serialization),
   * though the nice thing about it is that there's very little effort required to save arbitrary
   * objects.
   *
   * @param path directory to the input data files, the path can be comma separated paths
   * as a list of inputs
   * @param minPartitions suggested minimum number of partitions for the resulting RDD
   * @return RDD representing deserialized data from the file(s)
   */
  def objectFile[T: ClassTag](
      path: String,
      minPartitions: Int = defaultMinPartitions): RDD[T] = withScope {
    assertNotStopped()
    sequenceFile(path, classOf[NullWritable], classOf[BytesWritable], minPartitions)
      .flatMap(x => Utils.deserialize[Array[T]](x._2.getBytes, Utils.getContextOrSparkClassLoader))
  }

  protected[spark] def checkpointFile[T: ClassTag](path: String): RDD[T] = withScope {
    new ReliableCheckpointRDD[T](this, path)
  }

  /** Build the union of a list of RDDs. */
  def union[T: ClassTag](rdds: Seq[RDD[T]]): RDD[T] = withScope {
    val nonEmptyRdds = rdds.filter(!_.partitions.isEmpty)
    val partitioners = nonEmptyRdds.flatMap(_.partitioner).toSet
    if (nonEmptyRdds.forall(_.partitioner.isDefined) && partitioners.size == 1) {
      new PartitionerAwareUnionRDD(this, nonEmptyRdds)
    } else {
      new UnionRDD(this, nonEmptyRdds)
    }
  }

  /** Build the union of a list of RDDs passed as variable-length arguments. */
  def union[T: ClassTag](first: RDD[T], rest: RDD[T]*): RDD[T] = withScope {
    union(Seq(first) ++ rest)
  }

  /** Get an RDD that has no partitions or elements. */
  def emptyRDD[T: ClassTag]: RDD[T] = new EmptyRDD[T](this)

  // Methods for creating shared variables

  /**
   * Create an [[org.apache.spark.Accumulator]] variable of a given type, which tasks can "add"
   * values to using the `+=` method. Only the driver can access the accumulator's `value`.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulator[T](initialValue: T)(implicit param: AccumulatorParam[T]): Accumulator[T] = {
    val acc = new Accumulator(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulator]] variable of a given type, with a name for display
   * in the Spark UI. Tasks can "add" values to the accumulator using the `+=` method. Only the
   * driver can access the accumulator's `value`.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulator[T](initialValue: T, name: String)(implicit param: AccumulatorParam[T])
    : Accumulator[T] = {
    val acc = new Accumulator(initialValue, param, Option(name))
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulable]] shared variable, to which tasks can add values
   * with `+=`. Only the driver can access the accumulable's `value`.
   * @tparam R accumulator result type
   * @tparam T type that can be added to the accumulator
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulable[R, T](initialValue: R)(implicit param: AccumulableParam[R, T])
    : Accumulable[R, T] = {
    val acc = new Accumulable(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an [[org.apache.spark.Accumulable]] shared variable, with a name for display in the
   * Spark UI. Tasks can add values to the accumulable using the `+=` operator. Only the driver can
   * access the accumulable's `value`.
   * @tparam R accumulator result type
   * @tparam T type that can be added to the accumulator
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulable[R, T](initialValue: R, name: String)(implicit param: AccumulableParam[R, T])
    : Accumulable[R, T] = {
    val acc = new Accumulable(initialValue, param, Option(name))
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Create an accumulator from a "mutable collection" type.
   *
   * Growable and TraversableOnce are the standard APIs that guarantee += and ++=, implemented by
   * standard mutable collections. So you can use this with mutable Map, Set, etc.
   */
  @deprecated("use AccumulatorV2", "2.0.0")
  def accumulableCollection[R <% Growable[T] with TraversableOnce[T] with Serializable: ClassTag, T]
      (initialValue: R): Accumulable[R, T] = {
    // TODO the context bound (<%) above should be replaced with simple type bound and implicit
    // conversion but is a breaking change. This should be fixed in Spark 3.x.
    val param = new GrowableAccumulableParam[R, T]
    val acc = new Accumulable(initialValue, param)
    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
    acc
  }

  /**
   * Register the given accumulator.
   *
   * @note Accumulators must be registered before use, or it will throw exception.
   */
  def register(acc: AccumulatorV2[_, _]): Unit = {
    acc.register(this)
  }

  /**
   * Register the given accumulator with given name.
   *
   * @note Accumulators must be registered before use, or it will throw exception.
   */
  def register(acc: AccumulatorV2[_, _], name: String): Unit = {
    acc.register(this, name = Option(name))
  }

  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator: LongAccumulator = {
    val acc = new LongAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a long accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def longAccumulator(name: String): LongAccumulator = {
    val acc = new LongAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator: DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc)
    acc
  }

  /**
   * Create and register a double accumulator, which starts with 0 and accumulates inputs by `add`.
   */
  def doubleAccumulator(name: String): DoubleAccumulator = {
    val acc = new DoubleAccumulator
    register(acc, name)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T]: CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc)
    acc
  }

  /**
   * Create and register a `CollectionAccumulator`, which starts with empty list and accumulates
   * inputs by adding them into the list.
   */
  def collectionAccumulator[T](name: String): CollectionAccumulator[T] = {
    val acc = new CollectionAccumulator[T]
    register(acc, name)
    acc
  }

  /**
   * Broadcast a read-only variable to the cluster, returning a
   * [[org.apache.spark.broadcast.Broadcast]] object for reading it in distributed functions.
   * The variable will be sent to each cluster only once.
   *
   * @param value value to broadcast to the Spark nodes
   * @return `Broadcast` object, a read-only variable cached on each machine
   */
  def broadcast[T: ClassTag](value: T): Broadcast[T] = {
    assertNotStopped()
    require(!classOf[RDD[_]].isAssignableFrom(classTag[T].runtimeClass),
      "Can not directly broadcast RDDs; instead, call collect() and broadcast the result.")
    val bc = env.broadcastManager.newBroadcast[T](value, isLocal)
    val callSite = getCallSite
    logInfo("Created broadcast " + bc.id + " from " + callSite.shortForm)
    cleaner.foreach(_.registerBroadcastForCleanup(bc))
    bc
  }

  /**
   * Add a file to be downloaded with this Spark job on every node.
   *
   * If a file is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported
   * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,
   * use `SparkFiles.get(fileName)` to find its download location.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addFile(path: String): Unit = {
    addFile(path, false)
  }

  /**
   * Returns a list of file paths that are added to resources.
   */
  def listFiles(): Seq[String] = addedFiles.keySet.toSeq

  /**
   * Add a file to be downloaded with this Spark job on every node.
   *
   * If a file is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported
   * filesystems), or an HTTP, HTTPS or FTP URI. To access the file in Spark jobs,
   * use `SparkFiles.get(fileName)` to find its download location.
   * @param recursive if true, a directory can be given in `path`. Currently directories are
   * only supported for Hadoop-supported filesystems.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addFile(path: String, recursive: Boolean): Unit = {
    val uri = new Path(path).toUri
    val schemeCorrectedPath = uri.getScheme match {
      case null => new File(path).getCanonicalFile.toURI.toString
      case "local" =>
        logWarning("File with 'local' scheme is not supported to add to file server, since " +
          "it is already available on every node.")
        return
      case _ => path
    }

    val hadoopPath = new Path(schemeCorrectedPath)
    val scheme = new URI(schemeCorrectedPath).getScheme
    if (!Array("http", "https", "ftp").contains(scheme)) {
      val fs = hadoopPath.getFileSystem(hadoopConfiguration)
      val isDir = fs.getFileStatus(hadoopPath).isDirectory
      if (!isLocal && scheme == "file" && isDir) {
        throw new SparkException(s"addFile does not support local directories when not running " +
          "local mode.")
      }
      if (!recursive && isDir) {
        throw new SparkException(s"Added file $hadoopPath is a directory and recursive is not " +
          "turned on.")
      }
    } else {
      // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies
      Utils.validateURL(uri)
    }

    val key = if (!isLocal && scheme == "file") {
      env.rpcEnv.fileServer.addFile(new File(uri.getPath))
    } else {
      schemeCorrectedPath
    }
    val timestamp = System.currentTimeMillis
    if (addedFiles.putIfAbsent(key, timestamp).isEmpty) {
      logInfo(s"Added file $path at $key with timestamp $timestamp")
      // Fetch the file locally so that closures which are run on the driver can still use the
      // SparkFiles API to access files.
      Utils.fetchFile(uri.toString, new File(SparkFiles.getRootDirectory()), conf,
        env.securityManager, hadoopConfiguration, timestamp, useCache = false)
      postEnvironmentUpdate()
    } else {
      logWarning(s"The path $path has been added already. Overwriting of added paths " +
       "is not supported in the current version.")
    }
  }

  /**
   * :: DeveloperApi ::
   * Register a listener to receive up-calls from events that happen during execution.
   */
  @DeveloperApi
  def addSparkListener(listener: SparkListenerInterface) {
    listenerBus.addToSharedQueue(listener)
  }

  /**
   * :: DeveloperApi ::
   * Deregister the listener from Spark's listener bus.
   */
  @DeveloperApi
  def removeSparkListener(listener: SparkListenerInterface): Unit = {
    listenerBus.removeListener(listener)
  }

  private[spark] def getExecutorIds(): Seq[String] = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.getExecutorIds()
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        Nil
    }
  }

  /**
   * Get the max number of tasks that can be concurrent launched currently.
   * Note that please don't cache the value returned by this method, because the number can change
   * due to add/remove executors.
   *
   * @return The max number of tasks that can be concurrent launched currently.
   */
  private[spark] def maxNumConcurrentTasks(): Int = schedulerBackend.maxNumConcurrentTasks()

  /**
   * Update the cluster manager on our scheduling needs. Three bits of information are included
   * to help it make decisions.
   * @param numExecutors The total number of executors we'd like to have. The cluster manager
   *                     shouldn't kill any running executor to reach this number, but,
   *                     if all existing executors were to die, this is the number of executors
   *                     we'd want to be allocated.
   * @param localityAwareTasks The number of tasks in all active stages that have a locality
   *                           preferences. This includes running, pending, and completed tasks.
   * @param hostToLocalTaskCount A map of hosts to the number of tasks from all active stages
   *                             that would like to like to run on that host.
   *                             This includes running, pending, and completed tasks.
   * @return whether the request is acknowledged by the cluster manager.
   */
  @DeveloperApi
  def requestTotalExecutors(
      numExecutors: Int,
      localityAwareTasks: Int,
      hostToLocalTaskCount: scala.collection.immutable.Map[String, Int]
    ): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.requestTotalExecutors(numExecutors, localityAwareTasks, hostToLocalTaskCount)
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request an additional number of executors from the cluster manager.
   * @return whether the request is received.
   */
  @DeveloperApi
  def requestExecutors(numAdditionalExecutors: Int): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.requestExecutors(numAdditionalExecutors)
      case _ =>
        logWarning("Requesting executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request that the cluster manager kill the specified executors.
   *
   * This is not supported when dynamic allocation is turned on.
   *
   * @note This is an indication to the cluster manager that the application wishes to adjust
   * its resource usage downwards. If the application wishes to replace the executors it kills
   * through this method with new ones, it should follow up explicitly with a call to
   * {{SparkContext#requestExecutors}}.
   *
   * @return whether the request is received.
   */
  @DeveloperApi
  def killExecutors(executorIds: Seq[String]): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        require(executorAllocationManager.isEmpty,
          "killExecutors() unsupported with Dynamic Allocation turned on")
        b.killExecutors(executorIds, adjustTargetNumExecutors = true, countFailures = false,
          force = true).nonEmpty
      case _ =>
        logWarning("Killing executors is not supported by current scheduler.")
        false
    }
  }

  /**
   * :: DeveloperApi ::
   * Request that the cluster manager kill the specified executor.
   *
   * @note This is an indication to the cluster manager that the application wishes to adjust
   * its resource usage downwards. If the application wishes to replace the executor it kills
   * through this method with a new one, it should follow up explicitly with a call to
   * {{SparkContext#requestExecutors}}.
   *
   * @return whether the request is received.
   */
  @DeveloperApi
  def killExecutor(executorId: String): Boolean = killExecutors(Seq(executorId))

  /**
   * Request that the cluster manager kill the specified executor without adjusting the
   * application resource requirements.
   *
   * The effect is that a new executor will be launched in place of the one killed by
   * this request. This assumes the cluster manager will automatically and eventually
   * fulfill all missing application resource requests.
   *
   * @note The replace is by no means guaranteed; another application on the same cluster
   * can steal the window of opportunity and acquire this application's resources in the
   * mean time.
   *
   * @return whether the request is received.
   */
  private[spark] def killAndReplaceExecutor(executorId: String): Boolean = {
    schedulerBackend match {
      case b: ExecutorAllocationClient =>
        b.killExecutors(Seq(executorId), adjustTargetNumExecutors = false, countFailures = true,
          force = true).nonEmpty
      case _ =>
        logWarning("Killing executors is not supported by current scheduler.")
        false
    }
  }

  /** The version of Spark on which this application is running. */
  def version: String = SPARK_VERSION

  /**
   * Return a map from the slave to the max memory available for caching and the remaining
   * memory available for caching.
   */
  def getExecutorMemoryStatus: Map[String, (Long, Long)] = {
    assertNotStopped()
    env.blockManager.master.getMemoryStatus.map { case(blockManagerId, mem) =>
      (blockManagerId.host + ":" + blockManagerId.port, mem)
    }
  }

  /**
   * :: DeveloperApi ::
   * Return information about what RDDs are cached, if they are in mem or on disk, how much space
   * they take, etc.
   */
  @DeveloperApi
  def getRDDStorageInfo: Array[RDDInfo] = {
    getRDDStorageInfo(_ => true)
  }

  private[spark] def getRDDStorageInfo(filter: RDD[_] => Boolean): Array[RDDInfo] = {
    assertNotStopped()
    val rddInfos = persistentRdds.values.filter(filter).map(RDDInfo.fromRdd).toArray
    rddInfos.foreach { rddInfo =>
      val rddId = rddInfo.id
      val rddStorageInfo = statusStore.asOption(statusStore.rdd(rddId))
      rddInfo.numCachedPartitions = rddStorageInfo.map(_.numCachedPartitions).getOrElse(0)
      rddInfo.memSize = rddStorageInfo.map(_.memoryUsed).getOrElse(0L)
      rddInfo.diskSize = rddStorageInfo.map(_.diskUsed).getOrElse(0L)
    }
    rddInfos.filter(_.isCached)
  }

  /**
   * Returns an immutable map of RDDs that have marked themselves as persistent via cache() call.
   *
   * @note This does not necessarily mean the caching or computation was successful.
   */
  def getPersistentRDDs: Map[Int, RDD[_]] = persistentRdds.toMap

  /**
   * :: DeveloperApi ::
   * Return pools for fair scheduler
   */
  @DeveloperApi
  def getAllPools: Seq[Schedulable] = {
    assertNotStopped()
    // TODO(xiajunluan): We should take nested pools into account
    taskScheduler.rootPool.schedulableQueue.asScala.toSeq
  }

  /**
   * :: DeveloperApi ::
   * Return the pool associated with the given name, if one exists
   */
  @DeveloperApi
  def getPoolForName(pool: String): Option[Schedulable] = {
    assertNotStopped()
    Option(taskScheduler.rootPool.schedulableNameToSchedulable.get(pool))
  }

  /**
   * Return current scheduling mode
   */
  def getSchedulingMode: SchedulingMode.SchedulingMode = {
    assertNotStopped()
    taskScheduler.schedulingMode
  }

  /**
   * Gets the locality information associated with the partition in a particular rdd
   * @param rdd of interest
   * @param partition to be looked up for locality
   * @return list of preferred locations for the partition
   */
  private [spark] def getPreferredLocs(rdd: RDD[_], partition: Int): Seq[TaskLocation] = {
    dagScheduler.getPreferredLocs(rdd, partition)
  }

  /**
   * Register an RDD to be persisted in memory and/or disk storage
   */
  private[spark] def persistRDD(rdd: RDD[_]) {
    persistentRdds(rdd.id) = rdd
  }

  /**
   * Unpersist an RDD from memory and/or disk storage
   */
  private[spark] def unpersistRDD(rddId: Int, blocking: Boolean = true) {
    env.blockManager.master.removeRdd(rddId, blocking)
    persistentRdds.remove(rddId)
    listenerBus.post(SparkListenerUnpersistRDD(rddId))
  }

  /**
   * Adds a JAR dependency for all tasks to be executed on this `SparkContext` in the future.
   *
   * If a jar is added during execution, it will not be available until the next TaskSet starts.
   *
   * @param path can be either a local file, a file in HDFS (or other Hadoop-supported filesystems),
   * an HTTP, HTTPS or FTP URI, or local:/path for a file on every worker node.
   *
   * @note A path can be added only once. Subsequent additions of the same path are ignored.
   */
  def addJar(path: String) {
    def addJarFile(file: File): String = {
      try {
        if (!file.exists()) {
          throw new FileNotFoundException(s"Jar ${file.getAbsolutePath} not found")
        }
        if (file.isDirectory) {
          throw new IllegalArgumentException(
            s"Directory ${file.getAbsoluteFile} is not allowed for addJar")
        }
        env.rpcEnv.fileServer.addJar(file)
      } catch {
        case NonFatal(e) =>
          logError(s"Failed to add $path to Spark environment", e)
          null
      }
    }

    if (path == null) {
      logWarning("null specified as parameter to addJar")
    } else {
      val key = if (path.contains("\\")) {
        // For local paths with backslashes on Windows, URI throws an exception
        addJarFile(new File(path))
      } else {
        val uri = new URI(path)
        // SPARK-17650: Make sure this is a valid URL before adding it to the list of dependencies
        Utils.validateURL(uri)
        uri.getScheme match {
          // A JAR file which exists only on the driver node
          case null =>
            // SPARK-22585 path without schema is not url encoded
            addJarFile(new File(uri.getRawPath))
          // A JAR file which exists only on the driver node
          case "file" => addJarFile(new File(uri.getPath))
          // A JAR file which exists locally on every worker node
          case "local" => "file:" + uri.getPath
          case _ => path
        }
      }
      if (key != null) {
        val timestamp = System.currentTimeMillis
        if (addedJars.putIfAbsent(key, timestamp).isEmpty) {
          logInfo(s"Added JAR $path at $key with timestamp $timestamp")
          postEnvironmentUpdate()
        } else {
          logWarning(s"The jar $path has been added already. Overwriting of added jars " +
            "is not supported in the current version.")
        }
      }
    }
  }

  /**
   * Returns a list of jar files that are added to resources.
   */
  def listJars(): Seq[String] = addedJars.keySet.toSeq

  /**
   * When stopping SparkContext inside Spark components, it's easy to cause dead-lock since Spark
   * may wait for some internal threads to finish. It's better to use this method to stop
   * SparkContext instead.
   */
  private[spark] def stopInNewThread(): Unit = {
    new Thread("stop-spark-context") {
      setDaemon(true)

      override def run(): Unit = {
        try {
          SparkContext.this.stop()
        } catch {
          case e: Throwable =>
            logError(e.getMessage, e)
            throw e
        }
      }
    }.start()
  }

  /**
   * Shut down the SparkContext.
   */
  def stop(): Unit = {
    if (LiveListenerBus.withinListenerThread.value) {
      throw new SparkException(s"Cannot stop SparkContext within listener bus thread.")
    }
    // Use the stopping variable to ensure no contention for the stop scenario.
    // Still track the stopped variable for use elsewhere in the code.
    if (!stopped.compareAndSet(false, true)) {
      logInfo("SparkContext already stopped.")
      return
    }
    if (_shutdownHookRef != null) {
      ShutdownHookManager.removeShutdownHook(_shutdownHookRef)
    }

    Utils.tryLogNonFatalError {
      postApplicationEnd()
    }
    Utils.tryLogNonFatalError {
      _ui.foreach(_.stop())
    }
    if (env != null) {
      Utils.tryLogNonFatalError {
        env.metricsSystem.report()
      }
    }
    Utils.tryLogNonFatalError {
      _cleaner.foreach(_.stop())
    }
    Utils.tryLogNonFatalError {
      _executorAllocationManager.foreach(_.stop())
    }
    if (_dagScheduler != null) {
      Utils.tryLogNonFatalError {
        _dagScheduler.stop()
      }
      _dagScheduler = null
    }
    if (_listenerBusStarted) {
      Utils.tryLogNonFatalError {
        listenerBus.stop()
        _listenerBusStarted = false
      }
    }
    Utils.tryLogNonFatalError {
      _eventLogger.foreach(_.stop())
    }
    if (env != null && _heartbeatReceiver != null) {
      Utils.tryLogNonFatalError {
        env.rpcEnv.stop(_heartbeatReceiver)
      }
    }
    Utils.tryLogNonFatalError {
      _progressBar.foreach(_.stop())
    }
    _taskScheduler = null
    // TODO: Cache.stop()?
    if (_env != null) {
      Utils.tryLogNonFatalError {
        _env.stop()
      }
      SparkEnv.set(null)
    }
    if (_statusStore != null) {
      _statusStore.close()
    }
    // Clear this `InheritableThreadLocal`, or it will still be inherited in child threads even this
    // `SparkContext` is stopped.
    localProperties.remove()
    // Unset YARN mode system env variable, to allow switching between cluster types.
    SparkContext.clearActiveContext()
    logInfo("Successfully stopped SparkContext")
  }


  /**
   * Get Spark's home location from either a value set through the constructor,
   * or the spark.home Java property, or the SPARK_HOME environment variable
   * (in that order of preference). If neither of these is set, return None.
   */
  private[spark] def getSparkHome(): Option[String] = {
    conf.getOption("spark.home").orElse(Option(System.getenv("SPARK_HOME")))
  }

  /**
   * Set the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  def setCallSite(shortCallSite: String) {
    setLocalProperty(CallSite.SHORT_FORM, shortCallSite)
  }

  /**
   * Set the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  private[spark] def setCallSite(callSite: CallSite) {
    setLocalProperty(CallSite.SHORT_FORM, callSite.shortForm)
    setLocalProperty(CallSite.LONG_FORM, callSite.longForm)
  }

  /**
   * Clear the thread-local property for overriding the call sites
   * of actions and RDDs.
   */
  def clearCallSite() {
    setLocalProperty(CallSite.SHORT_FORM, null)
    setLocalProperty(CallSite.LONG_FORM, null)
  }

  /**
   * Capture the current user callsite and return a formatted version for printing. If the user
   * has overridden the call site using `setCallSite()`, this will return the user's version.
   */
  private[spark] def getCallSite(): CallSite = {
    lazy val callSite = Utils.getCallSite()
    CallSite(
      Option(getLocalProperty(CallSite.SHORT_FORM)).getOrElse(callSite.shortForm),
      Option(getLocalProperty(CallSite.LONG_FORM)).getOrElse(callSite.longForm)
    )
  }

  /**
   * Run a function on a given set of partitions in an RDD and pass the results to the given
   * handler function. This is the main entry point for all actions in Spark.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit): Unit = {
    if (stopped.get()) {
      throw new IllegalStateException("SparkContext has been shutdown")
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo("Starting job: " + callSite.shortForm)
    if (conf.getBoolean("spark.logLineage", false)) {
      logInfo("RDD's recursive dependencies:\n" + rdd.toDebugString)
    }
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)
    progressBar.foreach(_.finishAll())
    rdd.doCheckpoint()
  }

  /**
   * Run a function on a given set of partitions in an RDD and return the results as an array.
   * The function that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int]): Array[U] = {
    val results = new Array[U](partitions.size)
    runJob[T, U](rdd, func, partitions, (index, res) => results(index) = res)
    results
  }

  /**
   * Run a function on a given set of partitions in an RDD and return the results as an array.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: Iterator[T] => U,
      partitions: Seq[Int]): Array[U] = {
    val cleanedFunc = clean(func)
    runJob(rdd, (ctx: TaskContext, it: Iterator[T]) => cleanedFunc(it), partitions)
  }

  /**
   * Run a job on all partitions in an RDD and return the results in an array. The function
   * that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](rdd: RDD[T], func: (TaskContext, Iterator[T]) => U): Array[U] = {
    runJob(rdd, func, 0 until rdd.partitions.length)
  }

  /**
   * Run a job on all partitions in an RDD and return the results in an array.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @return in-memory collection with a result of the job (each collection element will contain
   * a result from one partition)
   */
  def runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] => U): Array[U] = {
    runJob(rdd, func, 0 until rdd.partitions.length)
  }

  /**
   * Run a job on all partitions in an RDD and pass the results to a handler function. The function
   * that is run against each partition additionally takes `TaskContext` argument.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
    rdd: RDD[T],
    processPartition: (TaskContext, Iterator[T]) => U,
    resultHandler: (Int, U) => Unit)
  {
    runJob[T, U](rdd, processPartition, 0 until rdd.partitions.length, resultHandler)
  }

  /**
   * Run a job on all partitions in an RDD and pass the results to a handler function.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param resultHandler callback to pass each result to
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      processPartition: Iterator[T] => U,
      resultHandler: (Int, U) => Unit)
  {
    val processFunc = (context: TaskContext, iter: Iterator[T]) => processPartition(iter)
    runJob[T, U](rdd, processFunc, 0 until rdd.partitions.length, resultHandler)
  }

  /**
   * :: DeveloperApi ::
   * Run a job that can return approximate results.
   *
   * @param rdd target RDD to run tasks on
   * @param func a function to run on each partition of the RDD
   * @param evaluator `ApproximateEvaluator` to receive the partial results
   * @param timeout maximum time to wait for the job, in milliseconds
   * @return partial result (how partial depends on whether the job was finished before or
   * after timeout)
   */
  @DeveloperApi
  def runApproximateJob[T, U, R](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      evaluator: ApproximateEvaluator[U, R],
      timeout: Long): PartialResult[R] = {
    assertNotStopped()
    val callSite = getCallSite
    logInfo("Starting job: " + callSite.shortForm)
    val start = System.nanoTime
    val cleanedFunc = clean(func)
    val result = dagScheduler.runApproximateJob(rdd, cleanedFunc, evaluator, callSite, timeout,
      localProperties.get)
    logInfo(
      "Job finished: " + callSite.shortForm + ", took " + (System.nanoTime - start) / 1e9 + " s")
    result
  }

  /**
   * Submit a job for execution and return a FutureJob holding the result.
   *
   * @param rdd target RDD to run tasks on
   * @param processPartition a function to run on each partition of the RDD
   * @param partitions set of partitions to run on; some jobs may not want to compute on all
   * partitions of the target RDD, e.g. for operations like `first()`
   * @param resultHandler callback to pass each result to
   * @param resultFunc function to be executed when the result is ready
   */
  def submitJob[T, U, R](
      rdd: RDD[T],
      processPartition: Iterator[T] => U,
      partitions: Seq[Int],
      resultHandler: (Int, U) => Unit,
      resultFunc: => R): SimpleFutureAction[R] =
  {
    assertNotStopped()
    val cleanF = clean(processPartition)
    val callSite = getCallSite
    val waiter = dagScheduler.submitJob(
      rdd,
      (context: TaskContext, iter: Iterator[T]) => cleanF(iter),
      partitions,
      callSite,
      resultHandler,
      localProperties.get)
    new SimpleFutureAction(waiter, resultFunc)
  }

  /**
   * Submit a map stage for execution. This is currently an internal API only, but might be
   * promoted to DeveloperApi in the future.
   */
  private[spark] def submitMapStage[K, V, C](dependency: ShuffleDependency[K, V, C])
      : SimpleFutureAction[MapOutputStatistics] = {
    assertNotStopped()
    val callSite = getCallSite()
    var result: MapOutputStatistics = null
    val waiter = dagScheduler.submitMapStage(
      dependency,
      (r: MapOutputStatistics) => { result = r },
      callSite,
      localProperties.get)
    new SimpleFutureAction[MapOutputStatistics](waiter, result)
  }

  /**
   * Cancel active jobs for the specified group. See `org.apache.spark.SparkContext.setJobGroup`
   * for more information.
   */
  def cancelJobGroup(groupId: String) {
    assertNotStopped()
    dagScheduler.cancelJobGroup(groupId)
  }

  /** Cancel all jobs that have been scheduled or are running.  */
  def cancelAllJobs() {
    assertNotStopped()
    dagScheduler.cancelAllJobs()
  }

  /**
   * Cancel a given job if it's scheduled or running.
   *
   * @param jobId the job ID to cancel
   * @param reason optional reason for cancellation
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelJob(jobId: Int, reason: String): Unit = {
    dagScheduler.cancelJob(jobId, Option(reason))
  }

  /**
   * Cancel a given job if it's scheduled or running.
   *
   * @param jobId the job ID to cancel
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelJob(jobId: Int): Unit = {
    dagScheduler.cancelJob(jobId, None)
  }

  /**
   * Cancel a given stage and all jobs associated with it.
   *
   * @param stageId the stage ID to cancel
   * @param reason reason for cancellation
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelStage(stageId: Int, reason: String): Unit = {
    dagScheduler.cancelStage(stageId, Option(reason))
  }

  /**
   * Cancel a given stage and all jobs associated with it.
   *
   * @param stageId the stage ID to cancel
   * @note Throws `InterruptedException` if the cancel message cannot be sent
   */
  def cancelStage(stageId: Int): Unit = {
    dagScheduler.cancelStage(stageId, None)
  }

  /**
   * Kill and reschedule the given task attempt. Task ids can be obtained from the Spark UI
   * or through SparkListener.onTaskStart.
   *
   * @param taskId the task ID to kill. This id uniquely identifies the task attempt.
   * @param interruptThread whether to interrupt the thread running the task.
   * @param reason the reason for killing the task, which should be a short string. If a task
   *   is killed multiple times with different reasons, only one reason will be reported.
   *
   * @return Whether the task was successfully killed.
   */
  def killTaskAttempt(
      taskId: Long,
      interruptThread: Boolean = true,
      reason: String = "killed via SparkContext.killTaskAttempt"): Boolean = {
    dagScheduler.killTaskAttempt(taskId, interruptThread, reason)
  }

  /**
   * Clean a closure to make it ready to be serialized and sent to tasks
   * (removes unreferenced variables in $outer's, updates REPL variables)
   * If <tt>checkSerializable</tt> is set, <tt>clean</tt> will also proactively
   * check to see if <tt>f</tt> is serializable and throw a <tt>SparkException</tt>
   * if not.
   *
   * @param f the closure to clean
   * @param checkSerializable whether or not to immediately check <tt>f</tt> for serializability
   * @throws SparkException if <tt>checkSerializable</tt> is set but <tt>f</tt> is not
   *   serializable
   * @return the cleaned closure
   */
  private[spark] def clean[F <: AnyRef](f: F, checkSerializable: Boolean = true): F = {
    ClosureCleaner.clean(f, checkSerializable)
    f
  }

  /**
   * Set the directory under which RDDs are going to be checkpointed.
   * @param directory path to the directory where checkpoint files will be stored
   * (must be HDFS path if running in cluster)
   */
  def setCheckpointDir(directory: String) {

    // If we are running on a cluster, log a warning if the directory is local.
    // Otherwise, the driver may attempt to reconstruct the checkpointed RDD from
    // its own local file system, which is incorrect because the checkpoint files
    // are actually on the executor machines.
    if (!isLocal && Utils.nonLocalPaths(directory).isEmpty) {
      logWarning("Spark is not running in local mode, therefore the checkpoint directory " +
        s"must not be on the local filesystem. Directory '$directory' " +
        "appears to be on the local filesystem.")
    }

    checkpointDir = Option(directory).map { dir =>
      val path = new Path(dir, UUID.randomUUID().toString)
      val fs = path.getFileSystem(hadoopConfiguration)
      fs.mkdirs(path)
      fs.getFileStatus(path).getPath.toString
    }
  }

  def getCheckpointDir: Option[String] = checkpointDir

  /** Default level of parallelism to use when not given by user (e.g. parallelize and makeRDD). */
  def defaultParallelism: Int = {
    assertNotStopped()
    taskScheduler.defaultParallelism
  }

  /**
   * Default min number of partitions for Hadoop RDDs when not given by user
   * Notice that we use math.min so the "defaultMinPartitions" cannot be higher than 2.
   * The reasons for this are discussed in https://github.com/mesos/spark/pull/718
   */
  def defaultMinPartitions: Int = math.min(defaultParallelism, 2)

  private val nextShuffleId = new AtomicInteger(0)

  private[spark] def newShuffleId(): Int = nextShuffleId.getAndIncrement()

  private val nextRddId = new AtomicInteger(0)

  /** Register a new RDD, returning its RDD ID */
  private[spark] def newRddId(): Int = nextRddId.getAndIncrement()

  /**
   * Registers listeners specified in spark.extraListeners, then starts the listener bus.
   * This should be called after all internal listeners have been registered with the listener bus
   * (e.g. after the web UI and event logging listeners have been registered).
   */
  private def setupAndStartListenerBus(): Unit = {
    try {
      conf.get(EXTRA_LISTENERS).foreach { classNames =>
        val listeners = Utils.loadExtensions(classOf[SparkListenerInterface], classNames, conf)
        listeners.foreach { listener =>
          listenerBus.addToSharedQueue(listener)
          logInfo(s"Registered listener ${listener.getClass().getName()}")
        }
      }
    } catch {
      case e: Exception =>
        try {
          stop()
        } finally {
          throw new SparkException(s"Exception when registering SparkListener", e)
        }
    }

    listenerBus.start(this, _env.metricsSystem)
    _listenerBusStarted = true
  }

  /** Post the application start event */
  private def postApplicationStart() {
    // Note: this code assumes that the task scheduler has been initialized and has contacted
    // the cluster manager to get an application ID (in case the cluster manager provides one).
    listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
      startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
  }

  /** Post the application end event */
  private def postApplicationEnd() {
    listenerBus.post(SparkListenerApplicationEnd(System.currentTimeMillis))
  }

  /** Post the environment update event once the task scheduler is ready */
  private def postEnvironmentUpdate() {
    if (taskScheduler != null) {
      val schedulingMode = getSchedulingMode.toString
      val addedJarPaths = addedJars.keys.toSeq
      val addedFilePaths = addedFiles.keys.toSeq
      val environmentDetails = SparkEnv.environmentDetails(conf, schedulingMode, addedJarPaths,
        addedFilePaths)
      val environmentUpdate = SparkListenerEnvironmentUpdate(environmentDetails)
      listenerBus.post(environmentUpdate)
    }
  }

  // In order to prevent multiple SparkContexts from being active at the same time, mark this
  // context as having finished construction.
  // NOTE: this must be placed at the end of the SparkContext constructor.
  SparkContext.setActiveContext(this, allowMultipleContexts)
}

/**
 * The SparkContext object contains a number of implicit conversions and parameters for use with
 * various Spark features.
 */
object SparkContext extends Logging {
  private val VALID_LOG_LEVELS =
    Set("ALL", "DEBUG", "ERROR", "FATAL", "INFO", "OFF", "TRACE", "WARN")

  /**
   * Lock that guards access to global variables that track SparkContext construction.
   */
  private val SPARK_CONTEXT_CONSTRUCTOR_LOCK = new Object()

  /**
   * The active, fully-constructed SparkContext.  If no SparkContext is active, then this is `null`.
   *
   * Access to this field is guarded by SPARK_CONTEXT_CONSTRUCTOR_LOCK.
   */
  private val activeContext: AtomicReference[SparkContext] =
    new AtomicReference[SparkContext](null)

  /**
   * Points to a partially-constructed SparkContext if some thread is in the SparkContext
   * constructor, or `None` if no SparkContext is being constructed.
   *
   * Access to this field is guarded by SPARK_CONTEXT_CONSTRUCTOR_LOCK
   */
  private var contextBeingConstructed: Option[SparkContext] = None

  /**
   * Called to ensure that no other SparkContext is running in this JVM.
   *
   * Throws an exception if a running context is detected and logs a warning if another thread is
   * constructing a SparkContext.  This warning is necessary because the current locking scheme
   * prevents us from reliably distinguishing between cases where another context is being
   * constructed and cases where another constructor threw an exception.
   */
  private def assertNoOtherContextIsRunning(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      Option(activeContext.get()).filter(_ ne sc).foreach { ctx =>
          val errMsg = "Only one SparkContext may be running in this JVM (see SPARK-2243)." +
            " To ignore this error, set spark.driver.allowMultipleContexts = true. " +
            s"The currently running SparkContext was created at:\n${ctx.creationSite.longForm}"
          val exception = new SparkException(errMsg)
          if (allowMultipleContexts) {
            logWarning("Multiple running SparkContexts detected in the same JVM!", exception)
          } else {
            throw exception
          }
        }

      contextBeingConstructed.filter(_ ne sc).foreach { otherContext =>
        // Since otherContext might point to a partially-constructed context, guard against
        // its creationSite field being null:
        val otherContextCreationSite =
          Option(otherContext.creationSite).map(_.longForm).getOrElse("unknown location")
        val warnMsg = "Another SparkContext is being constructed (or threw an exception in its" +
          " constructor).  This may indicate an error, since only one SparkContext may be" +
          " running in this JVM (see SPARK-2243)." +
          s" The other SparkContext was created at:\n$otherContextCreationSite"
        logWarning(warnMsg)
      }
    }
  }

  /**
   * This function may be used to get or instantiate a SparkContext and register it as a
   * singleton object. Because we can only have one active SparkContext per JVM,
   * this is useful when applications may wish to share a SparkContext.
   *
   * @note This function cannot be used to create multiple SparkContext instances
   * even if multiple contexts are allowed.
   * @param config `SparkConfig` that will be used for initialisation of the `SparkContext`
   * @return current `SparkContext` (or a new one if it wasn't created before the function call)
   */
  def getOrCreate(config: SparkConf): SparkContext = {
    // Synchronize to ensure that multiple create requests don't trigger an exception
    // from assertNoOtherContextIsRunning within setActiveContext
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      if (activeContext.get() == null) {
        setActiveContext(new SparkContext(config), allowMultipleContexts = false)
      } else {
        if (config.getAll.nonEmpty) {
          logWarning("Using an existing SparkContext; some configuration may not take effect.")
        }
      }
      activeContext.get()
    }
  }

  /**
   * This function may be used to get or instantiate a SparkContext and register it as a
   * singleton object. Because we can only have one active SparkContext per JVM,
   * this is useful when applications may wish to share a SparkContext.
   *
   * This method allows not passing a SparkConf (useful if just retrieving).
   *
   * @note This function cannot be used to create multiple SparkContext instances
   * even if multiple contexts are allowed.
   * @return current `SparkContext` (or a new one if wasn't created before the function call)
   */
  def getOrCreate(): SparkContext = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      if (activeContext.get() == null) {
        setActiveContext(new SparkContext(), allowMultipleContexts = false)
      }
      activeContext.get()
    }
  }

  /** Return the current active [[SparkContext]] if any. */
  private[spark] def getActive: Option[SparkContext] = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      Option(activeContext.get())
    }
  }

  /**
   * Called at the beginning of the SparkContext constructor to ensure that no SparkContext is
   * running.  Throws an exception if a running context is detected and logs a warning if another
   * thread is constructing a SparkContext.  This warning is necessary because the current locking
   * scheme prevents us from reliably distinguishing between cases where another context is being
   * constructed and cases where another constructor threw an exception.
   */
  private[spark] def markPartiallyConstructed(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      assertNoOtherContextIsRunning(sc, allowMultipleContexts)
      contextBeingConstructed = Some(sc)
    }
  }

  /**
   * Called at the end of the SparkContext constructor to ensure that no other SparkContext has
   * raced with this constructor and started.
   */
  private[spark] def setActiveContext(
      sc: SparkContext,
      allowMultipleContexts: Boolean): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      assertNoOtherContextIsRunning(sc, allowMultipleContexts)
      contextBeingConstructed = None
      activeContext.set(sc)
    }
  }

  /**
   * Clears the active SparkContext metadata.  This is called by `SparkContext#stop()`.  It's
   * also called in unit tests to prevent a flood of warnings from test suites that don't / can't
   * properly clean up their SparkContexts.
   */
  private[spark] def clearActiveContext(): Unit = {
    SPARK_CONTEXT_CONSTRUCTOR_LOCK.synchronized {
      activeContext.set(null)
    }
  }

  private[spark] val SPARK_JOB_DESCRIPTION = "spark.job.description"
  private[spark] val SPARK_JOB_GROUP_ID = "spark.jobGroup.id"
  private[spark] val SPARK_JOB_INTERRUPT_ON_CANCEL = "spark.job.interruptOnCancel"
  private[spark] val RDD_SCOPE_KEY = "spark.rdd.scope"
  private[spark] val RDD_SCOPE_NO_OVERRIDE_KEY = "spark.rdd.scope.noOverride"

  /**
   * Executor id for the driver.  In earlier versions of Spark, this was `<driver>`, but this was
   * changed to `driver` because the angle brackets caused escaping issues in URLs and XML (see
   * SPARK-6716 for more details).
   */
  private[spark] val DRIVER_IDENTIFIER = "driver"

  /**
   * Legacy version of DRIVER_IDENTIFIER, retained for backwards-compatibility.
   */
  private[spark] val LEGACY_DRIVER_IDENTIFIER = "<driver>"

  private implicit def arrayToArrayWritable[T <: Writable : ClassTag](arr: Traversable[T])
    : ArrayWritable = {
    def anyToWritable[U <: Writable](u: U): Writable = u

    new ArrayWritable(classTag[T].runtimeClass.asInstanceOf[Class[Writable]],
        arr.map(x => anyToWritable(x)).toArray)
  }

  /**
   * Find the JAR from which a given class was loaded, to make it easy for users to pass
   * their JARs to SparkContext.
   *
   * @param cls class that should be inside of the jar
   * @return jar that contains the Class, `None` if not found
   */
  def jarOfClass(cls: Class[_]): Option[String] = {
    val uri = cls.getResource("/" + cls.getName.replace('.', '/') + ".class")
    if (uri != null) {
      val uriStr = uri.toString
      if (uriStr.startsWith("jar:file:")) {
        // URI will be of the form "jar:file:/path/foo.jar!/package/cls.class",
        // so pull out the /path/foo.jar
        Some(uriStr.substring("jar:file:".length, uriStr.indexOf('!')))
      } else {
        None
      }
    } else {
      None
    }
  }

  /**
   * Find the JAR that contains the class of a particular object, to make it easy for users
   * to pass their JARs to SparkContext. In most cases you can call jarOfObject(this) in
   * your driver program.
   *
   * @param obj reference to an instance which class should be inside of the jar
   * @return jar that contains the class of the instance, `None` if not found
   */
  def jarOfObject(obj: AnyRef): Option[String] = jarOfClass(obj.getClass)

  /**
   * Creates a modified version of a SparkConf with the parameters that can be passed separately
   * to SparkContext, to make it easier to write SparkContext's constructors. This ignores
   * parameters that are passed as the default value of null, instead of throwing an exception
   * like SparkConf would.
   */
  private[spark] def updatedConf(
      conf: SparkConf,
      master: String,
      appName: String,
      sparkHome: String = null,
      jars: Seq[String] = Nil,
      environment: Map[String, String] = Map()): SparkConf =
  {
    val res = conf.clone()
    res.setMaster(master)
    res.setAppName(appName)
    if (sparkHome != null) {
      res.setSparkHome(sparkHome)
    }
    if (jars != null && !jars.isEmpty) {
      res.setJars(jars)
    }
    res.setExecutorEnv(environment.toSeq)
    res
  }

  /**
   * The number of cores available to the driver to use for tasks such as I/O with Netty
   */
  private[spark] def numDriverCores(master: String): Int = {
    numDriverCores(master, null)
  }

  /**
   * The number of cores available to the driver to use for tasks such as I/O with Netty
   */
  private[spark] def numDriverCores(master: String, conf: SparkConf): Int = {
    def convertToInt(threads: String): Int = {
      if (threads == "*") Runtime.getRuntime.availableProcessors() else threads.toInt
    }
    master match {
      case "local" => 1
      case SparkMasterRegex.LOCAL_N_REGEX(threads) => convertToInt(threads)
      case SparkMasterRegex.LOCAL_N_FAILURES_REGEX(threads, _) => convertToInt(threads)
      case "yarn" =>
        if (conf != null && conf.getOption("spark.submit.deployMode").contains("cluster")) {
          conf.getInt("spark.driver.cores", 0)
        } else {
          0
        }
      case _ => 0 // Either driver is not being used, or its core count will be interpolated later
    }
  }

  /**
   * Create a task scheduler based on a given master URL.
   * Return a 2-tuple of the scheduler backend and the task scheduler.
   */
  private def createTaskScheduler(
      sc: SparkContext,
      master: String,
      deployMode: String): (SchedulerBackend, TaskScheduler) = {
    import SparkMasterRegex._

    // When running locally, don't try to re-execute tasks on failure.
    val MAX_LOCAL_TASK_FAILURES = 1

    master match {
      case "local" =>
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, 1)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_REGEX(threads) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*] estimates the number of cores on the machine; local[N] uses exactly N threads.
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        if (threadCount <= 0) {
          throw new SparkException(s"Asked to run locally with $threadCount threads")
        }
        val scheduler = new TaskSchedulerImpl(sc, MAX_LOCAL_TASK_FAILURES, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_N_FAILURES_REGEX(threads, maxFailures) =>
        def localCpuCount: Int = Runtime.getRuntime.availableProcessors()
        // local[*, M] means the number of cores on the computer with M failures
        // local[N, M] means exactly N threads with M failures
        val threadCount = if (threads == "*") localCpuCount else threads.toInt
        val scheduler = new TaskSchedulerImpl(sc, maxFailures.toInt, isLocal = true)
        val backend = new LocalSchedulerBackend(sc.getConf, scheduler, threadCount)
        scheduler.initialize(backend)
        (backend, scheduler)

      case SPARK_REGEX(sparkUrl) =>
        val scheduler = new TaskSchedulerImpl(sc)
        val masterUrls = sparkUrl.split(",").map("spark://" + _)
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        (backend, scheduler)

      case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
        // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
        val memoryPerSlaveInt = memoryPerSlave.toInt
        if (sc.executorMemory > memoryPerSlaveInt) {
          throw new SparkException(
            "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
              memoryPerSlaveInt, sc.executorMemory))
        }

        val scheduler = new TaskSchedulerImpl(sc)
        val localCluster = new LocalSparkCluster(
          numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt, sc.conf)
        val masterUrls = localCluster.start()
        val backend = new StandaloneSchedulerBackend(scheduler, sc, masterUrls)
        scheduler.initialize(backend)
        backend.shutdownCallback = (backend: StandaloneSchedulerBackend) => {
          localCluster.stop()
        }
        (backend, scheduler)

      case masterUrl =>
        val cm = getClusterManager(masterUrl) match {
          case Some(clusterMgr) => clusterMgr
          case None => throw new SparkException("Could not parse Master URL: '" + master + "'")
        }
        try {
          val scheduler = cm.createTaskScheduler(sc, masterUrl)
          val backend = cm.createSchedulerBackend(sc, masterUrl, scheduler)
          cm.initialize(scheduler, backend)
          (backend, scheduler)
        } catch {
          case se: SparkException => throw se
          case NonFatal(e) =>
            throw new SparkException("External scheduler cannot be instantiated", e)
        }
    }
  }

  private def getClusterManager(url: String): Option[ExternalClusterManager] = {
    val loader = Utils.getContextOrSparkClassLoader
    val serviceLoaders =
      ServiceLoader.load(classOf[ExternalClusterManager], loader).asScala.filter(_.canCreate(url))
    if (serviceLoaders.size > 1) {
      throw new SparkException(
        s"Multiple external cluster managers registered for the url $url: $serviceLoaders")
    }
    serviceLoaders.headOption
  }
}

/**
 * A collection of regexes for extracting information from the master string.
 */
private object SparkMasterRegex {
  // Regular expression used for local[N] and local[*] master formats
  val LOCAL_N_REGEX = """local\[([0-9]+|\*)\]""".r
  // Regular expression for local[N, maxRetries], used in tests with failing tasks
  val LOCAL_N_FAILURES_REGEX = """local\[([0-9]+|\*)\s*,\s*([0-9]+)\]""".r
  // Regular expression for simulating a Spark cluster of [N, cores, memory] locally
  val LOCAL_CLUSTER_REGEX = """local-cluster\[\s*([0-9]+)\s*,\s*([0-9]+)\s*,\s*([0-9]+)\s*]""".r
  // Regular expression for connecting to Spark deploy clusters
  val SPARK_REGEX = """spark://(.*)""".r
}

/**
 * A class encapsulating how to convert some type `T` from `Writable`. It stores both the `Writable`
 * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the
 * conversion.
 * The getter for the writable class takes a `ClassTag[T]` in case this is a generic object
 * that doesn't know the type of `T` when it is created. This sounds strange but is necessary to
 * support converting subclasses of `Writable` to themselves (`writableWritableConverter()`).
 */
private[spark] class WritableConverter[T](
    val writableClass: ClassTag[T] => Class[_ <: Writable],
    val convert: Writable => T)
  extends Serializable

object WritableConverter {

  // Helper objects for converting common types to Writable
  private[spark] def simpleWritableConverter[T, W <: Writable: ClassTag](convert: W => T)
  : WritableConverter[T] = {
    val wClass = classTag[W].runtimeClass.asInstanceOf[Class[W]]
    new WritableConverter[T](_ => wClass, x => convert(x.asInstanceOf[W]))
  }

  // The following implicit functions were in SparkContext before 1.3 and users had to
  // `import SparkContext._` to enable them. Now we move them here to make the compiler find
  // them automatically. However, we still keep the old functions in SparkContext for backward
  // compatibility and forward to the following functions directly.

  // The following implicit declarations have been added on top of the very similar ones
  // below in order to enable compatibility with Scala 2.12. Scala 2.12 deprecates eta
  // expansion of zero-arg methods and thus won't match a no-arg method where it expects
  // an implicit that is a function of no args.

  implicit val intWritableConverterFn: () => WritableConverter[Int] =
    () => simpleWritableConverter[Int, IntWritable](_.get)

  implicit val longWritableConverterFn: () => WritableConverter[Long] =
    () => simpleWritableConverter[Long, LongWritable](_.get)

  implicit val doubleWritableConverterFn: () => WritableConverter[Double] =
    () => simpleWritableConverter[Double, DoubleWritable](_.get)

  implicit val floatWritableConverterFn: () => WritableConverter[Float] =
    () => simpleWritableConverter[Float, FloatWritable](_.get)

  implicit val booleanWritableConverterFn: () => WritableConverter[Boolean] =
    () => simpleWritableConverter[Boolean, BooleanWritable](_.get)

  implicit val bytesWritableConverterFn: () => WritableConverter[Array[Byte]] = {
    () => simpleWritableConverter[Array[Byte], BytesWritable] { bw =>
      // getBytes method returns array which is longer then data to be returned
      Arrays.copyOfRange(bw.getBytes, 0, bw.getLength)
    }
  }

  implicit val stringWritableConverterFn: () => WritableConverter[String] =
    () => simpleWritableConverter[String, Text](_.toString)

  implicit def writableWritableConverterFn[T <: Writable : ClassTag]: () => WritableConverter[T] =
    () => new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T])

  // These implicits remain included for backwards-compatibility. They fulfill the
  // same role as those above.

  implicit def intWritableConverter(): WritableConverter[Int] =
    simpleWritableConverter[Int, IntWritable](_.get)

  implicit def longWritableConverter(): WritableConverter[Long] =
    simpleWritableConverter[Long, LongWritable](_.get)

  implicit def doubleWritableConverter(): WritableConverter[Double] =
    simpleWritableConverter[Double, DoubleWritable](_.get)

  implicit def floatWritableConverter(): WritableConverter[Float] =
    simpleWritableConverter[Float, FloatWritable](_.get)

  implicit def booleanWritableConverter(): WritableConverter[Boolean] =
    simpleWritableConverter[Boolean, BooleanWritable](_.get)

  implicit def bytesWritableConverter(): WritableConverter[Array[Byte]] = {
    simpleWritableConverter[Array[Byte], BytesWritable] { bw =>
      // getBytes method returns array which is longer then data to be returned
      Arrays.copyOfRange(bw.getBytes, 0, bw.getLength)
    }
  }

  implicit def stringWritableConverter(): WritableConverter[String] =
    simpleWritableConverter[String, Text](_.toString)

  implicit def writableWritableConverter[T <: Writable](): WritableConverter[T] =
    new WritableConverter[T](_.runtimeClass.asInstanceOf[Class[T]], _.asInstanceOf[T])
}

/**
 * A class encapsulating how to convert some type `T` to `Writable`. It stores both the `Writable`
 * class corresponding to `T` (e.g. `IntWritable` for `Int`) and a function for doing the
 * conversion.
 * The `Writable` class will be used in `SequenceFileRDDFunctions`.
 */
private[spark] class WritableFactory[T](
    val writableClass: ClassTag[T] => Class[_ <: Writable],
    val convert: T => Writable) extends Serializable

object WritableFactory {

  private[spark] def simpleWritableFactory[T: ClassTag, W <: Writable : ClassTag](convert: T => W)
    : WritableFactory[T] = {
    val writableClass = implicitly[ClassTag[W]].runtimeClass.asInstanceOf[Class[W]]
    new WritableFactory[T](_ => writableClass, convert)
  }

  implicit def intWritableFactory: WritableFactory[Int] =
    simpleWritableFactory(new IntWritable(_))

  implicit def longWritableFactory: WritableFactory[Long] =
    simpleWritableFactory(new LongWritable(_))

  implicit def floatWritableFactory: WritableFactory[Float] =
    simpleWritableFactory(new FloatWritable(_))

  implicit def doubleWritableFactory: WritableFactory[Double] =
    simpleWritableFactory(new DoubleWritable(_))

  implicit def booleanWritableFactory: WritableFactory[Boolean] =
    simpleWritableFactory(new BooleanWritable(_))

  implicit def bytesWritableFactory: WritableFactory[Array[Byte]] =
    simpleWritableFactory(new BytesWritable(_))

  implicit def stringWritableFactory: WritableFactory[String] =
    simpleWritableFactory(new Text(_))

  implicit def writableWritableFactory[T <: Writable: ClassTag]: WritableFactory[T] =
    simpleWritableFactory(w => w)

}

Feb 26, 2021 7:24:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 96
Feb 26, 2021 7:24:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:24:02 INFO  compiling root (1 scala source)[0m
Feb 26, 2021 7:24:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:24:02 INFO  time: compiled root in 0.44s[0m
Feb 26, 2021 7:24:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:24:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:24:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 3 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 3 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 3 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:24:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 4 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:61)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:24:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:15 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 19:24:21 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(def rddParser)
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.26 19:24:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:21 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 19:24:26 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
org.scalameta.invariants.InvariantFailedException: invariant failed:
when verifying stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat))
found that stats.forall(((x$11: scala.meta.Stat) => scala.meta.internal.trees.`package`.XtensionTreesStat(x$11).isTopLevelStat)) is false
where stats = List(def rddParser(spark: SparkSession))
	at org.scalameta.invariants.InvariantFailedException$.raise(Exceptions.scala:19)
	at scala.meta.Pkg$.internal$254(Trees.scala:443)
	at scala.meta.Pkg$.apply(Trees.scala:441)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$9(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$batchSource$1(ScalametaParser.scala:5027)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.batchSource(ScalametaParser.scala:4995)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$source$1(ScalametaParser.scala:4978)
	at scala.meta.internal.parsers.ScalametaParser.atPos(ScalametaParser.scala:800)
	at scala.meta.internal.parsers.ScalametaParser.autoPos(ScalametaParser.scala:816)
	at scala.meta.internal.parsers.ScalametaParser.source(ScalametaParser.scala:4977)
	at scala.meta.internal.parsers.ScalametaParser.entrypointSource(ScalametaParser.scala:4983)
	at scala.meta.internal.parsers.ScalametaParser.$anonfun$parseSource$2(ScalametaParser.scala:142)
	at scala.meta.internal.parsers.ScalametaParser.parseRule(ScalametaParser.scala:52)
	at scala.meta.internal.parsers.ScalametaParser.parseSource(ScalametaParser.scala:142)
	at scala.meta.parsers.Parse$.$anonfun$parseSource$1(Parse.scala:29)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5040)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.mtags.ScalaMtags.<init>(ScalaMtags.scala:40)
	at scala.meta.internal.metals.Docstrings$Deindexer.<init>(Docstrings.scala:96)
	at scala.meta.internal.metals.Docstrings.expireSymbolDefinition(Docstrings.scala:60)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2115)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2(MetalsLanguageServer.scala:2068)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$reindexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2065)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.reindexWorkspaceSources(MetalsLanguageServer.scala:2065)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$2(MetalsLanguageServer.scala:1240)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.26 19:24:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:26 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 19:24:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:31 INFO  time: compiled root in 0.21s[0m
Feb 26, 2021 7:24:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 211
Feb 26, 2021 7:24:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:24:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:46 INFO  time: compiled root in 0.25s[0m
Feb 26, 2021 7:24:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 6 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:24:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:24:49 INFO  time: compiled root in 0.2s[0m
Feb 26, 2021 7:24:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 238
Feb 26, 2021 7:25:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 250
Feb 26, 2021 7:25:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:25:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:25:12 INFO  time: compiled root in 0.19s[0m
Feb 26, 2021 7:25:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:25:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:25:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:25:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 8 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

[0m2021.02.26 19:25:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:25:15 INFO  time: compiled root in 0.19s[0m
Feb 26, 2021 7:25:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 13 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 13 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 13 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:25:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
java.util.concurrent.CompletionException: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:292)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:308)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:661)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.completeExceptionally(CompletableFuture.java:1990)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:40)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.IllegalArgumentException: 12 is not a valid line number, allowed [0..2]
	at scala.meta.internal.inputs.InternalInput.lineToOffset(InternalInput.scala:38)
	at scala.meta.internal.inputs.InternalInput.lineToOffset$(InternalInput.scala:33)
	at scala.meta.inputs.Input$VirtualFile.lineToOffset(Input.scala:80)
	at scala.meta.inputs.Position$Range$.apply(Position.scala:59)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionLspRange.toMeta(CommonMtagsEnrichments.scala:183)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$5(InsertInferredType.scala:85)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.metals.codeactions.InsertInferredType.$anonfun$contribute$1(InsertInferredType.scala:84)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	... 4 more

Feb 26, 2021 7:25:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 274
[0m2021.02.26 19:25:35 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:25:35 INFO  time: compiled root in 0.41s[0m
[0m2021.02.26 19:25:41 INFO  compiling root (2 scala sources)[0m
Feb 26, 2021 7:25:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 317
[0m2021.02.26 19:25:41 INFO  time: compiled root in 0.27s[0m
[0m2021.02.26 19:25:41 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:25:41 INFO  time: compiled root in 0.3s[0m
[0m2021.02.26 19:25:48 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:25:48 INFO  time: compiled root in 0.32s[0m
[0m2021.02.26 19:25:51 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:25:51 INFO  time: compiled root in 0.35s[0m
[0m2021.02.26 19:25:59 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:25:59 INFO  time: compiled root in 0.27s[0m
[0m2021.02.26 19:26:07 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:26:07 INFO  time: compiled root in 0.3s[0m
[0m2021.02.26 19:26:18 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:26:18 INFO  time: compiled root in 0.43s[0m
[0m2021.02.26 19:26:24 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:26:24 INFO  time: compiled root in 0.26s[0m
[0m2021.02.26 19:26:29 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:26:29 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 19:26:34 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:26:34 INFO  time: compiled root in 0.6s[0m
[0m2021.02.26 19:29:11 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:11 INFO  time: compiled root in 0.53s[0m
[0m2021.02.26 19:29:15 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:15 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 19:29:18 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:18 INFO  time: compiled root in 0.4s[0m
[0m2021.02.26 19:29:30 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:30 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 19:29:34 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:34 INFO  time: compiled root in 0.53s[0m
[0m2021.02.26 19:29:41 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:41 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 19:29:49 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:49 INFO  time: compiled root in 0.41s[0m
[0m2021.02.26 19:29:51 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:51 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 19:29:53 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:53 INFO  time: compiled root in 0.25s[0m
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 570
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 574
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 572
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 573
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 579
Feb 26, 2021 7:29:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 576
[0m2021.02.26 19:29:59 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:29:59 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 19:30:07 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:30:07 INFO  time: compiled root in 0.52s[0m
[0m2021.02.26 19:30:14 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:30:14 INFO  time: compiled root in 0.45s[0m
[0m2021.02.26 19:30:19 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:30:19 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 19:30:21 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:30:21 INFO  time: compiled root in 0.47s[0m
[0m2021.02.26 19:31:08 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:08 INFO  time: compiled root in 0.3s[0m
[0m2021.02.26 19:31:15 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:15 INFO  time: compiled root in 0.28s[0m
[0m2021.02.26 19:31:23 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:23 INFO  time: compiled root in 0.4s[0m
[0m2021.02.26 19:31:47 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:47 INFO  time: compiled root in 0.56s[0m
[0m2021.02.26 19:31:50 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:50 INFO  time: compiled root in 0.43s[0m
[0m2021.02.26 19:31:53 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:31:53 INFO  time: compiled root in 0.39s[0m
[0m2021.02.26 19:32:28 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:32:28 INFO  time: compiled root in 0.56s[0m
[0m2021.02.26 19:32:41 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:32:41 INFO  time: compiled root in 0.58s[0m
[0m2021.02.26 19:32:53 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:32:53 INFO  time: compiled root in 0.58s[0m
Feb 26, 2021 7:33:04 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:198)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.02.26 19:33:06 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:06 INFO  time: compiled root in 0.4s[0m
[0m2021.02.26 19:33:13 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:13 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:33:16 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:16 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:33:35 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:35 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:33:41 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:41 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:33:47 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:47 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:33:48 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:48 INFO  time: compiled root in 0.16s[0m
[0m2021.02.26 19:33:50 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:33:50 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:34:11 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:34:11 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:34:52 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:34:52 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:34:55 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:34:55 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:34:58 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:34:58 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:35:04 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:35:04 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:35:12 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:35:12 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:35:55 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:35:55 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 19:36:07 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:36:07 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:36:09 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:36:09 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:36:14 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:36:14 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 19:36:25 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:36:25 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:36:28 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:36:28 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:37:15 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:15 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:37:21 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:21 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:37:25 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:25 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:37:30 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:30 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 19:37:34 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:34 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:37:37 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:37 INFO  time: compiled root in 0.16s[0m
[0m2021.02.26 19:37:45 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:45 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:37:47 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:47 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:37:55 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:37:55 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:38:32 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:32 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:38:36 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:36 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:38:37 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:37 INFO  time: compiled root in 0.16s[0m
[0m2021.02.26 19:38:40 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:40 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:38:42 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:42 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:38:58 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:38:58 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:39:05 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:05 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:39:07 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:07 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:39:15 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:15 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:39:19 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:19 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 19:39:30 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:30 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:39:33 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:33 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:39:42 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:42 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:39:49 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:49 INFO  time: compiled root in 98ms[0m
[0m2021.02.26 19:39:55 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:55 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:39:59 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:39:59 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 19:40:04 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:40:04 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:40:17 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:40:17 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:40:29 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:40:29 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:40:35 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:40:35 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:40:39 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:40:39 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:42:30 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:30 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:42:50 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:50 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:42:52 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:52 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:42:53 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:53 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:42:54 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:54 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:42:56 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:42:56 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:43:22 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:43:22 INFO  time: compiled root in 97ms[0m
[0m2021.02.26 19:43:26 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:43:26 INFO  time: compiled root in 97ms[0m
[0m2021.02.26 19:43:29 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:43:29 INFO  time: compiled root in 0.16s[0m
[0m2021.02.26 19:43:31 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:43:31 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:43:34 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:43:34 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:44:01 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:01 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:44:02 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:02 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:44:04 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:04 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:44:05 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:05 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:44:10 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:10 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:44:16 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:16 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:44:18 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:18 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:44:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.select("url_host_name", (count(*)).as "")
                                                                   ^[0m
[0m2021.02.26 19:44:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.select("url_host_name", (count(*)).as "")
                                                                   ^[0m
[0m2021.02.26 19:44:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.select("url_host_name", (count(*)).as "")
                                                                   ^[0m
[0m2021.02.26 19:44:20 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:20 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:68: stale bloop error: unclosed character literal
      val exampleFormat = df.select("url_host_name", (count(*)).as '')
                                                                   ^[0m
[0m2021.02.26 19:44:23 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:44:23 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:45:44 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:45:44 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:45:49 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:45:49 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:45:52 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:45:52 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 19:46:00 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:00 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:46:08 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:08 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:46:12 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:12 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:46:13 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:13 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:46:19 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:19 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:46:26 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:26 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:46:28 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:28 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:46:30 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:46:30 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:47:46 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:47:46 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:47:54 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:47:54 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:47:57 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:47:57 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:48:01 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:01 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:48:07 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:07 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:48:13 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:13 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:48:22 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:22 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:48:26 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:26 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:48:36 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:36 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:48:43 INFO  compiling root (2 scala sources)[0m
[0m2021.02.26 19:48:43 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 19:50:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:10 INFO  time: compiled root in 0.4s[0m
[0m2021.02.26 19:50:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:30 INFO  time: compiled root in 0.31s[0m
[0m2021.02.26 19:50:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:40 INFO  time: compiled root in 0.31s[0m
[0m2021.02.26 19:50:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:45 INFO  time: compiled root in 0.27s[0m
[0m2021.02.26 19:50:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:50 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:62: stale bloop error: ')' expected but character literal found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                             ^[0m
[0m2021.02.26 19:50:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:105: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name",  as 'n', arbitrary("url_path") as "sample_path")
                                                                                                        ^[0m
[0m2021.02.26 19:50:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:56 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:58 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:50:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:84: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count() as 'n', arbitrary("url_path") as "sample_path")
                                                                                   ^[0m
[0m2021.02.26 19:50:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:50:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:112: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                                               ^[0m
[0m2021.02.26 19:50:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:112: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                                               ^[0m
[0m2021.02.26 19:50:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:85: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                    ^[0m
[0m2021.02.26 19:50:59 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:51:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:112: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                                               ^[0m
[0m2021.02.26 19:51:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:85: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                    ^[0m
[0m2021.02.26 19:51:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:112: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                                               ^[0m
[0m2021.02.26 19:51:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:85: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                    ^[0m
[0m2021.02.26 19:51:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:112: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                                               ^[0m
[0m2021.02.26 19:51:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:85: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary("url_path") as "sample_path")
                                                                                    ^[0m
[0m2021.02.26 19:51:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:15 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: ';' expected but ')' found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path") as "sample_path")
                                                                                                              ^[0m
[0m2021.02.26 19:51:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                             ^[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                             ^[0m
[0m2021.02.26 19:51:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:98: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                 ^[0m
[0m2021.02.26 19:51:16 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                             ^[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:98: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                 ^[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                             ^[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:98: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                 ^[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                             ^[0m
[0m2021.02.26 19:51:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:98: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as "sample_path")
                                                                                                 ^[0m
[0m2021.02.26 19:51:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:19 INFO  time: compiled root in 0.76s[0m
[0m2021.02.26 19:51:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:9:3: stale bloop error: overriding method main in trait App of type (args: Array[String])Unit;
 method main needs `override' modifier
> def main(args: Array[String]): Unit = {
> 
>     val spark = SparkSession
>       .builder()
>       .appName("Get S3 Data")
>       .config("spark.master", "local[*]")
>       .getOrCreate()
> 
>     Logger.getLogger("org").setLevel(Level.WARN)
> 
>     val key = System.getenv(("AWS_ACCESS_KEY"))
>     val secret = System.getenv(("AWS_SECRET_KEY"))
> 
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
>     spark.sparkContext.hadoopConfiguration
>       .set("fs.s3a.endpoint", "s3.amazonaws.com")
> 
>     //rddParser(spark)
>     //dfParser(spark)
>     //urlIndex(spark)
>     jobExample(spark)
> 
>     spark.close()
>   }[0m
[0m2021.02.26 19:51:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:9:3: stale bloop error: overriding method main in trait App of type (args: Array[String])Unit;
 method main needs `override' modifier
> def main(args: Array[String]): Unit = {
> 
>     val spark = SparkSession
>       .builder()
>       .appName("Get S3 Data")
>       .config("spark.master", "local[*]")
>       .getOrCreate()
> 
>     Logger.getLogger("org").setLevel(Level.WARN)
> 
>     val key = System.getenv(("AWS_ACCESS_KEY"))
>     val secret = System.getenv(("AWS_SECRET_KEY"))
> 
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
>     spark.sparkContext.hadoopConfiguration
>       .set("fs.s3a.endpoint", "s3.amazonaws.com")
> 
>     //rddParser(spark)
>     //dfParser(spark)
>     //urlIndex(spark)
>     jobExample(spark)
> 
>     spark.close()
>   }[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:9:3: stale bloop error: overriding method main in trait App of type (args: Array[String])Unit;
 method main needs `override' modifier
> def main(args: Array[String]): Unit = {
> 
>     val spark = SparkSession
>       .builder()
>       .appName("Get S3 Data")
>       .config("spark.master", "local[*]")
>       .getOrCreate()
> 
>     Logger.getLogger("org").setLevel(Level.WARN)
> 
>     val key = System.getenv(("AWS_ACCESS_KEY"))
>     val secret = System.getenv(("AWS_SECRET_KEY"))
> 
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
>     spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
>     spark.sparkContext.hadoopConfiguration
>       .set("fs.s3a.endpoint", "s3.amazonaws.com")
> 
>     //rddParser(spark)
>     //dfParser(spark)
>     //urlIndex(spark)
>     jobExample(spark)
> 
>     spark.close()
>   }[0m
[0m2021.02.26 19:51:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path)
                                        ^[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path)
                                        ^[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:21 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path)
                                        ^[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path)
                                        ^[0m
[0m2021.02.26 19:51:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path)
                                        ^[0m
[0m2021.02.26 19:51:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:22 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:51:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:24 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 19:51:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:29 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 19:51:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:41: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path')
                                        ^[0m
[0m2021.02.26 19:51:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 19:51:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:31 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 19:51:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:111: stale bloop error: unclosed string literal
      val exampleFormat = df.selectExpr("url_host_name, count(*) as 'n', arbitrary(url_path) as 'sample_path'"")
                                                                                                              ^[0m
[0m2021.02.26 19:51:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:51:35 INFO  time: compiled root in 1.03s[0m
[0m2021.02.26 19:52:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:52:21 INFO  time: compiled root in 1.08s[0m
[0m2021.02.26 19:53:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:53:22 INFO  time: compiled root in 0.97s[0m
[0m2021.02.26 19:53:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:53:26 INFO  time: compiled root in 1.33s[0m
[0m2021.02.26 19:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:53:27 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 19:58:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:58:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", count(*) as 'n', arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                             ^[0m
[0m2021.02.26 19:58:26 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:58:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", count(*) as 'n', arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                             ^[0m
[0m2021.02.26 19:58:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:110: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", count(*) as 'n', arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                             ^[0m
[0m2021.02.26 19:58:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:58:30 INFO  time: compiled root in 0.97s[0m
[0m2021.02.26 19:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:58:33 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:58:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:58:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:114: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                                 ^[0m
[0m2021.02.26 19:58:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:114: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                                 ^[0m
[0m2021.02.26 19:58:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:37 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 19:58:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:114: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                                 ^[0m
[0m2021.02.26 19:58:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:114: stale bloop error: unclosed character literal
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                                                                 ^[0m
[0m2021.02.26 19:58:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:75: stale bloop error: ')' expected but string literal found.
      val exampleFormat = df.selectExpr("url_host_name", "count(*) as 'n'"", "arbitrary(url_path) as 'sample_path'").show(5, false)
                                                                          ^[0m
[0m2021.02.26 19:58:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 19:58:40 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 20:01:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:01:16 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 20:01:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:01:17 INFO  time: compiled root in 1.08s[0m
[0m2021.02.26 20:01:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:01:39 INFO  time: compiled root in 1.21s[0m
[0m2021.02.26 20:01:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:01:46 INFO  time: compiled root in 1.05s[0m
[0m2021.02.26 20:01:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:01:47 INFO  time: compiled root in 0.91s[0m
[0m2021.02.26 20:04:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:04:43 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 20:04:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:04:46 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 20:06:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:06:19 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 20:06:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:06:20 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 20:06:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:06:23 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 20:06:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:25: stale bloop error: value show is not a member of org.apache.spark.sql.RelationalGroupedDataset
possible cause: maybe a semicolon is missing before `value show'?
> df
>       .selectExpr(
>         "url_host_name",
>         "count(*) as n",
>         "url_path as sample_path"
>       )
>       .groupBy("1")
>       .show[0m
[0m2021.02.26 20:06:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:25: stale bloop error: value show is not a member of org.apache.spark.sql.RelationalGroupedDataset
possible cause: maybe a semicolon is missing before `value show'?
> df
>       .selectExpr(
>         "url_host_name",
>         "count(*) as n",
>         "url_path as sample_path"
>       )
>       .groupBy("1")
>       .show[0m
[0m2021.02.26 20:06:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:06:35 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 20:07:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:14 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 20:07:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:17 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 20:07:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:23 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 20:07:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:30 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 20:07:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:34 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 20:07:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:07:49 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 20:08:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:33 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 20:08:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:37 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:08:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:40 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:08:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:46 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:08:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:51 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 20:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:54 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:08:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:08:57 INFO  time: compiled root in 0.1s[0m
Feb 26, 2021 8:09:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3008
[0m2021.02.26 20:09:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:09:27 INFO  time: compiled root in 0.9s[0m
[0m2021.02.26 20:09:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:09:39 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 20:09:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:09:46 INFO  time: compiled root in 1.33s[0m
[0m2021.02.26 20:09:48 INFO  compiling root (1 scala source)[0m
Feb 26, 2021 8:09:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3059
[0m2021.02.26 20:09:49 INFO  time: compiled root in 1.14s[0m
[0m2021.02.26 20:09:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:09:51 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 20:10:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:10:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:10:02 INFO  time: compiled root in 0.83s[0m
[0m2021.02.26 20:10:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:10:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:10:25 INFO  time: compiled root in 0.88s[0m
[0m2021.02.26 20:10:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:10:27 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 20:10:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:10:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:10:29 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 20:10:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:10:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:10:42 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 20:13:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:13:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:13:59 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 20:14:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:14:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:14:04 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 20:17:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:17:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:17:58 INFO  time: compiled root in 0.85s[0m
[0m2021.02.26 20:18:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:18:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:18:00 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 20:18:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:18:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:18:02 INFO  time: compiled root in 0.81s[0m
[0m2021.02.26 20:18:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:18:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:18:11 INFO  time: compiled root in 0.86s[0m
[0m2021.02.26 20:18:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:18:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:18:14 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 20:20:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:20:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:20:51 INFO  time: compiled root in 0.83s[0m
[0m2021.02.26 20:27:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:05 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 20:27:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:09 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:27:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:11 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 20:27:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:17 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 20:27:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:21 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:27:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:27 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:27:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:28 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:27:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:27:30 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:28:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:28:29 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 20:28:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:28:57 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:28:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:28:58 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 20:29:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:29:03 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 20:29:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:29:08 INFO  time: compiled root in 0.15s[0m
[0m2021.02.26 20:29:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:29:10 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 20:29:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:29:12 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 20:55:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:55:11 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 20:55:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:55:14 INFO  time: compiled root in 0.31s[0m
Feb 26, 2021 8:55:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3635
[0m2021.02.26 20:55:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:55:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:55:58 INFO  time: compiled root in 1.47s[0m
[0m2021.02.26 20:56:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:56:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:56:13 INFO  time: compiled root in 0.89s[0m
[0m2021.02.26 20:56:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:56:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:56:20 INFO  time: compiled root in 0.83s[0m
[0m2021.02.26 20:56:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:56:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:56:24 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 20:56:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 20:56:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 20:56:38 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 21:09:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:09:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:09:50 INFO  time: compiled root in 0.97s[0m
[0m2021.02.26 21:09:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:09:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:09:55 INFO  time: compiled root in 0.85s[0m
Feb 26, 2021 9:09:57 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.26 21:10:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:01 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 21:10:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:03 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 21:10:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:09 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 21:10:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:33 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 21:10:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:10:37 INFO  time: compiled root in 0.87s[0m
[0m2021.02.26 21:10:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:49 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 21:10:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:10:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:10:54 INFO  time: compiled root in 1.73s[0m
[0m2021.02.26 21:11:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:11:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:11:13 INFO  time: compiled root in 1.08s[0m
Feb 26, 2021 9:11:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Feb 26, 2021 9:11:14 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala: 3550 >= 3550
java.lang.AssertionError: assertion failed: file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala: 3550 >= 3550
	at scala.reflect.internal.util.SourceFile.position(SourceFile.scala:26)
	at scala.tools.nsc.CompilationUnits$CompilationUnit.position(CompilationUnits.scala:111)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:22)

[0m2021.02.26 21:11:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:11:27 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 21:11:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:11:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:11:38 INFO  time: compiled root in 0.88s[0m
[0m2021.02.26 21:11:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:11:55 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 21:11:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:11:58 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 21:12:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:12:46 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 21:13:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:44 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 21:13:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:49 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 21:13:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:52 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 21:13:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:56 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 21:13:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:58 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 21:13:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:13:59 INFO  time: compiled root in 0.17s[0m
[0m2021.02.26 21:14:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:14:03 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 21:14:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:14:06 INFO  time: compiled root in 0.23s[0m
[0m2021.02.26 21:14:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:14:10 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 21:53:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:53:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:53:11 INFO  time: compiled root in 2.74s[0m
[0m2021.02.26 21:54:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:32 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 21:54:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:34 INFO  time: compiled root in 0.78s[0m
[0m2021.02.26 21:54:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:35 INFO  time: compiled root in 0.25s[0m
[0m2021.02.26 21:54:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:38 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 21:54:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:44 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 21:54:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:47 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 21:54:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:54:52 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 21:55:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:00 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 21:55:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:03 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 21:55:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:04 INFO  time: compiled root in 0.22s[0m
[0m2021.02.26 21:55:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:07 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 21:55:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:17 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 21:55:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:24 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 21:55:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:55:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:55:35 INFO  time: compiled root in 1.07s[0m
[0m2021.02.26 21:59:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:59:41 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 21:59:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 21:59:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 21:59:46 INFO  time: compiled root in 1.14s[0m
[0m2021.02.26 22:10:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:10:32 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 22:10:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:10:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:10:36 INFO  time: compiled root in 1.66s[0m
[0m2021.02.26 22:10:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:10:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:10:42 INFO  time: compiled root in 1.32s[0m
[0m2021.02.26 22:29:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:29:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:29:55 INFO  time: compiled root in 1.76s[0m
[0m2021.02.26 22:29:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:29:57 INFO  time: compiled root in 0.26s[0m
[0m2021.02.26 22:30:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:00 INFO  time: compiled root in 0.21s[0m
[0m2021.02.26 22:30:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:04 INFO  time: compiled root in 0.2s[0m
[0m2021.02.26 22:30:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:16 INFO  time: compiled root in 0.26s[0m
[0m2021.02.26 22:30:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:30:19 INFO  time: compiled root in 0.91s[0m
[0m2021.02.26 22:30:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:24 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 22:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:27 INFO  time: compiled root in 0.19s[0m
[0m2021.02.26 22:30:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:29 INFO  time: compiled root in 0.24s[0m
[0m2021.02.26 22:30:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:30:34 INFO  time: compiled root in 0.88s[0m
[0m2021.02.26 22:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:30:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:30:42 INFO  time: compiled root in 0.93s[0m
[0m2021.02.26 22:33:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:33:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:33:33 INFO  time: compiled root in 1.16s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.Map
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder

/**
 * A collection of implicit methods for converting common Scala objects into [[Dataset]]s.
 *
 * @since 1.6.0
 */
@InterfaceStability.Evolving
abstract class SQLImplicits extends LowPrioritySQLImplicits {

  protected def _sqlContext: SQLContext

  /**
   * Converts $"col name" into a [[Column]].
   *
   * @since 2.0.0
   */
  implicit class StringToColumn(val sc: StringContext) {
    def $(args: Any*): ColumnName = {
      new ColumnName(sc.s(args: _*))
    }
  }

  // Primitives

  /** @since 1.6.0 */
  implicit def newIntEncoder: Encoder[Int] = Encoders.scalaInt

  /** @since 1.6.0 */
  implicit def newLongEncoder: Encoder[Long] = Encoders.scalaLong

  /** @since 1.6.0 */
  implicit def newDoubleEncoder: Encoder[Double] = Encoders.scalaDouble

  /** @since 1.6.0 */
  implicit def newFloatEncoder: Encoder[Float] = Encoders.scalaFloat

  /** @since 1.6.0 */
  implicit def newByteEncoder: Encoder[Byte] = Encoders.scalaByte

  /** @since 1.6.0 */
  implicit def newShortEncoder: Encoder[Short] = Encoders.scalaShort

  /** @since 1.6.0 */
  implicit def newBooleanEncoder: Encoder[Boolean] = Encoders.scalaBoolean

  /** @since 1.6.0 */
  implicit def newStringEncoder: Encoder[String] = Encoders.STRING

  /** @since 2.2.0 */
  implicit def newJavaDecimalEncoder: Encoder[java.math.BigDecimal] = Encoders.DECIMAL

  /** @since 2.2.0 */
  implicit def newScalaDecimalEncoder: Encoder[scala.math.BigDecimal] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newDateEncoder: Encoder[java.sql.Date] = Encoders.DATE

  /** @since 2.2.0 */
  implicit def newTimeStampEncoder: Encoder[java.sql.Timestamp] = Encoders.TIMESTAMP


  // Boxed primitives

  /** @since 2.0.0 */
  implicit def newBoxedIntEncoder: Encoder[java.lang.Integer] = Encoders.INT

  /** @since 2.0.0 */
  implicit def newBoxedLongEncoder: Encoder[java.lang.Long] = Encoders.LONG

  /** @since 2.0.0 */
  implicit def newBoxedDoubleEncoder: Encoder[java.lang.Double] = Encoders.DOUBLE

  /** @since 2.0.0 */
  implicit def newBoxedFloatEncoder: Encoder[java.lang.Float] = Encoders.FLOAT

  /** @since 2.0.0 */
  implicit def newBoxedByteEncoder: Encoder[java.lang.Byte] = Encoders.BYTE

  /** @since 2.0.0 */
  implicit def newBoxedShortEncoder: Encoder[java.lang.Short] = Encoders.SHORT

  /** @since 2.0.0 */
  implicit def newBoxedBooleanEncoder: Encoder[java.lang.Boolean] = Encoders.BOOLEAN

  // Seqs

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newIntSeqEncoder: Encoder[Seq[Int]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newLongSeqEncoder: Encoder[Seq[Long]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newDoubleSeqEncoder: Encoder[Seq[Double]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newFloatSeqEncoder: Encoder[Seq[Float]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newByteSeqEncoder: Encoder[Seq[Byte]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newShortSeqEncoder: Encoder[Seq[Short]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newBooleanSeqEncoder: Encoder[Seq[Boolean]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newStringSeqEncoder: Encoder[Seq[String]] = ExpressionEncoder()

  /**
   * @since 1.6.1
   * @deprecated use [[newSequenceEncoder]]
   */
  def newProductSeqEncoder[A <: Product : TypeTag]: Encoder[Seq[A]] = ExpressionEncoder()

  /** @since 2.2.0 */
  implicit def newSequenceEncoder[T <: Seq[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Maps
  /** @since 2.3.0 */
  implicit def newMapEncoder[T <: Map[_, _] : TypeTag]: Encoder[T] = ExpressionEncoder()

  /**
   * Notice that we serialize `Set` to Catalyst array. The set property is only kept when
   * manipulating the domain objects. The serialization format doesn't keep the set property.
   * When we have a Catalyst array which contains duplicated elements and convert it to
   * `Dataset[Set[T]]` by using the encoder, the elements will be de-duplicated.
   *
   * @since 2.3.0
   */
  implicit def newSetEncoder[T <: Set[_] : TypeTag]: Encoder[T] = ExpressionEncoder()

  // Arrays

  /** @since 1.6.1 */
  implicit def newIntArrayEncoder: Encoder[Array[Int]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newLongArrayEncoder: Encoder[Array[Long]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newDoubleArrayEncoder: Encoder[Array[Double]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newFloatArrayEncoder: Encoder[Array[Float]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newByteArrayEncoder: Encoder[Array[Byte]] = Encoders.BINARY

  /** @since 1.6.1 */
  implicit def newShortArrayEncoder: Encoder[Array[Short]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newBooleanArrayEncoder: Encoder[Array[Boolean]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newStringArrayEncoder: Encoder[Array[String]] = ExpressionEncoder()

  /** @since 1.6.1 */
  implicit def newProductArrayEncoder[A <: Product : TypeTag]: Encoder[Array[A]] =
    ExpressionEncoder()

  /**
   * Creates a [[Dataset]] from an RDD.
   *
   * @since 1.6.0
   */
  implicit def rddToDatasetHolder[T : Encoder](rdd: RDD[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(rdd))
  }

  /**
   * Creates a [[Dataset]] from a local Seq.
   * @since 1.6.0
   */
  implicit def localSeqToDatasetHolder[T : Encoder](s: Seq[T]): DatasetHolder[T] = {
    DatasetHolder(_sqlContext.createDataset(s))
  }

  /**
   * An implicit conversion that turns a Scala `Symbol` into a [[Column]].
   * @since 1.3.0
   */
  implicit def symbolToColumn(s: Symbol): ColumnName = new ColumnName(s.name)

}

/**
 * Lower priority implicit methods for converting Scala objects into [[Dataset]]s.
 * Conflicting implicits are placed here to disambiguate resolution.
 *
 * Reasons for including specific implicits:
 * newProductEncoder - to disambiguate for `List`s which are both `Seq` and `Product`
 */
trait LowPrioritySQLImplicits {
  /** @since 1.6.0 */
  implicit def newProductEncoder[T <: Product : TypeTag]: Encoder[T] = Encoders.product[T]

}

[0m2021.02.26 22:37:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:37:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:37:11 INFO  time: compiled root in 1.37s[0m
[0m2021.02.26 22:37:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:37:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:37:12 INFO  time: compiled root in 1.02s[0m
[0m2021.02.26 22:37:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:37:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:37:14 INFO  time: compiled root in 1.09s[0m
Feb 26, 2021 10:50:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4833
[0m2021.02.26 22:51:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:51:19 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 22:51:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 22:51:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 22:51:21 INFO  time: compiled root in 0.99s[0m
[0m2021.02.26 23:00:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:00:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:00:55 INFO  time: compiled root in 1.12s[0m
[0m2021.02.26 23:13:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:13:59 INFO  time: compiled root in 0.59s[0m
[0m2021.02.26 23:14:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:14:00 INFO  time: compiled root in 0.27s[0m
[0m2021.02.26 23:14:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:14:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:14:04 INFO  time: compiled root in 1.49s[0m
[0m2021.02.26 23:14:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:14:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:14:07 INFO  time: compiled root in 1.01s[0m
[0m2021.02.26 23:14:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:14:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:14:25 INFO  time: compiled root in 0.92s[0m
[0m2021.02.26 23:14:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:14:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:14:27 INFO  time: compiled root in 0.97s[0m
[0m2021.02.26 23:17:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:17:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:17:25 INFO  time: compiled root in 0.93s[0m
[0m2021.02.26 23:24:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:24:53 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 23:24:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:24:56 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 23:25:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:25:15 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 23:25:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:25:19 INFO  time: compiled root in 0.17s[0m
Feb 26, 2021 11:25:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5072
[0m2021.02.26 23:26:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:00 INFO  time: compiled root in 0.13s[0m
[0m2021.02.26 23:26:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:02 INFO  time: compiled root in 0.11s[0m
[0m2021.02.26 23:26:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:06 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 23:26:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:09 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 23:26:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:18: stale bloop error: ')' expected but '.' found.
    exampleFormat.show(200, false)
                 ^[0m
[0m2021.02.26 23:26:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:18: stale bloop error: ')' expected but '.' found.
    exampleFormat.show(200, false)
                 ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:18: stale bloop error: ')' expected but '.' found.
    exampleFormat.show(200, false)
                 ^[0m
[0m2021.02.26 23:26:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:49: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages)
                                                ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:49: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages)
                                                ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 23:26:18 INFO  time: compiled root in 0.1s[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:49: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages)
                                                ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:49: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages)
                                                ^[0m
[0m2021.02.26 23:26:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 23:26:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:49: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages)
                                                ^[0m
[0m2021.02.26 23:26:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.26 23:26:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:20 INFO  time: compiled root in 99ms[0m
[0m2021.02.26 23:26:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:24 INFO  time: compiled root in 0.14s[0m
[0m2021.02.26 23:26:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:68: stale bloop error: unclosed string literal
      .filter(($"url_path" contains "job") and ("content_languages"")
                                                                   ^[0m
[0m2021.02.26 23:26:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:29 INFO  time: compiled root in 0.12s[0m
[0m2021.02.26 23:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:33 INFO  time: compiled root in 0.18s[0m
[0m2021.02.26 23:26:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:26:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:26:40 INFO  time: compiled root in 0.91s[0m
[0m2021.02.26 23:28:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:28:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:28:33 INFO  time: compiled root in 0.96s[0m
[0m2021.02.26 23:28:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.26 23:28:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.26 23:28:36 INFO  time: compiled root in 0.98s[0m
[0m2021.02.27 01:12:37 INFO  shutting down Metals[0m
[0m2021.02.27 01:12:37 INFO  Shut down connection with build server.[0m
[0m2021.02.27 01:12:37 INFO  Shut down connection with build server.[0m
[0m2021.02.27 01:12:37 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.02.27 02:56:15 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.27 02:56:16 INFO  time: initialize in 0.47s[0m
[0m2021.02.27 02:56:17 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.27 02:56:16 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8406240020817544262/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.27 02:56:17 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.27 02:56:19 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    val parsedRDD = rdd
      .flatMap(line =>
        line.split("""\s+""") match {
          case Array(href, _) => Some(href)
        }
      )

    parsedRDD.take(100).foreach(println)
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

[0m2021.02.27 02:56:22 INFO  time: code lens generation in 5.12s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8406240020817544262/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8406240020817544262/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 02:56:22 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 02:56:22 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.27 02:56:22 INFO  Attempting to connect to the build server...[0mOpening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8927353718602228927/bsp.socket'...

Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3504364515024624066/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3504364515024624066/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3504364515024624066/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 02:56:22 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8927353718602228927/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8927353718602228927/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 02:56:23 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 02:56:23 INFO  shutting down Metals[0m
[0m2021.02.27 02:56:22 INFO  time: Connected to build server in 6.22s[0m
[0m2021.02.27 02:56:22 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.27 13:27:55 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.27 13:27:55 INFO  time: initialize in 0.37s[0m
[0m2021.02.27 13:27:55 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4496759023652110180/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.27 13:27:55 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 13:27:55 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.27 13:27:57 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(100)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )

    rdd.take(200).foreach(println).saveAsTextFile("rdd")
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4496759023652110180/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4496759023652110180/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 13:27:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 13:27:59 INFO  time: code lens generation in 3.38s[0m
[0m2021.02.27 13:27:59 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.27 13:27:59 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1589670687891029195/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8599684480384972756/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1589670687891029195/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1589670687891029195/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 13:27:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8599684480384972756/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8599684480384972756/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 13:27:59 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 13:27:59 INFO  time: Connected to build server in 3.84s[0m
[0m2021.02.27 13:27:59 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.27 13:27:59 INFO  time: Imported build in 0.23s[0m
[0m2021.02.27 13:28:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:28:02 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.27 13:28:02 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.27 13:28:02 INFO  time: indexed workspace in 3.17s[0m
[0m2021.02.27 13:28:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:28:05 INFO  time: compiled root in 1.74s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.02.27 13:28:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:28:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:28:07 INFO  time: compiled root in 2.59s[0m
[0m2021.02.27 13:28:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:28:26 INFO  time: compiled root in 0.46s[0m
[0m2021.02.27 13:29:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:29:13 INFO  time: compiled root in 0.65s[0m
[0m2021.02.27 13:29:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:29:16 INFO  time: compiled root in 0.28s[0m
[0m2021.02.27 13:30:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:30:25 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 13:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:30:27 INFO  time: compiled root in 0.27s[0m
[0m2021.02.27 13:30:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:30:31 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 13:30:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:30:47 INFO  time: compiled root in 0.28s[0m
[0m2021.02.27 13:30:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:30:55 INFO  time: compiled root in 0.3s[0m
[0m2021.02.27 13:31:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:19 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 13:31:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:25 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 13:31:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:31:29 INFO  time: compiled root in 1.09s[0m
[0m2021.02.27 13:31:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:31:31 INFO  time: compiled root in 1.01s[0m
[0m2021.02.27 13:31:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:33 INFO  time: compiled root in 0.33s[0m
[0m2021.02.27 13:31:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:40 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 13:31:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:44 INFO  time: compiled root in 0.13s[0m
[0m2021.02.27 13:31:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:47 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 13:31:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:31:55 INFO  time: compiled root in 0.88s[0m
[0m2021.02.27 13:31:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:31:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:31:56 INFO  time: compiled root in 0.9s[0m
[0m2021.02.27 13:32:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:32:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:32:07 INFO  time: compiled root in 0.89s[0m
[0m2021.02.27 13:35:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:35:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:35:21 INFO  time: compiled root in 0.83s[0m
[0m2021.02.27 13:35:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:35:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:35:22 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 13:35:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:35:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:35:28 INFO  time: compiled root in 1.22s[0m
[0m2021.02.27 13:35:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:35:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:35:31 INFO  time: compiled root in 0.82s[0m
Feb 27, 2021 1:41:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 489
Feb 27, 2021 1:41:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 495
Feb 27, 2021 1:44:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 533
[0m2021.02.27 13:44:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:44:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:44:51 INFO  time: compiled root in 0.79s[0m
[0m2021.02.27 13:45:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:45:02 INFO  time: compiled root in 1.32s[0m
[0m2021.02.27 13:45:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:03 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 13:45:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:05 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 13:45:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:45:07 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 13:45:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:45:09 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 13:45:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:14 INFO  time: compiled root in 0.33s[0m
Feb 27, 2021 1:45:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 623
[0m2021.02.27 13:45:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:45:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:45:16 INFO  time: compiled root in 0.82s[0m
Feb 27, 2021 1:46:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 641
Feb 27, 2021 1:51:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 656
[0m2021.02.27 13:54:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:54:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:54:37 INFO  time: compiled root in 0.88s[0m
Feb 27, 2021 1:54:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 675
Feb 27, 2021 1:55:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 688
[0m2021.02.27 13:55:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:55:28 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 13:55:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 13:55:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 13:55:29 INFO  time: compiled root in 0.84s[0m
Feb 27, 2021 1:56:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 742
[0m2021.02.27 14:01:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:01:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:01:56 INFO  time: compiled root in 0.83s[0m
[0m2021.02.27 14:02:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:00 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 14:02:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:02:08 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 14:02:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:12 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:02:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:19 INFO  time: compiled root in 0.18s[0m
Feb 27, 2021 2:02:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 872
[0m2021.02.27 14:02:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:23 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 14:02:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:33 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:02:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:51 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:02:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:52 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 14:02:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:02:55 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 14:03:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:10 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 14:03:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:11 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:03:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:20 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:03:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:23 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:03:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:32 INFO  time: compiled root in 99ms[0m
[0m2021.02.27 14:03:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:41 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 14:03:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:03:44 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:04:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:00 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:04:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:05 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:04:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:15 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:04:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:16 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 14:04:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:20 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 14:04:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:53 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:04:54 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:05:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:05:45 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:05:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:05:47 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 14:05:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:05:50 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 14:05:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:05:52 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 14:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:05:54 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:06:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:07 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:06:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:09 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 14:06:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:14 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 14:06:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:21 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 14:06:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:23 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 14:06:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:28 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 14:06:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:34 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:06:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:46 INFO  time: compiled root in 0.27s[0m
[0m2021.02.27 14:06:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:47 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:06:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:06:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:06:54 INFO  time: compiled root in 1.2s[0m
[0m2021.02.27 14:07:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:07:02 INFO  time: compiled root in 0.83s[0m
[0m2021.02.27 14:07:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:07:37 INFO  time: compiled root in 0.74s[0m
[0m2021.02.27 14:07:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:40 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 14:07:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:42 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 14:07:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:43 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:07:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:45 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 14:07:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:52 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 14:07:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:07:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:07:58 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 14:09:55 INFO  shutting down Metals[0m
[0m2021.02.27 14:09:55 INFO  Shut down connection with build server.[0m
[0m2021.02.27 14:09:55 INFO  Shut down connection with build server.[0m
[0m2021.02.27 14:09:55 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.27 14:10:30 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.27 14:10:30 INFO  time: initialize in 0.48s[0m
[0m2021.02.27 14:10:31 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4738777906900864625/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.27 14:10:30 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 14:10:31 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.27 14:10:34 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap( line => line.split("<html>"))

    flatRDD.foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4738777906900864625/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4738777906900864625/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 14:10:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:10:37 INFO  time: code lens generation in 5.65s[0m
[0m2021.02.27 14:10:37 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5447980487719893199/bsp.socket'...
[0m2021.02.27 14:10:37 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher718494204738965384/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5447980487719893199/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5447980487719893199/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher718494204738965384/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher718494204738965384/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 14:10:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:10:37 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:10:37 INFO  time: Connected to build server in 6.37s[0m
[0m2021.02.27 14:10:37 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.27 14:10:37 INFO  time: Imported build in 0.18s[0m
[0m2021.02.27 14:10:39 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.27 14:10:39 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.27 14:10:39 INFO  time: indexed workspace in 2.6s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Feb 27, 2021 2:10:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.27 14:17:39 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.27 14:17:39 INFO  time: initialize in 0.39s[0m
[0m2021.02.27 14:17:43 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7143869801117380407/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.27 14:17:43 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.27 14:17:43 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.02.27 14:17:45 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    rddParser(spark)
    //dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap( line => line.split("<html>"))

    flatRDD.foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.take(50).foreach(println)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc).as("n")

    exampleFormat.show(200, false)
  }
}

No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7143869801117380407/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7143869801117380407/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 14:17:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:17:47 INFO  time: code lens generation in 3.75s[0m
[0m2021.02.27 14:17:47 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.27 14:17:47 INFO  Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2405307507612248491/bsp.socket'...Attempting to connect to the build server...
[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3324270885998100176/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2405307507612248491/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2405307507612248491/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3324270885998100176/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3324270885998100176/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.27 14:17:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:17:47 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.27 14:17:47 INFO  time: Connected to build server in 4.33s[0m
[0m2021.02.27 14:17:47 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.27 14:17:48 INFO  time: Imported build in 0.15s[0m
[0m2021.02.27 14:17:49 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.27 14:17:49 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.27 14:17:49 INFO  time: indexed workspace in 2.21s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Feb 27, 2021 2:17:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.02.27 14:17:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:18:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:18:03 INFO  time: compiled root in 2.65s[0m
[0m2021.02.27 14:18:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:18:05 INFO  time: compiled root in 1.65s[0m
[0m2021.02.27 14:18:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:06 INFO  time: compiled root in 0.19s[0m
Feb 27, 2021 2:18:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 31
Feb 27, 2021 2:18:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 32
[0m2021.02.27 14:18:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:08 INFO  time: compiled root in 0.19s[0m
Feb 27, 2021 2:18:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 38
[0m2021.02.27 14:18:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:11 INFO  time: compiled root in 0.75s[0m
[0m2021.02.27 14:18:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:18:14 INFO  time: compiled root in 1.09s[0m
[0m2021.02.27 14:18:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:18:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:18:34 INFO  time: compiled root in 1.34s[0m
Feb 27, 2021 2:21:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 88
[0m2021.02.27 14:23:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:23:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:23:48 INFO  time: compiled root in 1.16s[0m
[0m2021.02.27 14:24:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:24:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:24:05 INFO  time: compiled root in 1s[0m
[0m2021.02.27 14:24:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:24:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:24:16 INFO  time: compiled root in 1.09s[0m
[0m2021.02.27 14:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:40:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:40:49 INFO  time: compiled root in 1.01s[0m
[0m2021.02.27 14:41:53 INFO  compiling s3dataget-build (1 scala source)[0m
Feb 27, 2021 2:41:54 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 166
import _root_.scala.xml.{TopScope=>$scope}
import _root_.sbt._
import _root_.sbt.Keys._
import _root_.sbt.nio.Keys._
import _root_.sbt.ScriptedPlugin.autoImport._, _root_.sbt.plugins.MiniDependencyTreePlugin.autoImport._, _root_.bloop.integrations.sbt.BloopPlugin.autoImport._, _root_.sbtassembly.AssemblyPlugin.autoImport._
import _root_.sbt.plugins.IvyPlugin, _root_.sbt.plugins.JvmPlugin, _root_.sbt.plugins.CorePlugin, _root_.sbt.ScriptedPlugin, _root_.sbt.plugins.SbtPlugin, _root_.sbt.plugins.SemanticdbPlugin, _root_.sbt.plugins.JUnitXmlReportPlugin, _root_.sbt.plugins.Giter8TemplatePlugin, _root_.sbt.plugins.MiniDependencyTreePlugin, _root_.bloop.integrations.sbt.BloopPlugin, _root_.sbtassembly.AssemblyPlugin
import Dependencies._

ThisBuild / scalaVersion     := "2.11.12"
ThisBuild / version          := "0.1.0-SNAPSHOT"
ThisBuild / organization     := "com.revature"
ThisBuild / organizationName := "revature"

lazy val root = (project in file("."))
  .settings(
    name := "s3dataget",
    libraryDependencies += scalaTest % Test,
    libraryDependencies += "org.apache.spark" %% "spark-sql" % "2.4.7" % "provided",
    libraryDependencies += "org.apache.hadoop" % "hadoop-common" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "2.7.7",
    libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "2.7.7"
  )

  assemblyMergeStrategy in assembly := {
  case PathList("META-INF", xs @ _*) => MergeStrategy.discard
  case x => MergeStrategy.first
}

// See https://www.scala-sbt.org/1.x/docs/Using-Sonatype.html for instructions on how to publish to Sonatype.

[0m2021.02.27 14:41:57 INFO  time: code lens generation in 4.36s[0m
[0m2021.02.27 14:41:58 INFO  time: compiled s3dataget-build in 4.67s[0m
[0m2021.02.27 14:42:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:42:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:42:10 INFO  time: compiled root in 0.95s[0m
Feb 27, 2021 2:43:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 180
[0m2021.02.27 14:46:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:46:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:46:56 INFO  time: compiled root in 1.3s[0m
Feb 27, 2021 2:46:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 221
[0m2021.02.27 14:47:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:47:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:47:02 INFO  time: compiled root in 1.13s[0m
Feb 27, 2021 2:48:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 240
Feb 27, 2021 2:48:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 246
[0m2021.02.27 14:49:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 14:49:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 14:49:25 INFO  time: compiled root in 1.07s[0m
Feb 27, 2021 2:59:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 272
[0m2021.02.27 15:04:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:04:25 INFO  time: compiled root in 0.25s[0m
[0m2021.02.27 15:04:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:04:29 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 15:04:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:04:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:04:34 INFO  time: compiled root in 1.05s[0m
[0m2021.02.27 15:04:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:04:39 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 15:04:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:04:48 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 15:05:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:07 INFO  time: compiled root in 0.23s[0m
Feb 27, 2021 3:05:12 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:198)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.02.27 15:05:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:15 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 15:05:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:18 INFO  time: compiled root in 0.18s[0m
Feb 27, 2021 3:05:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 436
[0m2021.02.27 15:05:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:39 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 15:05:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:45 INFO  time: compiled root in 0.25s[0m
[0m2021.02.27 15:05:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:05:59 INFO  time: compiled root in 0.25s[0m
[0m2021.02.27 15:06:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:06:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:06:05 INFO  time: compiled root in 0.98s[0m
Feb 27, 2021 3:06:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 531
[0m2021.02.27 15:06:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:06:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:06:08 INFO  time: compiled root in 0.92s[0m
[0m2021.02.27 15:06:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:06:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:06:21 INFO  time: compiled root in 0.92s[0m
[0m2021.02.27 15:06:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:06:27 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 15:06:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:06:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:06:30 INFO  time: compiled root in 0.93s[0m
Feb 27, 2021 3:14:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 600
Feb 27, 2021 3:16:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 606
[0m2021.02.27 15:16:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:16:41 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 15:16:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:16:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:16:43 INFO  time: compiled root in 0.9s[0m
[0m2021.02.27 15:16:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:16:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:16:54 INFO  time: compiled root in 0.78s[0m
Feb 27, 2021 3:17:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 672
Feb 27, 2021 3:25:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 680
[0m2021.02.27 15:31:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:32:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:32:01 INFO  time: compiled root in 1.57s[0m
Feb 27, 2021 3:32:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 709
Feb 27, 2021 3:34:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 716
[0m2021.02.27 15:44:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:44:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:44:15 INFO  time: compiled root in 2.92s[0m
Feb 27, 2021 3:44:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 751
Feb 27, 2021 3:45:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 757
[0m2021.02.27 15:50:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:50:31 INFO  time: compiled root in 0.76s[0m
[0m2021.02.27 15:50:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:50:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:50:35 INFO  time: compiled root in 1.02s[0m
[0m2021.02.27 15:50:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:50:43 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 15:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:50:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:50:48 INFO  time: compiled root in 1.54s[0m
[0m2021.02.27 15:51:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:51:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:51:05 INFO  time: compiled root in 1.93s[0m
[0m2021.02.27 15:51:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:51:06 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 15:51:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:51:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:51:09 INFO  time: compiled root in 1.7s[0m
[0m2021.02.27 15:51:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:51:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:51:25 INFO  time: compiled root in 1.27s[0m
[0m2021.02.27 15:51:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:51:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:51:29 INFO  time: compiled root in 1.47s[0m
Feb 27, 2021 3:52:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 864
Feb 27, 2021 3:58:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 875
[0m2021.02.27 15:58:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:58:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 15:58:57 INFO  time: compiled root in 0.87s[0m
[0m2021.02.27 15:59:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:59:45 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 15:59:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:59:47 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 15:59:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:59:48 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 15:59:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:59:50 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 15:59:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 15:59:54 INFO  time: compiled root in 0.12s[0m
Feb 27, 2021 3:59:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 929
[0m2021.02.27 16:00:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:00:01 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:00:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:00:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:00:04 INFO  time: compiled root in 0.8s[0m
Feb 27, 2021 4:00:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 949
[0m2021.02.27 16:00:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:00:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:00:57 INFO  time: compiled root in 0.77s[0m
Feb 27, 2021 4:00:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1006
[0m2021.02.27 16:01:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:01:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:01:10 INFO  time: compiled root in 0.77s[0m
Feb 27, 2021 4:01:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1034
[0m2021.02.27 16:01:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:01:43 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 16:01:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:01:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:01:49 INFO  time: compiled root in 0.8s[0m
Feb 27, 2021 4:01:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1086
Feb 27, 2021 4:03:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1092
Feb 27, 2021 4:03:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1104
Feb 27, 2021 4:04:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1110
[0m2021.02.27 16:04:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:18 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:04:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:21 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:04:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:24 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:04:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:27 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 16:04:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:04:29 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 4:04:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1187
Feb 27, 2021 4:04:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1193
[0m2021.02.27 16:04:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:04:41 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 16:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:04:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:04:54 INFO  time: compiled root in 0.8s[0m
Feb 27, 2021 4:04:57 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1231
[0m2021.02.27 16:05:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:05:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:05:12 INFO  time: compiled root in 0.79s[0m
Feb 27, 2021 4:05:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1263
[0m2021.02.27 16:05:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:05:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:05:19 INFO  time: compiled root in 0.75s[0m
[0m2021.02.27 16:05:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:05:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:05:34 INFO  time: compiled root in 0.77s[0m
[0m2021.02.27 16:05:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:05:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:05:37 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 16:06:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:06:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:06:00 INFO  time: compiled root in 0.74s[0m
[0m2021.02.27 16:06:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:06:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:06:03 INFO  time: compiled root in 1.52s[0m
[0m2021.02.27 16:06:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:06:11 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 16:06:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:06:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:06:13 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 4:06:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1351
[0m2021.02.27 16:06:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:06:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:06:22 INFO  time: compiled root in 0.76s[0m
Feb 27, 2021 4:06:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1368
Feb 27, 2021 4:06:45 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1380
[0m2021.02.27 16:07:14 INFO  compiling root (1 scala source)[0m
Feb 27, 2021 4:07:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1419
[0m2021.02.27 16:07:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:07:14 INFO  time: compiled root in 0.91s[0m
[0m2021.02.27 16:08:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:02 INFO  time: compiled root in 0.75s[0m
[0m2021.02.27 16:08:04 INFO  compiling root (1 scala source)[0m
Feb 27, 2021 4:08:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1437
[0m2021.02.27 16:08:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:04 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 16:08:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:21 INFO  time: compiled root in 0.75s[0m
[0m2021.02.27 16:08:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:25 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 16:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:27 INFO  time: compiled root in 0.79s[0m
[0m2021.02.27 16:08:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:35 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 16:08:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:38 INFO  time: compiled root in 0.77s[0m
[0m2021.02.27 16:08:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:40 INFO  time: compiled root in 0.73s[0m
Feb 27, 2021 4:08:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1501
[0m2021.02.27 16:08:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:08:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:08:57 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 16:09:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:01 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 16:09:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:04 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:09:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:06 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:09:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:08 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 16:09:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:13 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:09:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:19 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 16:09:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:22 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:09:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:23 INFO  time: compiled root in 0.1s[0m
Feb 27, 2021 4:09:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1633
[0m2021.02.27 16:09:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:37 INFO  time: compiled root in 0.1s[0m
Feb 27, 2021 4:09:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1649
[0m2021.02.27 16:09:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:45 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:09:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:47 INFO  time: compiled root in 0.15s[0m
[0m2021.02.27 16:09:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:09:50 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:10:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:10:03 INFO  time: compiled root in 0.1s[0m
Feb 27, 2021 4:12:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1704
[0m2021.02.27 16:12:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:12:33 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:12:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:12:36 INFO  time: compiled root in 0.1s[0m
Feb 27, 2021 4:12:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1724
[0m2021.02.27 16:12:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:12:50 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:12:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:12:53 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:12:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:12:59 INFO  time: compiled root in 0.15s[0m
[0m2021.02.27 16:13:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:04 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 16:13:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:10 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:13:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:11 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:13:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:13 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:13:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:23 INFO  time: compiled root in 0.11s[0m
Feb 27, 2021 4:13:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1831
[0m2021.02.27 16:13:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:29 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:13:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:35 INFO  time: compiled root in 0.14s[0m
Feb 27, 2021 4:13:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1873
[0m2021.02.27 16:13:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:13:52 INFO  time: compiled root in 0.11s[0m
Feb 27, 2021 4:13:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1898
[0m2021.02.27 16:14:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:14:01 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:14:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:14:26 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 16:15:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:15:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:15:14 INFO  time: compiled root in 0.94s[0m
Feb 27, 2021 4:15:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1931
Feb 27, 2021 4:19:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1940
Feb 27, 2021 4:20:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1949
Feb 27, 2021 4:22:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1979
Feb 27, 2021 4:22:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1987
Feb 27, 2021 4:22:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2006
Feb 27, 2021 4:23:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2024
Feb 27, 2021 4:24:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2040
Feb 27, 2021 4:25:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2057
Feb 27, 2021 4:26:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2071
Feb 27, 2021 4:26:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2081
[0m2021.02.27 16:26:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:26:54 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 16:26:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:26:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:26:56 INFO  time: compiled root in 0.77s[0m
Feb 27, 2021 4:27:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2122
Feb 27, 2021 4:27:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2130
Feb 27, 2021 4:28:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2145
Feb 27, 2021 4:30:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2169
Feb 27, 2021 4:30:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2175
Feb 27, 2021 4:31:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2193
[0m2021.02.27 16:31:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:31:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:31:45 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 16:32:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:32:24 INFO  time: compiled root in 0.19s[0m
Feb 27, 2021 4:33:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2261
Feb 27, 2021 4:44:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2366
Feb 27, 2021 4:45:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2408
Feb 27, 2021 4:46:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2431
[0m2021.02.27 16:47:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:47:44 INFO  time: compiled root in 0.27s[0m
[0m2021.02.27 16:47:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:47:51 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 16:47:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:47:53 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:48:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:02 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 16:48:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:12 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 16:48:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:25 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:48:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:27 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 16:48:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:36 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:38 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:48:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:43 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:48:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:49 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 16:48:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:48:51 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:49:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:49:00 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:49:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:49:05 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:49:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:49:10 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:49:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:49:11 INFO  time: compiled root in 0.23s[0m
Feb 27, 2021 4:49:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2726
Feb 27, 2021 4:50:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2743
[0m2021.02.27 16:50:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:50:01 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 16:50:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:50:12 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:50:14 INFO  time: compiled root in 0.18s[0m
Feb 27, 2021 4:50:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2763
[0m2021.02.27 16:50:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:50:38 INFO  time: compiled root in 0.21s[0m
Feb 27, 2021 4:50:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2783
[0m2021.02.27 16:51:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:51:05 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:51:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:51:27 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 16:51:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:51:31 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 16:51:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:51:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:51:43 INFO  time: compiled root in 0.97s[0m
Feb 27, 2021 4:52:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2919
[0m2021.02.27 16:52:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:52:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:52:32 INFO  time: compiled root in 0.76s[0m
[0m2021.02.27 16:52:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:52:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:52:34 INFO  time: compiled root in 0.94s[0m
[0m2021.02.27 16:52:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:52:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:52:38 INFO  time: compiled root in 0.85s[0m
Feb 27, 2021 4:52:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2972
Feb 27, 2021 4:58:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2979
Feb 27, 2021 4:59:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2988
[0m2021.02.27 16:59:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:59:38 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 16:59:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:59:42 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 16:59:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:59:49 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 16:59:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 16:59:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 16:59:57 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 17:00:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:00:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:00:00 INFO  time: compiled root in 0.76s[0m
Feb 27, 2021 5:08:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3063
[0m2021.02.27 17:10:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:10:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:10:01 INFO  time: compiled root in 1.36s[0m
Feb 27, 2021 5:22:28 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3084
[0m2021.02.27 17:26:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:26:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:26:55 INFO  time: compiled root in 0.79s[0m
[0m2021.02.27 17:27:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:27:31 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 17:27:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:27:37 INFO  time: compiled root in 0.89s[0m
[0m2021.02.27 17:27:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:39 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 17:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:45 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 17:27:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:46 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:27:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:27:51 INFO  time: compiled root in 2.21s[0m
[0m2021.02.27 17:27:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:55 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:27:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:27:59 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 17:28:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:03 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:28:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:06 INFO  time: compiled root in 0.14s[0m
Feb 27, 2021 5:28:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3235
[0m2021.02.27 17:28:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:28:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:28:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:62: stale bloop error: unclosed string literal
      df.filter(($"value" contains ""Accept-Language":"en-US")
                                                             ^[0m
[0m2021.02.27 17:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:62: stale bloop error: unclosed string literal
      df.filter(($"value" contains ""Accept-Language":"en-US")
                                                             ^[0m
[0m2021.02.27 17:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:25 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:28:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:62: stale bloop error: unclosed string literal
      df.filter(($"value" contains ""Accept-Language":"en-US")
                                                             ^[0m
[0m2021.02.27 17:28:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:62: stale bloop error: unclosed string literal
      df.filter(($"value" contains ""Accept-Language":"en-US")
                                                             ^[0m
[0m2021.02.27 17:28:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:62: stale bloop error: unclosed string literal
      df.filter(($"value" contains ""Accept-Language":"en-US")
                                                             ^[0m
[0m2021.02.27 17:28:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:60: stale bloop error: unclosed string literal
      df.filter(($"value" contains Accept-Language":"en-US")
                                                           ^[0m
[0m2021.02.27 17:28:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:60: stale bloop error: unclosed string literal
      df.filter(($"value" contains Accept-Language":"en-US")
                                                           ^[0m
[0m2021.02.27 17:28:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:34 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:28:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:60: stale bloop error: unclosed string literal
      df.filter(($"value" contains Accept-Language":"en-US")
                                                           ^[0m
[0m2021.02.27 17:28:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:60: stale bloop error: unclosed string literal
      df.filter(($"value" contains Accept-Language":"en-US")
                                                           ^[0m
[0m2021.02.27 17:28:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:60: stale bloop error: unclosed string literal
      df.filter(($"value" contains Accept-Language":"en-US")
                                                           ^[0m
[0m2021.02.27 17:28:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:28:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:36 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:28:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:28:41 INFO  time: compiled root in 0.15s[0m
[0m2021.02.27 17:28:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: identifier expected but string literal found.
      df.filter(($"value" contains "Accept-Language":"en-US") and ()
                                                     ^[0m
[0m2021.02.27 17:28:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:28:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: identifier expected but string literal found.
      df.filter(($"value" contains "Accept-Language":"en-US") and ()
                                                     ^[0m
[0m2021.02.27 17:28:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: identifier expected but string literal found.
      df.filter(($"value" contains "Accept-Language":"en-US") and ()
                                                     ^[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: unclosed string literal
      df.filter(($"value" contains "Accept-Language":"en-US
                                                     ^[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: unclosed string literal
      df.filter(($"value" contains "Accept-Language":"en-US
                                                     ^[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:29:00 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: unclosed string literal
      df.filter(($"value" contains "Accept-Language":"en-US
                                                     ^[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: unclosed string literal
      df.filter(($"value" contains "Accept-Language":"en-US
                                                     ^[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:29:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:54: stale bloop error: unclosed string literal
      df.filter(($"value" contains "Accept-Language":"en-US
                                                     ^[0m
[0m2021.02.27 17:29:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:29:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:29:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:04 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:07 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:11 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:15 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 17:29:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:17 INFO  time: compiled root in 0.1s[0m
Feb 27, 2021 5:29:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3348
[0m2021.02.27 17:29:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""
                                    ^[0m
[0m2021.02.27 17:29:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""
                                    ^[0m
[0m2021.02.27 17:29:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""
                                    ^[0m
[0m2021.02.27 17:29:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:77: stale bloop error: unclosed string literal
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                                                            ^[0m
[0m2021.02.27 17:29:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:77: stale bloop error: unclosed string literal
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                                                            ^[0m
[0m2021.02.27 17:29:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                    ^[0m
[0m2021.02.27 17:29:33 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:77: stale bloop error: unclosed string literal
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                                                            ^[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                    ^[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:77: stale bloop error: unclosed string literal
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                                                            ^[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                    ^[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:77: stale bloop error: unclosed string literal
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                                                            ^[0m
[0m2021.02.27 17:29:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:37: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" (contains "Accept-Language":"en-US" and ""Path":"/"
                                    ^[0m
[0m2021.02.27 17:29:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:36 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:29:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:44 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 17:29:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:29:48 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:30:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:02 INFO  time: compiled root in 95ms[0m
[0m2021.02.27 17:30:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:22 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:30:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:24 INFO  time: compiled root in 93ms[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:53: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains "Accept-Language":"en-US" and "Path":"/job-listing")
                                                    ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:53: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains "Accept-Language":"en-US" and "Path":"/job-listing")
                                                    ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:53: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains "Accept-Language":"en-US" and "Path":"/job-listing")
                                                    ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:53: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains "Accept-Language":"en-US" and "Path":"/job-listing")
                                                    ^[0m
[0m2021.02.27 17:30:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:53: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains "Accept-Language":"en-US" and "Path":"/job-listing")
                                                    ^[0m
[0m2021.02.27 17:30:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:43 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:35: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US" and "Path":"/job-listing")
                                  ^[0m
[0m2021.02.27 17:30:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:47 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:76: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains """Accept-Language":"en-US""" and "Path":"/job-listing")
                                                                           ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:76: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains """Accept-Language":"en-US""" and "Path":"/job-listing")
                                                                           ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:76: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains """Accept-Language":"en-US""" and "Path":"/job-listing")
                                                                           ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:76: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains """Accept-Language":"en-US""" and "Path":"/job-listing")
                                                                           ^[0m
[0m2021.02.27 17:30:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:76: stale bloop error: identifier expected but string literal found.
      df.filter($"value" contains """Accept-Language":"en-US""" and "Path":"/job-listing")
                                                                           ^[0m
[0m2021.02.27 17:30:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:30:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:30:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:30:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:51 INFO  time: compiled root in 0.13s[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:30:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:30:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:30:54 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 17:31:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:31:08 INFO  time: compiled root in 96ms[0m
[0m2021.02.27 17:31:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:31:11 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 17:31:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:31:53 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 17:31:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:31:57 INFO  time: compiled root in 98ms[0m
[0m2021.02.27 17:32:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:00 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 17:32:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:21 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:32:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:32:24 INFO  time: compiled root in 0.77s[0m
[0m2021.02.27 17:32:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:41 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:32:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and )
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and )
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and )
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:44 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:50 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 17:32:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:17: stale bloop error: missing argument list for method and in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `and _` or `and(_)` instead of `and`.
      df.filter($"value" contains """Path":"/job-listing""" and)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:32:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:71: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                                      ^[0m
[0m2021.02.27 17:32:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:71: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                                      ^[0m
[0m2021.02.27 17:32:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:61: stale bloop error: ')' expected but string literal found.
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                            ^[0m
[0m2021.02.27 17:32:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:51 INFO  time: compiled root in 97ms[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:71: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                                      ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:61: stale bloop error: ')' expected but string literal found.
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                            ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:71: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                                      ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:61: stale bloop error: ')' expected but string literal found.
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                            ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:71: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                                      ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:61: stale bloop error: ')' expected but string literal found.
      df.filter($"value" contains """Path":"/job-listing""" and"Path":"/)
                                                            ^[0m
[0m2021.02.27 17:32:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:54 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: unclosed string literal
      df.filter($"value" contains """Path":"/job-listing""" and "Path":"/)
                                                                       ^[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.27 17:32:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/)
                                                                ^[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/)
                                                                ^[0m
[0m2021.02.27 17:32:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:32:57 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 17:33:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/)
                                                                ^[0m
[0m2021.02.27 17:33:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/)
                                                                ^[0m
[0m2021.02.27 17:33:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/)
                                                                ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:02 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:65: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Path":"/job-listing""" and """Path":"/job)
                                                                ^[0m
[0m2021.02.27 17:33:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:05 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:33:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:31 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 17:33:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:34 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:33:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value and is not a member of String
      df.filter($"value" contains ("""Path":"/job-listing""" and """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:45 INFO  time: compiled root in 97ms[0m
[0m2021.02.27 17:33:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:47 INFO  time: compiled root in 0.12s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.02.27 17:33:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:50 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 17:33:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:51 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 17:33:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing""")
                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing""")
                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing""")
                                                                    ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:33:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:33:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:33:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:33:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:33:53 INFO  time: compiled root in 99ms[0m
[0m2021.02.27 17:34:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:34:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:01 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:34:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:16 INFO  time: compiled root in 0.14s[0m
[0m2021.02.27 17:34:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:18 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 17:34:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:33 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 17:34:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value && is not a member of String
      df.filter($"value" contains ("""Accept-Language":"en-US""" && """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:34:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value && is not a member of String
      df.filter($"value" contains ("""Accept-Language":"en-US""" && """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:36: stale bloop error: value && is not a member of String
      df.filter($"value" contains ("""Accept-Language":"en-US""" && """Path":"/job-listing"""))
                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:34:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:34:44 INFO  time: compiled root in 95ms[0m
[0m2021.02.27 17:34:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:34:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:69: stale bloop error: unclosed multi-line string literal
      df.filter($"value" contains """Accept-Language":"en-US""" and """Path":"/job-listing")
                                                                    ^[0m
[0m2021.02.27 17:34:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:2: stale bloop error: ')' expected but eof found.
}
 ^[0m
[0m2021.02.27 17:34:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:34:49 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 17:35:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:16 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 17:35:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:19 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 17:35:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:28 INFO  time: compiled root in 0.11s[0m
[0m2021.02.27 17:35:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:32 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter(($"value" contains """Accept-Language":"en-US""") and ("""Path":"/job-listing"""))
                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter(($"value" contains """Accept-Language":"en-US""") and ("""Path":"/job-listing"""))
                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:72: stale bloop error: type mismatch;
 found   : String("Path\":\"/job-listing")
 required: org.apache.spark.sql.Column
      df.filter(($"value" contains """Accept-Language":"en-US""") and ("""Path":"/job-listing"""))
                                                                       ^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.02.27 17:35:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:43 INFO  time: compiled root in 0.1s[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:104: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                                                       ^[0m
[0m2021.02.27 17:35:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:80: stale bloop error: ')' expected but string literal found.
      df.filter(($"value" contains """Accept-Language":"en-US""") and ($"value"""Path":"/job-listing"""))
                                                                               ^[0m
[0m2021.02.27 17:35:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:35:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:35:49 INFO  time: compiled root in 0.75s[0m
Feb 27, 2021 5:36:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3975
[0m2021.02.27 17:37:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:37:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:37:08 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 17:37:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:37:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:37:12 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 17:42:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:42:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:42:09 INFO  time: compiled root in 0.74s[0m
Feb 27, 2021 5:42:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4052
Feb 27, 2021 5:42:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4060
Feb 27, 2021 5:48:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4073
[0m2021.02.27 17:48:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:48:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:48:25 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 17:48:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:48:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:48:31 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 17:48:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:48:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:48:36 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 17:48:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:48:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:48:39 INFO  time: compiled root in 0.88s[0m
Feb 27, 2021 5:49:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4150
[0m2021.02.27 17:50:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:50:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:50:13 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 17:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:50:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:50:14 INFO  time: compiled root in 0.79s[0m
[0m2021.02.27 17:50:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:50:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:50:16 INFO  time: compiled root in 0.95s[0m
Feb 27, 2021 5:51:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4183
[0m2021.02.27 17:51:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:51:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:51:41 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 17:51:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:51:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:51:47 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 17:51:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:51:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:51:52 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 17:51:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:51:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:51:55 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 17:51:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:51:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:51:58 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 17:52:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:52:11 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 17:52:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 17:52:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 17:52:12 INFO  time: compiled root in 0.87s[0m
Feb 27, 2021 5:53:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4263
Feb 27, 2021 6:00:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4269
Feb 27, 2021 6:06:35 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4283
Feb 27, 2021 6:07:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4294
Feb 27, 2021 6:07:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4300
[0m2021.02.27 18:07:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:07:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:07:27 INFO  time: compiled root in 0.8s[0m
[0m2021.02.27 18:07:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:07:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:07:35 INFO  time: compiled root in 0.91s[0m
Feb 27, 2021 6:08:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4328
[0m2021.02.27 18:09:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:09:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:09:25 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 18:09:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:09:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:09:35 INFO  time: compiled root in 0.91s[0m
[0m2021.02.27 18:09:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:09:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:09:38 INFO  time: compiled root in 1.08s[0m
[0m2021.02.27 18:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:09:38 INFO  time: compiled root in 96ms[0m
[0m2021.02.27 18:09:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:09:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:09:43 INFO  time: compiled root in 0.83s[0m
Feb 27, 2021 6:10:44 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Feb 27, 2021 6:10:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4398
Feb 27, 2021 6:10:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4400
Feb 27, 2021 6:10:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4403
Feb 27, 2021 6:10:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4405
[0m2021.02.27 18:10:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:10:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:10:49 INFO  time: compiled root in 0.89s[0m
Feb 27, 2021 6:11:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4431
[0m2021.02.27 18:13:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:13:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:13:30 INFO  time: compiled root in 1.02s[0m
[0m2021.02.27 18:13:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:13:30 INFO  time: compiled root in 0.14s[0m
Feb 27, 2021 6:18:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4454
Feb 27, 2021 6:19:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4460
[0m2021.02.27 18:19:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:19:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:19:33 INFO  time: compiled root in 0.89s[0m
[0m2021.02.27 18:25:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:25:04 INFO  time: compiled root in 0.85s[0m
[0m2021.02.27 18:25:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:06 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:25:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:15 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 18:25:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:22 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:25:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:32 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 18:25:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:36 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 18:25:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:39 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:25:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:41 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 18:25:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:44 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 18:25:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:47 INFO  time: compiled root in 0.27s[0m
[0m2021.02.27 18:25:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:51 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 18:25:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:53 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 18:25:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:25:58 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:26:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:00 INFO  time: compiled root in 0.3s[0m
[0m2021.02.27 18:26:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:7: stale bloop error: df is already defined as value df
> val df = spark.read
>         .format("text")
>         .options(Map(
>           "compression" -> "gzip",
>           "path" -> ""
>         ))[0m
[0m2021.02.27 18:26:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:7: stale bloop error: value show is not a member of org.apache.spark.sql.DataFrameReader
      df.show(200, false)
      ^^^^^^^[0m
[0m2021.02.27 18:26:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:7: stale bloop error: df is already defined as value df
> val df = spark.read
>         .format("text")
>         .options(Map(
>           "compression" -> "gzip",
>           "path" -> ""
>         ))[0m
[0m2021.02.27 18:26:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:7: stale bloop error: value show is not a member of org.apache.spark.sql.DataFrameReader
      df.show(200, false)
      ^^^^^^^[0m
[0m2021.02.27 18:26:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:77:7: stale bloop error: df is already defined as value df
> val df = spark.read
>         .format("text")
>         .options(Map(
>           "compression" -> "gzip",
>           "path" -> ""
>         ))[0m
[0m2021.02.27 18:26:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:7: stale bloop error: value show is not a member of org.apache.spark.sql.DataFrameReader
      df.show(200, false)
      ^^^^^^^[0m
[0m2021.02.27 18:26:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:21: stale bloop error: unclosed string literal
          "path" -> "    val df = spark.read
                    ^[0m
[0m2021.02.27 18:26:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:21: stale bloop error: unclosed string literal
          "path" -> "    val df = spark.read
                    ^[0m
[0m2021.02.27 18:26:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:14: stale bloop error: unclosed string literal
      .load()"
             ^[0m
[0m2021.02.27 18:26:05 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 18:26:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:21: stale bloop error: unclosed string literal
          "path" -> "    val df = spark.read
                    ^[0m
[0m2021.02.27 18:26:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:14: stale bloop error: unclosed string literal
      .load()"
             ^[0m
[0m2021.02.27 18:26:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:21: stale bloop error: unclosed string literal
          "path" -> "    val df = spark.read
                    ^[0m
[0m2021.02.27 18:26:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:14: stale bloop error: unclosed string literal
      .load()"
             ^[0m
[0m2021.02.27 18:26:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:07 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 18:26:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:25 INFO  time: compiled root in 0.27s[0m
[0m2021.02.27 18:26:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:37 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 18:26:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:40 INFO  time: compiled root in 0.23s[0m
[0m2021.02.27 18:26:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:45 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 18:26:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:48 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:26:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:52 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 18:26:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:26:58 INFO  time: compiled root in 0.18s[0m
[0m2021.02.27 18:27:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:27:01 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:27:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:27:10 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:27:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:27:16 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:27:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:27:24 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 18:27:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:28:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:28:02 INFO  time: compiled root in 2.36s[0m
Feb 27, 2021 6:29:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4991
[0m2021.02.27 18:29:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:29:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:29:47 INFO  time: compiled root in 1.19s[0m
Feb 27, 2021 6:30:16 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5014
[0m2021.02.27 18:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:31:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:31:50 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 6:36:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5038
[0m2021.02.27 18:37:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:37:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:37:24 INFO  time: compiled root in 0.9s[0m
[0m2021.02.27 18:37:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:37:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:37:28 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 6:44:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5073
Feb 27, 2021 6:44:41 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5079
Feb 27, 2021 6:44:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5085
[0m2021.02.27 18:45:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:45:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:45:24 INFO  time: compiled root in 1.01s[0m
Feb 27, 2021 6:45:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5105
[0m2021.02.27 18:51:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:51:03 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 18:51:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:51:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:51:07 INFO  time: compiled root in 0.84s[0m
[0m2021.02.27 18:51:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:51:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:51:24 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 18:51:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:51:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:51:29 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 6:53:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5198
[0m2021.02.27 18:53:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:53:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:53:26 INFO  time: compiled root in 0.88s[0m
Feb 27, 2021 6:53:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5239
[0m2021.02.27 18:54:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:54:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:54:01 INFO  time: compiled root in 0.88s[0m
[0m2021.02.27 18:54:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:54:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:54:09 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 18:54:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:54:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:54:14 INFO  time: compiled root in 1.1s[0m
Feb 27, 2021 6:55:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5314
[0m2021.02.27 18:56:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:56:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:56:17 INFO  time: compiled root in 1.06s[0m
[0m2021.02.27 18:57:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:57:04 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 18:57:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:07 INFO  time: compiled root in 0.13s[0m
[0m2021.02.27 18:57:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:09 INFO  time: compiled root in 0.33s[0m
[0m2021.02.27 18:57:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:13 INFO  time: compiled root in 0.34s[0m
[0m2021.02.27 18:57:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:19 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 18:57:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:22 INFO  time: compiled root in 0.28s[0m
[0m2021.02.27 18:57:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:23 INFO  time: compiled root in 0.22s[0m
[0m2021.02.27 18:57:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:25 INFO  time: compiled root in 0.12s[0m
[0m2021.02.27 18:57:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:28 INFO  time: compiled root in 0.24s[0m
[0m2021.02.27 18:57:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:31 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 18:57:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:32 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 18:57:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 18:57:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 18:57:47 INFO  time: compiled root in 1s[0m
[0m2021.02.27 19:00:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:00:12 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 19:00:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:00:20 INFO  time: compiled root in 0.25s[0m
[0m2021.02.27 19:00:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:00:25 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 19:00:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:00:28 INFO  time: compiled root in 0.21s[0m
Feb 27, 2021 7:01:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5634
[0m2021.02.27 19:02:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:02:47 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 19:02:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:02:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:02:51 INFO  time: compiled root in 0.89s[0m
[0m2021.02.27 19:03:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:03:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:03:12 INFO  time: compiled root in 0.84s[0m
[0m2021.02.27 19:03:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:03:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:03:15 INFO  time: compiled root in 0.91s[0m
Feb 27, 2021 7:04:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5788
Feb 27, 2021 7:04:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5804
[0m2021.02.27 19:04:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:04:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:04:35 INFO  time: compiled root in 0.95s[0m
[0m2021.02.27 19:04:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:04:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:04:38 INFO  time: compiled root in 0.97s[0m
Feb 27, 2021 7:04:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5833
Feb 27, 2021 7:04:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5839
Feb 27, 2021 7:05:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5845
Feb 27, 2021 7:06:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5858
[0m2021.02.27 19:07:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:07:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:07:38 INFO  time: compiled root in 1.35s[0m
[0m2021.02.27 19:07:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:07:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:07:41 INFO  time: compiled root in 0.87s[0m
[0m2021.02.27 19:07:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:07:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:07:42 INFO  time: compiled root in 0.93s[0m
[0m2021.02.27 19:07:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:07:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:07:44 INFO  time: compiled root in 0.97s[0m
Feb 27, 2021 7:09:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5893
Feb 27, 2021 7:09:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5899
[0m2021.02.27 19:10:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:10:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:10:57 INFO  time: compiled root in 0.88s[0m
Feb 27, 2021 7:12:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5919
[0m2021.02.27 19:12:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:12:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:12:27 INFO  time: compiled root in 0.88s[0m
[0m2021.02.27 19:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:12:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:12:34 INFO  time: compiled root in 0.84s[0m
[0m2021.02.27 19:12:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:12:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:12:35 INFO  time: compiled root in 0.91s[0m
[0m2021.02.27 19:12:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:12:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:12:37 INFO  time: compiled root in 0.98s[0m
[0m2021.02.27 19:12:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:12:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:12:40 INFO  time: compiled root in 1.14s[0m
Feb 27, 2021 7:13:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5989
[0m2021.02.27 19:14:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:14:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:14:25 INFO  time: compiled root in 1.06s[0m
Feb 27, 2021 7:15:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6005
[0m2021.02.27 19:21:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:21:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:21:03 INFO  time: compiled root in 0.9s[0m
[0m2021.02.27 19:21:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:21:12 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 19:21:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:21:25 INFO  time: compiled root in 0.21s[0m
[0m2021.02.27 19:21:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:21:26 INFO  time: compiled root in 0.2s[0m
[0m2021.02.27 19:21:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:21:29 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 19:22:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:22:05 INFO  time: compiled root in 0.11s[0m
Feb 27, 2021 7:22:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6196
[0m2021.02.27 19:22:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:22:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:22:22 INFO  time: compiled root in 0.96s[0m
[0m2021.02.27 19:22:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:22:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:22:27 INFO  time: compiled root in 1s[0m
Feb 27, 2021 7:24:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6240
[0m2021.02.27 19:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:34:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:34:47 INFO  time: compiled root in 0.94s[0m
[0m2021.02.27 19:35:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:35:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:35:02 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 19:35:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:35:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:35:08 INFO  time: compiled root in 0.84s[0m
[0m2021.02.27 19:35:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:35:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:35:21 INFO  time: compiled root in 1.95s[0m
[0m2021.02.27 19:35:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:35:21 INFO  time: compiled root in 0.19s[0m
[0m2021.02.27 19:35:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:35:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:35:26 INFO  time: compiled root in 1.34s[0m
Feb 27, 2021 7:36:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6363
Feb 27, 2021 7:36:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6374
[0m2021.02.27 19:37:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:37:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:37:15 INFO  time: compiled root in 0.76s[0m
[0m2021.02.27 19:37:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:37:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:37:25 INFO  time: compiled root in 0.81s[0m
Feb 27, 2021 7:39:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6416
Feb 27, 2021 7:40:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6428
Feb 27, 2021 7:40:25 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6440
[0m2021.02.27 19:40:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:40:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:40:39 INFO  time: compiled root in 0.77s[0m
[0m2021.02.27 19:42:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:42:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:42:03 INFO  time: compiled root in 0.95s[0m
Feb 27, 2021 7:43:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6475
[0m2021.02.27 19:44:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:44:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:44:18 INFO  time: compiled root in 0.85s[0m
Feb 27, 2021 7:50:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6514
[0m2021.02.27 19:50:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:50:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:50:55 INFO  time: compiled root in 0.72s[0m
Feb 27, 2021 7:51:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6534
[0m2021.02.27 19:51:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:51:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:51:29 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 19:53:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:53:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:53:46 INFO  time: compiled root in 1.5s[0m
Feb 27, 2021 7:55:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6576
[0m2021.02.27 19:55:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:55:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:55:48 INFO  time: compiled root in 0.93s[0m
Feb 27, 2021 7:55:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6598
[0m2021.02.27 19:56:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 19:56:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 19:56:07 INFO  time: compiled root in 0.86s[0m
Feb 27, 2021 7:56:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6620
[0m2021.02.27 20:06:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:06:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:06:05 INFO  time: compiled root in 0.92s[0m
Feb 27, 2021 8:09:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6648
[0m2021.02.27 20:09:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:09:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:09:35 INFO  time: compiled root in 0.91s[0m
[0m2021.02.27 20:09:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:09:43 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 20:10:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (func: org.apache.spark.sql.Row => Boolean)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to ()
    df.filter()
    ^^^^^^^^^[0m
[0m2021.02.27 20:10:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: overloaded method value filter with alternatives:
  (func: org.apache.spark.api.java.function.FilterFunction[org.apache.spark.sql.Row])org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (func: org.apache.spark.sql.Row => Boolean)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (conditionExpr: String)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] <and>
  (condition: org.apache.spark.sql.Column)org.apache.spark.sql.Dataset[org.apache.spark.sql.Row]
 cannot be applied to ()
    df.filter()
    ^^^^^^^^^[0m
[0m2021.02.27 20:10:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:10:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:10:03 INFO  time: compiled root in 0.94s[0m
[0m2021.02.27 20:10:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:10:06 INFO  time: compiled root in 0.17s[0m
[0m2021.02.27 20:10:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:10:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:10:13 INFO  time: compiled root in 0.83s[0m
[0m2021.02.27 20:10:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:10:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:10:20 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 20:13:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:13:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:13:19 INFO  time: compiled root in 0.92s[0m
[0m2021.02.27 20:13:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:13:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:13:23 INFO  time: compiled root in 0.79s[0m
[0m2021.02.27 20:17:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:17:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:17:59 INFO  time: compiled root in 0.76s[0m
[0m2021.02.27 20:18:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:18:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:18:01 INFO  time: compiled root in 0.78s[0m
[0m2021.02.27 20:18:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:18:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:18:04 INFO  time: compiled root in 0.81s[0m
[0m2021.02.27 20:18:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:18:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:18:41 INFO  time: compiled root in 0.82s[0m
[0m2021.02.27 20:18:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:18:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:18:44 INFO  time: compiled root in 0.86s[0m
[0m2021.02.27 20:21:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:21:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:21:09 INFO  time: compiled root in 0.85s[0m
[0m2021.02.27 20:21:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:21:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:21:12 INFO  time: compiled root in 0.84s[0m
[0m2021.02.27 20:25:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.27 20:25:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.27 20:25:14 INFO  time: compiled root in 0.86s[0m
[0m2021.02.28 01:17:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:17:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:17:44 INFO  time: compiled root in 4.39s[0m
[0m2021.02.28 01:18:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:18:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:18:07 INFO  time: compiled root in 1.21s[0m
[0m2021.02.28 01:18:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:18:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:18:13 INFO  time: compiled root in 1.17s[0m
[0m2021.02.28 01:18:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:18:33 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 01:22:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:22:40 INFO  time: compiled root in 0.27s[0m
Feb 28, 2021 1:28:14 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.28 01:28:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:28:15 INFO  time: compiled root in 0.37s[0m
[0m2021.02.28 01:31:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:31:12 INFO  time: compiled root in 1.11s[0m
[0m2021.02.28 01:31:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:31:14 INFO  time: compiled root in 1.26s[0m
[0m2021.02.28 01:31:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:14 INFO  time: compiled root in 0.17s[0m
[0m2021.02.28 01:31:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:1: stale bloop error: expected class or object definition
/
^[0m
[0m2021.02.28 01:31:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:31:20 INFO  time: compiled root in 1.34s[0m
[0m2021.02.28 01:31:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:31:24 INFO  time: compiled root in 1.31s[0m
[0m2021.02.28 01:31:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:31:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:31:33 INFO  time: compiled root in 1.12s[0m
[0m2021.02.28 01:32:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:40 INFO  time: compiled root in 1.22s[0m
[0m2021.02.28 01:32:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:42 INFO  time: compiled root in 1.13s[0m
[0m2021.02.28 01:32:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:47 INFO  time: compiled root in 1.16s[0m
[0m2021.02.28 01:32:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:50 INFO  time: compiled root in 1.23s[0m
[0m2021.02.28 01:32:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:52 INFO  time: compiled root in 1.14s[0m
[0m2021.02.28 01:32:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:32:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:32:55 INFO  time: compiled root in 1.09s[0m
[0m2021.02.28 01:32:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:01 INFO  time: compiled root in 4.66s[0m
[0m2021.02.28 01:33:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:09 INFO  time: compiled root in 1.5s[0m
[0m2021.02.28 01:33:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:16 INFO  time: compiled root in 1.04s[0m
[0m2021.02.28 01:33:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:21 INFO  time: compiled root in 0.95s[0m
[0m2021.02.28 01:33:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:27 INFO  time: compiled root in 1.14s[0m
[0m2021.02.28 01:33:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:29 INFO  time: compiled root in 1.04s[0m
[0m2021.02.28 01:33:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:33 INFO  time: compiled root in 1.41s[0m
[0m2021.02.28 01:33:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:36 INFO  time: compiled root in 0.93s[0m
[0m2021.02.28 01:33:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:43 INFO  time: compiled root in 1.16s[0m
[0m2021.02.28 01:33:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:45 INFO  time: compiled root in 1.03s[0m
[0m2021.02.28 01:33:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:52 INFO  time: compiled root in 1.13s[0m
[0m2021.02.28 01:33:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:33:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:33:55 INFO  time: compiled root in 0.92s[0m
[0m2021.02.28 01:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 01:34:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 01:34:01 INFO  time: compiled root in 0.95s[0m
[0m2021.02.28 02:11:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:10 INFO  time: compiled root in 4.26s[0m
[0m2021.02.28 02:11:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:14 INFO  time: compiled root in 3.13s[0m
[0m2021.02.28 02:11:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:16 INFO  time: compiled root in 2.12s[0m
[0m2021.02.28 02:11:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:26 INFO  time: compiled root in 1.15s[0m
[0m2021.02.28 02:11:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:31 INFO  time: compiled root in 1.14s[0m
[0m2021.02.28 02:11:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:34 INFO  time: compiled root in 1.66s[0m
[0m2021.02.28 02:11:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:36 INFO  time: compiled root in 1.21s[0m
[0m2021.02.28 02:11:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:39 INFO  time: compiled root in 1.06s[0m
[0m2021.02.28 02:11:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:42 INFO  time: compiled root in 1.25s[0m
[0m2021.02.28 02:11:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:45 INFO  time: compiled root in 0.96s[0m
[0m2021.02.28 02:11:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:50 INFO  time: compiled root in 1.15s[0m
[0m2021.02.28 02:11:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 02:11:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 02:11:52 INFO  time: compiled root in 1s[0m
[0m2021.02.28 03:15:31 INFO  shutting down Metals[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
Feb 28, 2021 3:15:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.ClosedChannelException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy9.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SinkChannelImpl.ensureOpen(SinkChannelImpl.java:154)
	at sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:158)
	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
	at java.nio.channels.Channels.writeFully(Channels.java:98)
	at java.nio.channels.Channels.access$000(Channels.java:61)
	at java.nio.channels.Channels$1.write(Channels.java:174)
	at java.io.OutputStream.write(OutputStream.java:75)
	at java.nio.channels.Channels$1.write(Channels.java:155)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:125)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

Feb 28, 2021 3:15:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.ClosedChannelException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy9.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SinkChannelImpl.ensureOpen(SinkChannelImpl.java:154)
	at sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:158)
	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
	at java.nio.channels.Channels.writeFully(Channels.java:98)
	at java.nio.channels.Channels.access$000(Channels.java:61)
	at java.nio.channels.Channels$1.write(Channels.java:174)
	at java.io.OutputStream.write(OutputStream.java:75)
	at java.nio.channels.Channels$1.write(Channels.java:155)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:125)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

Feb 28, 2021 3:15:32 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint notify
INFO: Failed to send notification message.
org.eclipse.lsp4j.jsonrpc.JsonRpcException: java.nio.channels.ClosedChannelException
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:72)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.notify(RemoteEndpoint.java:126)
	at org.eclipse.lsp4j.jsonrpc.services.EndpointProxy.invoke(EndpointProxy.java:88)
	at com.sun.proxy.$Proxy9.onBuildExit(Unknown Source)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1(BuildServerConnection.scala:89)
	at scala.meta.internal.metals.BuildServerConnection.$anonfun$shutdown$1$adapted(BuildServerConnection.scala:85)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.nio.channels.ClosedChannelException
	at sun.nio.ch.SinkChannelImpl.ensureOpen(SinkChannelImpl.java:154)
	at sun.nio.ch.SinkChannelImpl.write(SinkChannelImpl.java:158)
	at java.nio.channels.Channels.writeFullyImpl(Channels.java:78)
	at java.nio.channels.Channels.writeFully(Channels.java:98)
	at java.nio.channels.Channels.access$000(Channels.java:61)
	at java.nio.channels.Channels$1.write(Channels.java:174)
	at java.io.OutputStream.write(OutputStream.java:75)
	at java.nio.channels.Channels$1.write(Channels.java:155)
	at scala.meta.internal.metals.ClosableOutputStream.write(ClosableOutputStream.scala:26)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:125)
	at java.io.FilterOutputStream.write(FilterOutputStream.java:97)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:67)
	... 14 more

[0m2021.02.28 03:15:32 INFO  Shut down connection with build server.[0m
[0m2021.02.28 03:15:32 INFO  Shut down connection with build server.[0m
[0m2021.02.28 03:15:32 INFO  Shut down connection with build server.[0m
[0m2021.02.28 03:16:06 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.28 03:16:06 INFO  time: initialize in 0.51s[0m
[0m2021.02.28 03:16:07 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.28 03:16:07 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher368309079123207756/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.28 03:16:08 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.02.28 03:16:10 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter($"value" contains "Qualifications:")
      .show(false)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 03:16:13 INFO  time: code lens generation in 5.88s[0m
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher368309079123207756/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher368309079123207756/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 03:16:13 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 03:16:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.28 03:16:13 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher823124826123790091/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4374080451144815394/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher823124826123790091/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher823124826123790091/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4374080451144815394/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4374080451144815394/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 03:16:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 03:16:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 03:16:14 INFO  time: Connected to build server in 7.25s[0m
[0m2021.02.28 03:16:14 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.28 03:16:14 INFO  time: Imported build in 0.4s[0m
[0m2021.02.28 03:16:18 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.28 03:16:18 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.28 03:16:18 INFO  shutting down Metals[0m
[0m2021.02.28 03:16:18 INFO  Shut down connection with build server.[0m
[0m2021.02.28 03:16:18 INFO  Shut down connection with build server.[0m
[0m2021.02.28 03:16:18 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.02.28 15:34:01 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.02.28 15:34:02 INFO  time: initialize in 0.82s[0m
[0m2021.02.28 15:34:03 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2503131495420733091/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.02.28 15:34:02 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.02.28 15:34:03 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2503131495420733091/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2503131495420733091/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 15:34:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 15:34:05 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.02.28 15:34:05 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8793664086449435078/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1987156433809418154/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8793664086449435078/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8793664086449435078/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1987156433809418154/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1987156433809418154/bsp.socket...
[0m2021.02.28 15:34:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.02.28 15:34:05 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.02.28 15:34:05 INFO  time: Connected to build server in 2.59s[0m
[0m2021.02.28 15:34:05 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.02.28 15:34:07 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.02.28 15:34:07 INFO  time: Imported build in 0.21s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")
      .count

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:34:15 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/https://commoncrawl.s3.amazonaws.com/crawl-data/CC-MAIN-2021-04/wet.paths.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    df.filter(_ contains "Experience:")
      .show(false)

    df.filter(_ contains "Experience:")
      .count

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/**
  * val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  * 
  * 
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */
[0m2021.02.28 15:34:15 INFO  time: code lens generation in 12s[0m
[0m2021.02.28 15:34:19 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.02.28 15:34:19 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.02.28 15:34:22 INFO  time: indexed workspace in 14s[0m
[0m2021.02.28 15:34:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:34:40 INFO  time: compiled root in 7.84s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.02.28 15:34:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:34:51 INFO  time: compiled root in 0.77s[0m
Feb 28, 2021 3:34:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 20
[0m2021.02.28 15:34:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:34:55 INFO  time: compiled root in 0.59s[0m
[0m2021.02.28 15:34:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:34:59 INFO  time: compiled root in 0.3s[0m
[0m2021.02.28 15:35:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:35:02 INFO  time: compiled root in 0.47s[0m
[0m2021.02.28 15:35:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:35:07 INFO  time: compiled root in 0.35s[0m
[0m2021.02.28 15:35:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:35:11 INFO  time: compiled root in 1.26s[0m
[0m2021.02.28 15:36:09 INFO  compiling root (1 scala source)[0m
Feb 28, 2021 3:36:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 92
Feb 28, 2021 3:36:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 96
Feb 28, 2021 3:36:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 94
[0m2021.02.28 15:36:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:36:16 INFO  time: compiled root in 7.1s[0m
Feb 28, 2021 3:38:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 125
Feb 28, 2021 3:39:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 131
[0m2021.02.28 15:39:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:39:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:39:13 INFO  time: compiled root in 2.21s[0m
[0m2021.02.28 15:39:14 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:39:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:39:16 INFO  time: compiled root in 2.07s[0m
[0m2021.02.28 15:39:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:39:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:39:20 INFO  time: compiled root in 1.41s[0m
[0m2021.02.28 15:39:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:39:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:39:25 INFO  time: compiled root in 2.08s[0m
[0m2021.02.28 15:40:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:40:43 INFO  time: compiled root in 0.41s[0m
[0m2021.02.28 15:40:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:40:45 INFO  time: compiled root in 0.37s[0m
[0m2021.02.28 15:40:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:40:48 INFO  time: compiled root in 0.77s[0m
[0m2021.02.28 15:40:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:40:51 INFO  time: compiled root in 0.38s[0m
[0m2021.02.28 15:40:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:41:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:41:01 INFO  time: compiled root in 1.62s[0m
[0m2021.02.28 15:41:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:41:02 INFO  time: compiled root in 0.28s[0m
[0m2021.02.28 15:41:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:41:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:41:07 INFO  time: compiled root in 1.3s[0m
[0m2021.02.28 15:44:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:44:40 INFO  time: compiled root in 0.27s[0m
[0m2021.02.28 15:44:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:44:43 INFO  time: compiled root in 0.45s[0m
[0m2021.02.28 15:45:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:00 INFO  time: compiled root in 0.17s[0m
[0m2021.02.28 15:45:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:08 INFO  time: compiled root in 0.15s[0m
[0m2021.02.28 15:45:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:19 INFO  time: compiled root in 0.15s[0m
[0m2021.02.28 15:45:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:26 INFO  time: compiled root in 0.14s[0m
Feb 28, 2021 3:45:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 362
Feb 28, 2021 3:45:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 365
Feb 28, 2021 3:45:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 363
[0m2021.02.28 15:45:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:30 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 15:45:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:45:59 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 15:46:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:46:27 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 15:46:29 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78: error: identifier expected but . found
      .format("text")
      ^[0m
[0m2021.02.28 15:46:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:46:39 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 15:46:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:46:42 INFO  time: compiled root in 0.14s[0m
[0m2021.02.28 15:47:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:05 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 15:47:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:10 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 15:47:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:11 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 15:47:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: ')' expected but '(' found.
    df.filter.show()
                  ^[0m
[0m2021.02.28 15:47:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: ')' expected but '(' found.
    df.filter.show()
                  ^[0m
[0m2021.02.28 15:47:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: ')' expected but '(' found.
    df.filter.show()
                  ^[0m
[0m2021.02.28 15:47:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:48: stale bloop error: unclosed string literal
    val df = spark.sparkContext.wholeTextFiles("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00001.warc.wet.gz)
                                               ^[0m
[0m2021.02.28 15:47:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:48: stale bloop error: unclosed string literal
    val df = spark.sparkContext.wholeTextFiles("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00001.warc.wet.gz)
                                               ^[0m
[0m2021.02.28 15:47:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 15:47:21 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 15:47:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:48: stale bloop error: unclosed string literal
    val df = spark.sparkContext.wholeTextFiles("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00001.warc.wet.gz)
                                               ^[0m
[0m2021.02.28 15:47:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 15:47:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:48: stale bloop error: unclosed string literal
    val df = spark.sparkContext.wholeTextFiles("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00001.warc.wet.gz)
                                               ^[0m
[0m2021.02.28 15:47:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 15:47:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:25 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 15:47:30 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:30 INFO  time: compiled root in 0.25s[0m
[0m2021.02.28 15:47:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:47:32 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 15:50:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:50:22 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 15:50:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:50:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:50:32 INFO  time: compiled root in 0.89s[0m
[0m2021.02.28 15:54:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:54:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:54:40 INFO  time: compiled root in 0.89s[0m
[0m2021.02.28 15:54:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:54:43 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 15:54:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 15:54:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 15:54:48 INFO  time: compiled root in 1.45s[0m
[0m2021.02.28 16:07:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:07:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:07:59 INFO  time: compiled root in 1.19s[0m
[0m2021.02.28 16:49:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:23 INFO  time: compiled root in 2.86s[0m
[0m2021.02.28 16:49:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:27 INFO  time: compiled root in 2.67s[0m
[0m2021.02.28 16:49:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:34 INFO  time: compiled root in 1.74s[0m
[0m2021.02.28 16:49:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:35 INFO  time: compiled root in 0.47s[0m
[0m2021.02.28 16:49:37 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:37 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 16:49:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:41 INFO  time: compiled root in 0.88s[0m
[0m2021.02.28 16:49:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:43 INFO  time: compiled root in 1.14s[0m
[0m2021.02.28 16:49:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:45 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 16:49:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:46 INFO  time: compiled root in 0.2s[0m
[0m2021.02.28 16:49:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:48 INFO  time: compiled root in 0.93s[0m
[0m2021.02.28 16:49:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:49:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:49:53 INFO  time: compiled root in 0.84s[0m
[0m2021.02.28 16:50:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:50:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:50:01 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 16:50:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 16:50:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 16:50:07 INFO  time: compiled root in 1.1s[0m
Feb 28, 2021 5:09:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 823
[0m2021.02.28 17:13:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:13:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:13:27 INFO  time: compiled root in 0.98s[0m
[0m2021.02.28 17:17:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:17:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:17:59 INFO  time: compiled root in 1.23s[0m
[0m2021.02.28 17:18:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:18:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:18:01 INFO  time: compiled root in 1.04s[0m
[0m2021.02.28 17:18:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:18:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:18:08 INFO  time: compiled root in 0.8s[0m
[0m2021.02.28 17:18:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:18:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:18:10 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 17:18:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:18:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:18:23 INFO  time: compiled root in 1.62s[0m
[0m2021.02.28 17:23:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:23:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:23:58 INFO  time: compiled root in 0.76s[0m
[0m2021.02.28 17:24:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:24:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:15: stale bloop error: unclosed string literal
      .format("json
              ^[0m
[0m2021.02.28 17:24:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:15: stale bloop error: unclosed string literal
      .format("json
              ^[0m
[0m2021.02.28 17:24:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:7: stale bloop error: unclosed string literal
      ")
      ^[0m
[0m2021.02.28 17:24:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 17:24:00 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:15: stale bloop error: unclosed string literal
      .format("json
              ^[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:7: stale bloop error: unclosed string literal
      ")
      ^[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:78:15: stale bloop error: unclosed string literal
      .format("json
              ^[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:7: stale bloop error: unclosed string literal
      ")
      ^[0m
[0m2021.02.28 17:24:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 17:24:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:24:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:24:03 INFO  time: compiled root in 1.05s[0m
[0m2021.02.28 17:31:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:31:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:31:03 INFO  time: compiled root in 0.78s[0m
[0m2021.02.28 17:49:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:49:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:49:03 INFO  time: compiled root in 1.2s[0m
Feb 28, 2021 5:53:07 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 963
[0m2021.02.28 17:53:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 17:53:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 17:53:57 INFO  time: compiled root in 1.43s[0m
[0m2021.02.28 21:02:22 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:24 INFO  time: compiled root in 1.36s[0m
[0m2021.02.28 21:02:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:29 INFO  time: compiled root in 0.65s[0m
[0m2021.02.28 21:02:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:35 INFO  time: compiled root in 1.01s[0m
[0m2021.02.28 21:02:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:02:39 INFO  time: compiled root in 2.83s[0m
[0m2021.02.28 21:02:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:02:43 INFO  time: compiled root in 0.96s[0m
[0m2021.02.28 21:02:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:02:44 INFO  time: compiled root in 0.8s[0m
[0m2021.02.28 21:02:48 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:48 INFO  time: compiled root in 0.34s[0m
[0m2021.02.28 21:02:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:02:50 INFO  time: compiled root in 0.84s[0m
[0m2021.02.28 21:02:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:02:51 INFO  time: compiled root in 0.9s[0m
[0m2021.02.28 21:02:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:55 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:02:59 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:02:59 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:03:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:01 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:03:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:03 INFO  time: compiled root in 0.1s[0m
[0m2021.02.28 21:03:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:06 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:03:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:11 INFO  time: compiled root in 0.14s[0m
[0m2021.02.28 21:03:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:13 INFO  time: compiled root in 0.1s[0m
[0m2021.02.28 21:03:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:19 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:03:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:23 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:03:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:03:25 INFO  time: compiled root in 0.1s[0m
Feb 28, 2021 9:03:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1314
[0m2021.02.28 21:04:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:04:11 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:04:13 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:04:13 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:04:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:04:20 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:04:26 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:04:26 INFO  time: compiled root in 0.1s[0m
Feb 28, 2021 9:04:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1388
[0m2021.02.28 21:05:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:05:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:05:55 INFO  time: compiled root in 0.83s[0m
[0m2021.02.28 21:05:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:05:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:05:57 INFO  time: compiled root in 0.94s[0m
Feb 28, 2021 9:06:00 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.02.28 21:06:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:06:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:06:01 INFO  time: compiled root in 0.97s[0m
Feb 28, 2021 9:11:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1446
[0m2021.02.28 21:20:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:20:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:20:52 INFO  time: compiled root in 0.83s[0m
[0m2021.02.28 21:20:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:20:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:20:55 INFO  time: compiled root in 0.97s[0m
[0m2021.02.28 21:20:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:20:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:20:58 INFO  time: compiled root in 0.91s[0m
[0m2021.02.28 21:21:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:21:02 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 21:21:06 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:21:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:21:06 INFO  time: compiled root in 0.87s[0m
Feb 28, 2021 9:24:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1559
[0m2021.02.28 21:25:05 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:25:05 INFO  time: compiled root in 0.86s[0m
[0m2021.02.28 21:25:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:25:10 INFO  time: compiled root in 2.5s[0m
[0m2021.02.28 21:25:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:25:11 INFO  time: compiled root in 0.89s[0m
[0m2021.02.28 21:25:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:25:17 INFO  time: compiled root in 0.77s[0m
[0m2021.02.28 21:25:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:24 INFO  time: compiled root in 0.22s[0m
[0m2021.02.28 21:25:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:25:28 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 21:25:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:31 INFO  time: compiled root in 0.19s[0m
[0m2021.02.28 21:25:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:51 INFO  time: compiled root in 0.22s[0m
[0m2021.02.28 21:25:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:25:58 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 21:26:03 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:03 INFO  time: compiled root in 0.17s[0m
[0m2021.02.28 21:26:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:16 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 21:26:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:17 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 21:26:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:21 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 21:26:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:26:25 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 21:26:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:28 INFO  time: compiled root in 0.23s[0m
[0m2021.02.28 21:26:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:26:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:26:38 INFO  time: compiled root in 0.79s[0m
Feb 28, 2021 9:28:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1795
[0m2021.02.28 21:45:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:45:55 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:45:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:45:58 INFO  time: compiled root in 0.19s[0m
[0m2021.02.28 21:46:00 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:46:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:46:00 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 21:52:15 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:15 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:52:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:21 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:52:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:24 INFO  time: compiled root in 0.15s[0m
[0m2021.02.28 21:52:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:27 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:52:29 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:29 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:52:33 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:33 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:52:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:34 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 21:52:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:42 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:52:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:50 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:52:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:52:51 INFO  time: compiled root in 0.14s[0m
Feb 28, 2021 9:52:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2008
[0m2021.02.28 21:53:09 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:09 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:53:17 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111: error: ) expected but } found
  }
  ^[0m
[0m2021.02.28 21:53:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:25 INFO  time: compiled root in 0.14s[0m
[0m2021.02.28 21:53:34 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:34 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 21:53:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:35 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 21:53:36 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:36 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 21:53:39 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:39 INFO  time: compiled root in 0.14s[0m
[0m2021.02.28 21:53:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:53:42 INFO  time: compiled root in 0.9s[0m
[0m2021.02.28 21:53:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 21:53:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 21:53:44 INFO  time: compiled root in 0.86s[0m
Feb 28, 2021 9:54:27 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2112
Feb 28, 2021 9:56:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2120
Feb 28, 2021 9:57:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2137
Feb 28, 2021 9:57:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2143
[0m2021.02.28 22:03:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:56 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:120: stale bloop error: unclosed string literal
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                                                                                       ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:57: stale bloop error: ')' expected but string literal found.
        ($"value" contains "Accept-Language: en-USor ($"value" contains "<p>") or ($"value" contains "WARC-TARGET-URI: ")
                                                        ^[0m
[0m2021.02.28 22:03:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:7: stale bloop error: ';' expected but ')' found.
      )
      ^[0m
[0m2021.02.28 22:03:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:03:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:03:58 INFO  time: compiled root in 1s[0m
[0m2021.02.28 22:35:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:35:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:35:49 INFO  time: compiled root in 0.85s[0m
[0m2021.02.28 22:35:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:35:54 INFO  time: compiled root in 0.15s[0m
[0m2021.02.28 22:35:57 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:35:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:35:57 INFO  time: compiled root in 0.8s[0m
[0m2021.02.28 22:40:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:40:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:40:56 INFO  time: compiled root in 0.85s[0m
[0m2021.02.28 22:45:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:45:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:45:40 INFO  time: compiled root in 0.98s[0m
[0m2021.02.28 22:45:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:45:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:45:44 INFO  time: compiled root in 0.89s[0m
[0m2021.02.28 22:46:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:46:41 INFO  time: compiled root in 0.25s[0m
[0m2021.02.28 22:46:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:46:42 INFO  time: compiled root in 0.38s[0m
[0m2021.02.28 22:46:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:46:53 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 22:47:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:01 INFO  time: compiled root in 0.29s[0m
[0m2021.02.28 22:47:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:47:10 INFO  time: compiled root in 0.98s[0m
[0m2021.02.28 22:47:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:47:12 INFO  time: compiled root in 0.95s[0m
[0m2021.02.28 22:47:18 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:47:19 INFO  time: compiled root in 1.02s[0m
[0m2021.02.28 22:47:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:47:21 INFO  time: compiled root in 1.14s[0m
[0m2021.02.28 22:47:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:47:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:47:27 INFO  time: compiled root in 0.94s[0m
[0m2021.02.28 22:52:20 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 22:52:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 22:52:20 INFO  time: compiled root in 0.92s[0m
[0m2021.02.28 23:01:04 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:01:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:01:04 INFO  time: compiled root in 0.94s[0m
[0m2021.02.28 23:01:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:01:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:01:09 INFO  time: compiled root in 1.19s[0m
[0m2021.02.28 23:05:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:23 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 23:05:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:05:47 INFO  time: compiled root in 2.49s[0m
[0m2021.02.28 23:05:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:47 INFO  time: compiled root in 0.22s[0m
[0m2021.02.28 23:05:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:05:50 INFO  time: compiled root in 1.16s[0m
[0m2021.02.28 23:05:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:50 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 23:05:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:05:53 INFO  time: compiled root in 1.26s[0m
[0m2021.02.28 23:05:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:55 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 23:05:58 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:05:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:05:59 INFO  time: compiled root in 1s[0m
Feb 28, 2021 11:06:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2712
[0m2021.02.28 23:06:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:27 INFO  time: compiled root in 0.91s[0m
[0m2021.02.28 23:06:31 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:31 INFO  time: compiled root in 0.93s[0m
Feb 28, 2021 11:06:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2735
[0m2021.02.28 23:06:44 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:44 INFO  time: compiled root in 0.99s[0m
[0m2021.02.28 23:06:45 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:46 INFO  time: compiled root in 1.03s[0m
[0m2021.02.28 23:06:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:48 INFO  time: compiled root in 1.04s[0m
[0m2021.02.28 23:06:50 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:50 INFO  time: compiled root in 0.97s[0m
[0m2021.02.28 23:06:52 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:06:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:06:53 INFO  time: compiled root in 1.14s[0m
Feb 28, 2021 11:06:53 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2776
[0m2021.02.28 23:07:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:07:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:07:08 INFO  time: compiled root in 0.92s[0m
[0m2021.02.28 23:07:11 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:07:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:07:12 INFO  time: compiled root in 1.35s[0m
[0m2021.02.28 23:08:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:08:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:08:54 INFO  time: compiled root in 0.93s[0m
[0m2021.02.28 23:08:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:08:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:08:56 INFO  time: compiled root in 0.84s[0m
[0m2021.02.28 23:09:01 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:09:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:09:02 INFO  time: compiled root in 1.01s[0m
Feb 28, 2021 11:10:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2856
Feb 28, 2021 11:10:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2863
[0m2021.02.28 23:23:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:23:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:26: stale bloop error: unclosed string literal
      .filter(_ contains "Accept-Language: en-US)
                         ^[0m
[0m2021.02.28 23:23:46 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 23:23:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:26: stale bloop error: unclosed string literal
      .filter(_ contains "Accept-Language: en-US)
                         ^[0m
[0m2021.02.28 23:23:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:26: stale bloop error: unclosed string literal
      .filter(_ contains "Accept-Language: en-US)
                         ^[0m
[0m2021.02.28 23:23:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:23:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:23:51 INFO  time: compiled root in 1.15s[0m
[0m2021.02.28 23:32:38 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:32:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:32:38 INFO  time: compiled root in 0.93s[0m
[0m2021.02.28 23:33:08 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:33:08 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 23:33:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:33:10 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 23:33:17 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:33:17 INFO  time: compiled root in 0.2s[0m
[0m2021.02.28 23:33:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:12: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.ColumnName
 required: org.apache.spark.sql.Row => ?
    df.map($"value")
           ^^^^^^^^[0m
[0m2021.02.28 23:33:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:12: stale bloop error: type mismatch;
 found   : org.apache.spark.sql.ColumnName
 required: org.apache.spark.sql.Row => ?
    df.map($"value")
           ^^^^^^^^[0m
[0m2021.02.28 23:33:27 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:33:27 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 23:34:10 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:10 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 23:34:25 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:25 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 23:34:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:28 INFO  time: compiled root in 0.2s[0m
[0m2021.02.28 23:34:35 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:35 INFO  time: compiled root in 0.1s[0m
[0m2021.02.28 23:34:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:42 INFO  time: compiled root in 0.22s[0m
[0m2021.02.28 23:34:43 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:43 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 23:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:47 INFO  time: compiled root in 0.17s[0m
[0m2021.02.28 23:34:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:51 INFO  time: compiled root in 0.25s[0m
[0m2021.02.28 23:34:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:54 INFO  time: compiled root in 0.12s[0m
[0m2021.02.28 23:34:56 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:34:56 INFO  time: compiled root in 0.19s[0m
Feb 28, 2021 11:35:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3162
[0m2021.02.28 23:36:41 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:36:41 INFO  time: compiled root in 0.2s[0m
[0m2021.02.28 23:36:42 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:36:42 INFO  time: compiled root in 0.2s[0m
[0m2021.02.28 23:36:46 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:36:46 INFO  time: compiled root in 0.21s[0m
[0m2021.02.28 23:40:54 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:40:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:40:54 INFO  time: compiled root in 0.81s[0m
[0m2021.02.28 23:41:02 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:02 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 23:41:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:07 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 23:41:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:16 INFO  time: compiled root in 0.18s[0m
[0m2021.02.28 23:41:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:41:19 INFO  time: compiled root in 0.83s[0m
[0m2021.02.28 23:41:24 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:41:24 INFO  time: compiled root in 0.82s[0m
[0m2021.02.28 23:41:40 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:41:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:31: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>))
                              ^[0m
[0m2021.02.28 23:41:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:31: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>))
                              ^[0m
[0m2021.02.28 23:41:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:41:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 23:41:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:41:40 INFO  time: compiled root in 0.15s[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:31: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>))
                              ^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:31: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>))
                              ^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 23:42:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:31: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>))
                              ^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:49 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:40: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>""))
                                       ^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:40: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>""))
                                       ^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:49 INFO  time: compiled root in 0.11s[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:40: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>""))
                                       ^[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:40: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>""))
                                       ^[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:40: stale bloop error: unclosed string literal
    df.select(split($"value", "</html>""))
                                       ^[0m
[0m2021.02.28 23:42:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:1:1: stale bloop error: expected class or object definition
hpackage `com.revature.scala`
^[0m
[0m2021.02.28 23:42:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:43: stale bloop error: ';' expected but '=' found.
  def urlIndex(spark: SparkSession): Unit = {
                                          ^[0m
[0m2021.02.28 23:42:51 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:42:51 INFO  time: compiled root in 0.11s[0m
Feb 28, 2021 11:43:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3359
[0m2021.02.28 23:43:32 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:43:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:43:32 INFO  time: compiled root in 0.97s[0m
[0m2021.02.28 23:52:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:52:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:52:12 INFO  time: compiled root in 0.96s[0m
[0m2021.02.28 23:52:16 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:52:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:52:16 INFO  time: compiled root in 0.8s[0m
[0m2021.02.28 23:52:19 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:52:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:52:20 INFO  time: compiled root in 1.01s[0m
[0m2021.02.28 23:56:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:56:23 INFO  time: compiled root in 0.13s[0m
[0m2021.02.28 23:56:53 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:56:53 INFO  time: compiled root in 0.26s[0m
[0m2021.02.28 23:56:55 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:56:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:56:55 INFO  time: compiled root in 0.89s[0m
[0m2021.02.28 23:57:07 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:57:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:57:07 INFO  time: compiled root in 0.83s[0m
[0m2021.02.28 23:57:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:57:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:57:23 INFO  time: compiled root in 2.16s[0m
[0m2021.02.28 23:59:12 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:59:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:59:13 INFO  time: compiled root in 1.26s[0m
[0m2021.02.28 23:59:21 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:59:21 INFO  time: compiled root in 0.17s[0m
[0m2021.02.28 23:59:23 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:59:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:59:23 INFO  time: compiled root in 0.82s[0m
Feb 28, 2021 11:59:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
java.util.concurrent.CompletionException: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1523)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1521)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.reflect.internal.Symbols$Symbol.lock(Symbols.scala:567)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1521)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.qual(Contexts.scala:1416)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.importedSymbol(Contexts.scala:1450)
	at scala.tools.nsc.typechecker.Contexts$Context.importedAccessibleSymbol(Contexts.scala:968)
	at scala.tools.nsc.typechecker.Contexts$Context.lookupImport$1(Contexts.scala:1114)
	at scala.tools.nsc.typechecker.Contexts$Context.lookupSymbol(Contexts.scala:1135)
	at scala.meta.internal.pc.Signatures$ShortenedNames$$anonfun$1.apply(Signatures.scala:53)
	at scala.meta.internal.pc.Signatures$ShortenedNames$$anonfun$1.apply(Signatures.scala:49)
	at scala.meta.internal.pc.Signatures$ShortenedNames.nameResolvesToSymbol(Signatures.scala:143)
	at scala.meta.internal.pc.Signatures$ShortenedNames.isSymbolInScope(Signatures.scala:136)
	at scala.meta.internal.pc.MetalsGlobal.scala$meta$internal$pc$MetalsGlobal$$loop$1(MetalsGlobal.scala:198)
	at scala.meta.internal.pc.MetalsGlobal.shortType(MetalsGlobal.scala:346)
	at scala.meta.internal.pc.Signatures$ShortenedNames$.synthesize(Signatures.scala:59)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:151)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.02.28 23:59:28 INFO  compiling root (1 scala source)[0m
[0m2021.02.28 23:59:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.02.28 23:59:28 INFO  time: compiled root in 0.97s[0m
Mar 01, 2021 12:01:14 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3640
[0m2021.03.01 00:02:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:02:02 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 00:02:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:02:04 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 00:02:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:02:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:02:07 INFO  time: compiled root in 0.95s[0m
[0m2021.03.01 00:04:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:04:32 INFO  time: compiled root in 0.18s[0m
Mar 01, 2021 12:04:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3728
Mar 01, 2021 12:04:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3727
Mar 01, 2021 12:04:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3729
[0m2021.03.01 00:04:37 INFO  compiling root (1 scala source)[0m
Mar 01, 2021 12:04:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint fallbackResponseError
SEVERE: Internal error: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
java.util.concurrent.CompletionException: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at java.util.concurrent.CompletableFuture.encodeThrowable(CompletableFuture.java:273)
	at java.util.concurrent.CompletableFuture.completeThrowable(CompletableFuture.java:280)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:673)
	at java.util.concurrent.CompletableFuture$UniAccept.tryFire(CompletableFuture.java:646)
	at java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:488)
	at java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:1975)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1(CancelTokens.scala:42)
	at scala.meta.internal.metals.CancelTokens$.$anonfun$future$1$adapted(CancelTokens.scala:38)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving value <import>
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1523)
	at scala.reflect.internal.Symbols$Symbol$$anonfun$info$3.apply(Symbols.scala:1521)
	at scala.Function0$class.apply$mcV$sp(Function0.scala:34)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:12)
	at scala.reflect.internal.Symbols$Symbol.lock(Symbols.scala:567)
	at scala.reflect.internal.Symbols$Symbol.info(Symbols.scala:1521)
	at scala.tools.nsc.typechecker.Contexts$ImportInfo.qual(Contexts.scala:1416)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.pre$lzycompute$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.scala$meta$internal$pc$completions$Completions$class$$anonfun$$pre$1(Completions.scala:719)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:722)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1$$anonfun$apply$1.apply(Completions.scala:720)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:720)
	at scala.meta.internal.pc.completions.Completions$$anonfun$renamedSymbols$1.apply(Completions.scala:718)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at scala.meta.internal.pc.completions.Completions$class.renamedSymbols(Completions.scala:718)
	at scala.meta.internal.pc.MetalsGlobal.renamedSymbols(MetalsGlobal.scala:30)
	at scala.meta.internal.pc.Signatures$ShortenedNames$.synthesize(Signatures.scala:56)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:151)
	at scala.meta.internal.pc.CompletionProvider$$anonfun$1.apply(CompletionProvider.scala:75)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)
	at scala.collection.Iterator$class.toStream(Iterator.scala:1320)
	at scala.collection.AbstractIterator.toStream(Iterator.scala:1334)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.Iterator$$anonfun$toStream$1.apply(Iterator.scala:1320)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1233)
	at scala.collection.immutable.Stream$Cons.tail(Stream.scala:1223)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$$anonfun$next$1.apply(Stream.scala:1120)
	at scala.collection.immutable.StreamIterator$LazyCell.v$lzycompute(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator$LazyCell.v(Stream.scala:1109)
	at scala.collection.immutable.StreamIterator.hasNext(Stream.scala:1114)
	at scala.collection.convert.Wrappers$IteratorWrapper.hasNext(Wrappers.scala:30)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:134)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.CollectionTypeAdapter.write(CollectionTypeAdapter.java:40)
	at com.google.gson.internal.bind.TypeAdapterRuntimeTypeWrapper.write(TypeAdapterRuntimeTypeWrapper.java:69)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$1.write(ReflectiveTypeAdapterFactory.java:125)
	at com.google.gson.internal.bind.ReflectiveTypeAdapterFactory$Adapter.write(ReflectiveTypeAdapterFactory.java:243)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:423)
	at org.eclipse.lsp4j.jsonrpc.json.adapters.MessageTypeAdapter.write(MessageTypeAdapter.java:55)
	at com.google.gson.Gson.toJson(Gson.java:669)
	at com.google.gson.Gson.toJson(Gson.java:648)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:145)
	at org.eclipse.lsp4j.jsonrpc.json.MessageJsonHandler.serialize(MessageJsonHandler.java:140)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageConsumer.consume(StreamMessageConsumer.java:59)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.lambda$handleRequest$1(RemoteEndpoint.java:281)
	at java.util.concurrent.CompletableFuture.uniAccept(CompletableFuture.java:670)
	... 9 more

[0m2021.03.01 00:04:37 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 00:04:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:04:42 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 00:04:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:04:44 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:05:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:05:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:05:10 INFO  time: compiled root in 0.94s[0m
[0m2021.03.01 00:05:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:05:11 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 00:05:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:05:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:05:14 INFO  time: compiled root in 0.81s[0m
[0m2021.03.01 00:05:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:05:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:05:17 INFO  time: compiled root in 0.79s[0m
[0m2021.03.01 00:07:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:07:50 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:07:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:07:52 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:08:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:08:16 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 00:08:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:08:18 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 00:08:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:08:22 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 00:08:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:08:23 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 00:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:08:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:08:27 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 00:09:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:09:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:09:03 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 00:10:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:10:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:10:13 INFO  time: compiled root in 0.9s[0m
[0m2021.03.01 00:10:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:10:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:10:19 INFO  time: compiled root in 1s[0m
Mar 01, 2021 12:11:09 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4032
[0m2021.03.01 00:24:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:24:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:24:11 INFO  time: compiled root in 0.78s[0m
[0m2021.03.01 00:25:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:25:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:25:46 INFO  time: compiled root in 0.87s[0m
[0m2021.03.01 00:25:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:25:49 INFO  time: compiled root in 0.24s[0m
[0m2021.03.01 00:25:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:25:52 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 00:25:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:25:56 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 00:25:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:25:59 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 00:26:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:04 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 00:26:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:08 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 00:26:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:14 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 00:26:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:13: stale bloop error: not enough arguments for method array_contains: (column: org.apache.spark.sql.Column, value: Any)org.apache.spark.sql.Column.
Unspecified value parameter value.
    .filter(array_contains("Websites"))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.01 00:26:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:13: stale bloop error: not enough arguments for method array_contains: (column: org.apache.spark.sql.Column, value: Any)org.apache.spark.sql.Column.
Unspecified value parameter value.
    .filter(array_contains("Websites"))
            ^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.01 00:26:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:23 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 00:26:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:28 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:26:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:26:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:26:31 INFO  time: compiled root in 0.93s[0m
[0m2021.03.01 00:29:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:29:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:29:15 INFO  time: compiled root in 1.03s[0m
[0m2021.03.01 00:29:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:29:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:29:17 INFO  time: compiled root in 0.82s[0m
[0m2021.03.01 00:29:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:29:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:29:23 INFO  time: compiled root in 1.01s[0m
[0m2021.03.01 00:44:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:44:32 INFO  time: compiled root in 1.13s[0m
[0m2021.03.01 00:44:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:32 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 00:44:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:35 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 00:44:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:41 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 00:44:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:47 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 00:44:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:51 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:44:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:54 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 00:44:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:56 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 00:44:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:44:59 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 00:45:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:45:04 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 00:45:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:45:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:45:08 INFO  time: compiled root in 0.97s[0m
[0m2021.03.01 00:45:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:45:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:45:12 INFO  time: compiled root in 0.84s[0m
[0m2021.03.01 00:48:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:48:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:48:56 INFO  time: compiled root in 0.92s[0m
[0m2021.03.01 00:49:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:49:10 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 00:49:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:49:12 INFO  time: compiled root in 1.11s[0m
[0m2021.03.01 00:49:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:49:25 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 00:49:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:49:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:49:38 INFO  time: compiled root in 0.91s[0m
[0m2021.03.01 00:49:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:49:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:49:44 INFO  time: compiled root in 1.16s[0m
[0m2021.03.01 00:53:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:53:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:53:49 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 00:55:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 00:55:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 00:55:05 INFO  time: compiled root in 0.84s[0m
[0m2021.03.01 01:05:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:05:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:05:20 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 01:05:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:05:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:05:28 INFO  time: compiled root in 0.95s[0m
[0m2021.03.01 01:06:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:06:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 01:06:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:06:54 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 01:06:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:06:59 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 01:07:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:07:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:07:04 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 01:07:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:07:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:07:08 INFO  time: compiled root in 0.82s[0m
[0m2021.03.01 01:07:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:07:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:07:13 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 01:08:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 01:08:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 01:08:37 INFO  time: compiled root in 0.85s[0m
[0m2021.03.01 01:40:21 INFO  shutting down Metals[0m
[0m2021.03.01 01:40:22 INFO  Shut down connection with build server.[0m
[0m2021.03.01 01:40:22 INFO  Shut down connection with build server.[0m
[0m2021.03.01 01:40:22 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.01 14:56:17 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.01 14:56:18 INFO  time: initialize in 1.05s[0m
[0m2021.03.01 14:56:19 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.01 14:56:19 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher822606651279427875/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.01 14:56:20 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.01 14:56:23 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD = rdd.flatMap(line => line.split("<html>"))

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val df = spark.read
      .format("text")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed"
        )
      )
      .load()

    val splitDF = df.select(split($"value", "</html>").as("Websites")).drop("value")

    splitDF
    .select($"Websites", expr("Websites[1]")).show(1000)

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val jobSiteIndex = df
      .select("url")
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
          .contains("/jobs|job-listing"))
          .as("Job Listing URLs")
      )

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.01 14:56:26 INFO  time: code lens generation in 6.01s[0m
[0m2021.03.01 14:56:26 INFO  time: code lens generation in 6.05s[0m
Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher822606651279427875/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher822606651279427875/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:56:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:56:26 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.01 14:56:26 INFO  Attempting to connect to the build server...[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2546438659969472410/bsp.socket'...
Starting the bsp launcher for bloop...
Waiting for the bsp connection to come up...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2901737594059733684/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2901737594059733684/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2901737594059733684/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:56:27 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2546438659969472410/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2546438659969472410/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.01 14:56:27 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.01 14:56:27 INFO  time: Connected to build server in 7.68s[0m
[0m2021.03.01 14:56:27 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.01 14:56:27 INFO  time: Imported build in 0.37s[0m
[0m2021.03.01 14:56:31 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.01 14:56:31 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.01 14:56:32 INFO  time: indexed workspace in 5.34s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 01, 2021 2:56:34 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

Mar 01, 2021 2:56:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 17
[0m2021.03.01 14:57:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 14:57:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 14:57:17 INFO  time: compiled root in 11s[0m
Mar 01, 2021 3:02:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 78
[0m2021.03.01 15:03:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:03:54 INFO  time: compiled root in 1.68s[0m
[0m2021.03.01 15:03:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:03:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:03:58 INFO  time: compiled root in 3.73s[0m
[0m2021.03.01 15:04:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:03 INFO  time: compiled root in 3.01s[0m
[0m2021.03.01 15:04:04 INFO  compiling root (1 scala source)[0m
Mar 01, 2021 3:04:06 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 113
[0m2021.03.01 15:04:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:07 INFO  time: compiled root in 2.85s[0m
[0m2021.03.01 15:04:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:21 INFO  time: compiled root in 3.14s[0m
[0m2021.03.01 15:04:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:26 INFO  time: compiled root in 2.3s[0m
[0m2021.03.01 15:04:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:28 INFO  time: compiled root in 2.24s[0m
[0m2021.03.01 15:04:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:28 INFO  time: compiled root in 0.52s[0m
[0m2021.03.01 15:04:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:32 INFO  time: compiled root in 1.88s[0m
[0m2021.03.01 15:04:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:04:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:04:35 INFO  time: compiled root in 2.23s[0m
[0m2021.03.01 15:11:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:11:40 INFO  time: compiled root in 0.32s[0m
Mar 01, 2021 3:11:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 204
[0m2021.03.01 15:11:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:11:43 INFO  time: compiled root in 0.9s[0m
[0m2021.03.01 15:11:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:11:45 INFO  time: compiled root in 0.54s[0m
Mar 01, 2021 3:11:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 231
[0m2021.03.01 15:11:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:11:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:11:55 INFO  time: compiled root in 1.24s[0m
[0m2021.03.01 15:12:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:05 INFO  time: compiled root in 2.07s[0m
[0m2021.03.01 15:12:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:08 INFO  time: compiled root in 1.38s[0m
[0m2021.03.01 15:12:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:13 INFO  time: compiled root in 1.43s[0m
[0m2021.03.01 15:12:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:16 INFO  time: compiled root in 1.72s[0m
[0m2021.03.01 15:12:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:21 INFO  time: compiled root in 0.68s[0m
[0m2021.03.01 15:12:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:24 INFO  time: compiled root in 1.85s[0m
[0m2021.03.01 15:12:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:33 INFO  time: compiled root in 1.21s[0m
[0m2021.03.01 15:12:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:37 INFO  time: compiled root in 1.92s[0m
[0m2021.03.01 15:12:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:39 INFO  time: compiled root in 0.43s[0m
[0m2021.03.01 15:12:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:44 INFO  time: compiled root in 2.22s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import org.apache.spark.annotation.InterfaceStability

/**
 * A container for a [[Dataset]], used for implicit conversions in Scala.
 *
 * To use this, import implicit conversions in SQL:
 * {{{
 *   val spark: SparkSession = ...
 *   import spark.implicits._
 * }}}
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
case class DatasetHolder[T] private[sql](private val ds: Dataset[T]) {

  // This is declared with parentheses to prevent the Scala compiler from treating
  // `rdd.toDS("1")` as invoking this toDS and then apply on the returned Dataset.
  def toDS(): Dataset[T] = ds

  // This is declared with parentheses to prevent the Scala compiler from treating
  // `rdd.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = ds.toDF()

  def toDF(colNames: String*): DataFrame = ds.toDF(colNames : _*)
}

[0m2021.03.01 15:12:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:52 INFO  time: compiled root in 2.17s[0m
[0m2021.03.01 15:12:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:53 INFO  time: compiled root in 0.32s[0m
[0m2021.03.01 15:12:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:12:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:12:57 INFO  time: compiled root in 1.99s[0m
[0m2021.03.01 15:13:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:13:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:13:02 INFO  time: compiled root in 2.06s[0m
[0m2021.03.01 15:13:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:13:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:13:05 INFO  time: compiled root in 2.73s[0m
[0m2021.03.01 15:13:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:13:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:13:09 INFO  time: compiled root in 1.71s[0m
[0m2021.03.01 15:13:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:13:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:13:15 INFO  time: compiled root in 1.89s[0m
[0m2021.03.01 15:13:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:13:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:13:21 INFO  time: compiled root in 2.76s[0m
[0m2021.03.01 15:14:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:14:51 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 15:15:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:15:56 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 15:15:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:15:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:15:58 INFO  time: compiled root in 0.99s[0m
[0m2021.03.01 15:16:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:16:11 INFO  time: compiled root in 0.27s[0m
[0m2021.03.01 15:16:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:16:23 INFO  time: compiled root in 0.41s[0m
[0m2021.03.01 15:18:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:18:46 INFO  time: compiled root in 0.28s[0m
[0m2021.03.01 15:20:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:06 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 15:20:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:11 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 15:20:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:19 INFO  time: compiled root in 0.14s[0m
Mar 01, 2021 3:20:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 714
[0m2021.03.01 15:20:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:26 INFO  time: compiled root in 0.33s[0m
[0m2021.03.01 15:20:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:20:29 INFO  time: compiled root in 1.38s[0m
[0m2021.03.01 15:20:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 15:20:32 INFO  time: compiled root in 2.02s[0m
Mar 01, 2021 3:20:33 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.01 15:20:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:20:34 INFO  time: compiled root in 0.72s[0m
[0m2021.03.01 15:46:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:41 INFO  time: compiled root in 0.2s[0m
Mar 01, 2021 3:46:44 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.01 15:46:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:45 INFO  time: compiled root in 0.32s[0m
[0m2021.03.01 15:46:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:46 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 15:46:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:49 INFO  time: compiled root in 0.24s[0m
[0m2021.03.01 15:46:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:52 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 15:46:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:46:54 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 15:47:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:13 INFO  time: compiled root in 0.24s[0m
[0m2021.03.01 15:47:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:15 INFO  time: compiled root in 0.24s[0m
Mar 01, 2021 3:47:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 901
[0m2021.03.01 15:47:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:46 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 15:47:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:47 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 15:47:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:55 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 15:47:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:47:59 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 15:48:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 15:48:04 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 16:57:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:57:06 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 16:57:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:57:11 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 16:57:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 16:57:15 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 17:04:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:04:36 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:07:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:07:12 INFO  time: compiled root in 0.2s[0m
Mar 01, 2021 5:07:19 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1258
[0m2021.03.01 17:07:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:07:42 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:07:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:07:54 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 17:07:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:07:59 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:08:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:05 INFO  time: compiled root in 0.26s[0m
[0m2021.03.01 17:08:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:07 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:08:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:09 INFO  time: compiled root in 0.19s[0m
Mar 01, 2021 5:08:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1454
[0m2021.03.01 17:08:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:16 INFO  time: compiled root in 0.21s[0m
Mar 01, 2021 5:08:18 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 01, 2021 5:08:19 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.01 17:08:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:20 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:08:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:24 INFO  time: compiled root in 0.28s[0m
[0m2021.03.01 17:08:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:28 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 17:08:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:39 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:08:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:08:43 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:10:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:10:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:10:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:10:35 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 5:12:26 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1607
[0m2021.03.01 17:13:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:13:03 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:13:05 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83: error: ) expected but ( found
        StructField(Container: struct (nullable = true)
                                      ^[0m
[0m2021.03.01 17:13:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:13:23 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:41: stale bloop error: ')' expected but ':' found.
        StructField(Container: Container: struct (nullable = true)
                                        ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:50: stale bloop error: ')' expected but '(' found.
        StructField(Container: Container: struct (nullable = true)
                                                 ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true)
                              ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ';' expected but '(' found.
 |    |-- Filename: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:41: stale bloop error: ')' expected but ':' found.
        StructField(Container: Container: struct (nullable = true)
                                        ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:50: stale bloop error: ')' expected but '(' found.
        StructField(Container: Container: struct (nullable = true)
                                                 ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true)
                              ^[0m
[0m2021.03.01 17:14:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ';' expected but '(' found.
 |    |-- Filename: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:41: stale bloop error: ')' expected but ':' found.
        StructField(Container: Container: struct (nullable = true)
                                        ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:50: stale bloop error: ')' expected but '(' found.
        StructField(Container: Container: struct (nullable = true)
                                                 ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true)
                              ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ';' expected but '(' found.
 |    |-- Filename: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:14:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:16 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83:21: stale bloop error: unclosed string literal
        StructField("Container: Container: struct (nullable = true)
                    ^[0m
[0m2021.03.01 17:14:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:142:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:14:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:19 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:23 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:14:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:30 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:14:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:36 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:14:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:52 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:14:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:14:54 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:15:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:15:12 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true) (nullable = true
                         ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true)
                         ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true) (nullable = true
                         ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true)
                         ^[0m
[0m2021.03.01 17:16:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true) (nullable = true
                         ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:31: stale bloop error: ')' expected but '(' found.
 |    |-- Compressed: boolean (nullable = true
                              ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:28: stale bloop error: ')' expected but '(' found.
 |    |-- Filename: string (nullable = true
                           ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:33: stale bloop error: ')' expected but '(' found.
 |    |-- Gzip-Metadata: struct (nullable = true
                                ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: ')' expected but '(' found.
 |    |    |-- Deflate-Length: string (nullable = true
                                      ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Footer-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:38: stale bloop error: ')' expected but '(' found.
 |    |    |-- Header-Length: string (nullable = true
                                     ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:37: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-CRC: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:40: stale bloop error: ')' expected but '(' found.
 |    |    |-- Inflated-Length: string (nullable = true
                                       ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:26: stale bloop error: ')' expected but '(' found.
 |    |-- Offset: string (nullable = true)
                         ^[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:16:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:16:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:84:11: stale bloop error: unclosed string literal
 |    |-- "Compressed: boolean (nullable = true
          ^[0m
[0m2021.03.01 17:16:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:21 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:16:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:22 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:16:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:27 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:16:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:29 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 17:16:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:34 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:16:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:16:37 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:17:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:17:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:03 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:17:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:08 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 17:17:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:10 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:17:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:17:13 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85: error: ) expected but ( found
Filename: string (nullable = true
                 ^[0m
[0m2021.03.01 17:17:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:24 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:17:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:31 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:17:34 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86: error: ) expected but ( found
Filename: string (nullable = true
                 ^[0m
[0m2021.03.01 17:17:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:40 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:17:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:43 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:17:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:46 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:17:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:17:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:17:59 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:18:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:01 INFO  time: compiled root in 0.16s[0m
[0m2021.03.01 17:18:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:07 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:18:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:08 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:18:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:18:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:14 INFO  time: compiled root in 0.17s[0m
[0m2021.03.01 17:18:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:18:26 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:19:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:26 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:19:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:31 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:19:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:37 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:19:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:38 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:19:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:44 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:19:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:45 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:19:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:54 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:19:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:19:57 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:20:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:01 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:20:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:09 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:49: stale bloop error: ')' expected but '(' found.
            StructField("Gzip-Metadata": struct (S = true
                                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true) (nullable = true
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:94:33: stale bloop error: ')' expected but '(' found.
StructField(Compressed: boolean (nullable = true
                                ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:30: stale bloop error: ')' expected but '(' found.
StructField(Filename: string (nullable = true
                             ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:35: stale bloop error: ')' expected but '(' found.
StructField(Gzip-Metadata: struct (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:36: stale bloop error: ')' expected but '(' found.
StructField(Deflate-Length: string (nullable = true
                                   ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:35: stale bloop error: ')' expected but '(' found.
StructField(Footer-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:35: stale bloop error: ')' expected but '(' found.
StructField(Header-Length: string (nullable = true
                                  ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:34: stale bloop error: ')' expected but '(' found.
StructField(Inflated-CRC: string (nullable = true
                                 ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: ')' expected but '(' found.
StructField(Inflated-Length: string (nullable = true
                                    ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:28: stale bloop error: ')' expected but '(' found.
StructField(Offset: string (nullable = true)
                           ^[0m
[0m2021.03.01 17:20:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:5: stale bloop error: ')' expected but 'val' found.
    val df = spark.read
    ^[0m
[0m2021.03.01 17:20:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:25: stale bloop error: unclosed string literal
            StructField("Gzip-Metadata, struct (S = true
                        ^[0m
[0m2021.03.01 17:20:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:25: stale bloop error: unclosed string literal
            StructField("Gzip-Metadata, struct (S = true
                        ^[0m
[0m2021.03.01 17:20:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:20:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:20:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:25: stale bloop error: unclosed string literal
            StructField("Gzip-Metadata, struct (S = true
                        ^[0m
[0m2021.03.01 17:20:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:20:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:25: stale bloop error: unclosed string literal
            StructField("Gzip-Metadata, struct (S = true
                        ^[0m
[0m2021.03.01 17:20:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:20:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:25: stale bloop error: unclosed string literal
            StructField("Gzip-Metadata, struct (S = true
                        ^[0m
[0m2021.03.01 17:20:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.01 17:20:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:20:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:28 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:20:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:29 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:20:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:20:36 INFO  time: compiled root in 0.13s[0m
Mar 01, 2021 5:21:01 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2428
[0m2021.03.01 17:21:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:13 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:21:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:21:16 INFO  time: compiled root in 0.91s[0m
[0m2021.03.01 17:21:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:19 INFO  time: compiled root in 0.27s[0m
[0m2021.03.01 17:21:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:21:22 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 17:21:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:21:24 INFO  time: compiled root in 0.81s[0m
[0m2021.03.01 17:21:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:32 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:21:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:35 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:21:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:37 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 17:21:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:21:43 INFO  time: compiled root in 0.2s[0m
Mar 01, 2021 5:21:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2661
[0m2021.03.01 17:24:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:10 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:24:11 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83: error: ) expected but ( found
        Compressed: boolean (nullable = true)
                            ^[0m
[0m2021.03.01 17:24:15 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:83: error: ) expected but ( found
        Compressed: boolean (nullable = true)
                            ^[0m
[0m2021.03.01 17:24:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:24 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:24:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:27 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:24:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:33 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 17:24:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:38 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:24:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:42 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 17:24:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:44 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:24:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:48 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 17:24:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:55 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:24:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:24:59 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 17:25:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:01 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 17:25:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:05 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 17:25:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:09 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 17:25:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:13 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 17:25:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:15 INFO  time: compiled root in 0.23s[0m
[0m2021.03.01 17:25:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:17 INFO  time: compiled root in 0.1s[0m
Mar 01, 2021 5:25:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2828
[0m2021.03.01 17:25:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:36 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:25:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:25:40 INFO  time: compiled root in 0.19s[0m
Mar 01, 2021 5:25:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2857
[0m2021.03.01 17:26:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:01 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:26:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:14 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:26:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:17 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:26:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:21 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:26:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:23 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:26:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:27 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:26:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:32 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:26:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:36 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:26:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:38 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:26:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:41 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:26:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:43 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:26:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:44 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:26:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:55 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 17:26:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:26:58 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:27:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:01 INFO  time: compiled root in 0.25s[0m
[0m2021.03.01 17:27:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:02 INFO  time: compiled root in 0.22s[0m
[0m2021.03.01 17:27:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:11 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:27:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:27:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:16 INFO  time: compiled root in 0.1s[0m
Mar 01, 2021 5:27:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3120
[0m2021.03.01 17:27:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:25 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:27:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:30 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:27:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:34 INFO  time: compiled root in 0.16s[0m
[0m2021.03.01 17:27:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:38 INFO  time: compiled root in 98ms[0m
Mar 01, 2021 5:27:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3184
[0m2021.03.01 17:27:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:27:55 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:28:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:02 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:28:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:41 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:28:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:44 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:28:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:48 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:28:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:28:50 INFO  time: compiled root in 97ms[0m
Mar 01, 2021 5:28:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3277
[0m2021.03.01 17:29:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:14 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:29:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:16 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:29:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:29:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:27 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:29:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:34 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:29:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:38 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:29:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:49 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:29:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:52 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:29:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:29:57 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:30:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:00 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:06 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:30:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:07 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:30:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:13 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:30:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:16 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:30:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:21 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:30:32 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95: error: ) expected but val found
    val df = spark.read
    ^[0m
[0m2021.03.01 17:30:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:46 INFO  time: compiled root in 0.29s[0m
[0m2021.03.01 17:30:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:30:50 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:31:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:31:13 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:31:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:31:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:31:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:31:57 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:32:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:05 INFO  time: compiled root in 0.27s[0m
[0m2021.03.01 17:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:18 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:32:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:24 INFO  time: compiled root in 0.27s[0m
[0m2021.03.01 17:32:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:28 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:32:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:32:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:32:53 INFO  time: compiled root in 0.1s[0m
[0m2021.03.01 17:33:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:33:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:01 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:33:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:33:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:25 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:33:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:33:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:33:56 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:34:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:34:04 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:34:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:34:33 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:34:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:34:48 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:35:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:35:00 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:35:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:35:01 INFO  time: compiled root in 0.14s[0m
[0m2021.03.01 17:35:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:35:12 INFO  time: compiled root in 0.26s[0m
[0m2021.03.01 17:35:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:35:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:36:20 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95: error: ) expected but val found
    val df = spark.read
    ^[0m
[0m2021.03.01 17:36:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:36:43 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:38:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:38:34 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:38:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:38:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:38:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:38:56 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:39:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:39:04 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:39:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:39:14 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:39:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:39:15 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:40:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:40:20 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:40:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:40:28 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:40:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:40:29 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:40:30 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:93: error: illegal start of simple expression
          ),)
            ^[0m
[0m2021.03.01 17:41:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:21 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:41:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:25 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:41:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:27 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:41:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:31 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:41:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:34 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:41:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:37 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:41:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:41:43 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:47:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:47:53 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:48:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:01 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:48:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:30 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:48:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:41 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:48:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:46 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:48:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:51 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:48:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:48:55 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:49:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:49:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:49:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:49:45 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:49:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:49:51 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:49:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:49:54 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:50:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:50:00 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:50:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:50:12 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:50:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:50:52 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:50:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:50:56 INFO  time: compiled root in 0.13s[0m
[0m2021.03.01 17:51:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:51:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:03 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 17:51:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:06 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 17:51:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:08 INFO  time: compiled root in 0.15s[0m
[0m2021.03.01 17:51:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:51:09 INFO  time: compiled root in 0.9s[0m
[0m2021.03.01 17:51:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:51:13 INFO  time: compiled root in 0.98s[0m
[0m2021.03.01 17:51:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:51:55 INFO  time: compiled root in 0.32s[0m
[0m2021.03.01 17:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:52:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:52:04 INFO  time: compiled root in 0.92s[0m
[0m2021.03.01 17:52:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:52:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:52:07 INFO  time: compiled root in 1s[0m
[0m2021.03.01 17:53:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:53:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:53:19 INFO  time: compiled root in 0.99s[0m
[0m2021.03.01 17:53:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:53:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:53:24 INFO  time: compiled root in 1.39s[0m
[0m2021.03.01 17:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:53:27 INFO  time: compiled root in 0.41s[0m
[0m2021.03.01 17:54:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:54:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:54:29 INFO  time: compiled root in 0.79s[0m
[0m2021.03.01 17:54:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:54:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:54:33 INFO  time: compiled root in 0.94s[0m
Mar 01, 2021 5:56:02 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4549
[0m2021.03.01 17:56:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:56:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:56:26 INFO  time: compiled root in 1.04s[0m
[0m2021.03.01 17:56:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:56:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:56:28 INFO  time: compiled root in 0.83s[0m
[0m2021.03.01 17:56:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 17:56:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 17:56:33 INFO  time: compiled root in 1.04s[0m
Mar 01, 2021 5:57:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4588
Mar 01, 2021 6:00:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4595
Mar 01, 2021 6:00:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4603
[0m2021.03.01 18:01:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:34 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 18:01:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:44 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 18:01:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:45 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 18:01:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:53 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 18:01:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:56 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 18:01:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:01:59 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 18:02:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:05 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 18:02:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:10 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 18:02:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:12 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 18:02:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:26 INFO  time: compiled root in 0.12s[0m
[0m2021.03.01 18:02:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:02:28 INFO  time: compiled root in 0.8s[0m
[0m2021.03.01 18:02:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:02:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:02:38 INFO  time: compiled root in 1.07s[0m
[0m2021.03.01 18:07:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:07:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:07:08 INFO  time: compiled root in 0.89s[0m
[0m2021.03.01 18:08:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:08:18 INFO  time: compiled root in 0.81s[0m
[0m2021.03.01 18:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:27 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 18:08:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:28 INFO  time: compiled root in 0.18s[0m
[0m2021.03.01 18:08:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:41 INFO  time: compiled root in 0.11s[0m
[0m2021.03.01 18:08:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:08:44 INFO  time: compiled root in 1.14s[0m
[0m2021.03.01 18:08:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:08:50 INFO  time: compiled root in 1.06s[0m
[0m2021.03.01 18:08:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:08:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:08:55 INFO  time: compiled root in 0.94s[0m
Mar 01, 2021 6:09:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4956
[0m2021.03.01 18:09:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:09:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:09:57 INFO  time: compiled root in 0.98s[0m
[0m2021.03.01 18:10:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:10:05 INFO  time: compiled root in 1.28s[0m
[0m2021.03.01 18:10:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:10:16 INFO  time: compiled root in 2s[0m
[0m2021.03.01 18:10:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:10:18 INFO  time: compiled root in 1.07s[0m
[0m2021.03.01 18:10:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:21 INFO  time: compiled root in 0.2s[0m
[0m2021.03.01 18:10:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:24 INFO  time: compiled root in 0.21s[0m
[0m2021.03.01 18:10:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:10:26 INFO  time: compiled root in 0.15s[0m
Mar 01, 2021 6:10:32 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5018
[0m2021.03.01 18:12:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:12:11 INFO  time: compiled root in 0.11s[0m
Mar 01, 2021 6:12:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5042
Mar 01, 2021 6:12:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5049
[0m2021.03.01 18:12:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:12:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:12:44 INFO  time: compiled root in 1.5s[0m
[0m2021.03.01 18:12:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:12:46 INFO  time: compiled root in 0.22s[0m
Mar 01, 2021 6:12:51 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5095
[0m2021.03.01 18:13:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:13:01 INFO  time: compiled root in 0.19s[0m
[0m2021.03.01 18:13:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:13:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:13:06 INFO  time: compiled root in 1.03s[0m
[0m2021.03.01 18:15:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:16:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:16:00 INFO  time: compiled root in 1.05s[0m
[0m2021.03.01 18:16:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:16:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:16:24 INFO  time: compiled root in 1.59s[0m
[0m2021.03.01 18:21:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.01 18:21:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.01 18:21:18 INFO  time: compiled root in 0.99s[0m
Mar 01, 2021 6:51:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5175
[0m2021.03.02 10:48:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:48:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 10:48:30 INFO  time: compiled root in 1.59s[0m
[0m2021.03.02 10:48:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 10:48:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 10:48:44 INFO  time: compiled root in 1.59s[0m
[0m2021.03.02 11:15:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:13 INFO  time: compiled root in 0.38s[0m
[0m2021.03.02 11:15:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:15:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:15:16 INFO  time: compiled root in 1.38s[0m
[0m2021.03.02 11:21:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:36 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 11:21:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:21:41 INFO  time: compiled root in 1.32s[0m
[0m2021.03.02 11:21:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:21:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:21:48 INFO  time: compiled root in 2.28s[0m
[0m2021.03.02 11:24:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:24:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:24:56 INFO  time: compiled root in 1.28s[0m
[0m2021.03.02 11:24:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:25:00 INFO  time: compiled root in 1.4s[0m
[0m2021.03.02 11:25:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:08 INFO  time: compiled root in 0.3s[0m
[0m2021.03.02 11:25:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:25:11 INFO  time: compiled root in 1.62s[0m
[0m2021.03.02 11:25:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:26 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 11:25:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:25:33 INFO  time: compiled root in 1.56s[0m
[0m2021.03.02 11:25:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:36 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 11:25:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:25:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:25:41 INFO  time: compiled root in 2s[0m
[0m2021.03.02 11:26:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:26:03 INFO  time: compiled root in 0.59s[0m
[0m2021.03.02 11:26:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:26:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:26:12 INFO  time: compiled root in 2.65s[0m
[0m2021.03.02 11:26:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:26:14 INFO  time: compiled root in 0.51s[0m
[0m2021.03.02 11:26:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:26:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:26:19 INFO  time: compiled root in 2.18s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala

import scala.language.implicitConversions

import scala.collection.{ mutable, immutable, generic }
import immutable.StringOps
import mutable.ArrayOps
import generic.CanBuildFrom
import scala.annotation.{ elidable, implicitNotFound }
import scala.annotation.elidable.ASSERTION
import scala.io.StdIn

/** The `Predef` object provides definitions that are accessible in all Scala
 *  compilation units without explicit qualification.
 *
 *  === Commonly Used Types ===
 *  Predef provides type aliases for types which are commonly used, such as
 *  the immutable collection types [[scala.collection.immutable.Map]],
 *  [[scala.collection.immutable.Set]], and the [[scala.collection.immutable.List]]
 *  constructors ([[scala.collection.immutable.::]] and
 *  [[scala.collection.immutable.Nil]]).
 *
 *  === Console Output ===
 *  For basic console output, `Predef` provides convenience methods [[print(x:Any* print]] and [[println(x:Any* println]],
 *  which are aliases of the methods in the object [[scala.Console]].
 *
 *  === Assertions ===
 *  A set of `assert` functions are provided for use as a way to document
 *  and dynamically check invariants in code. Invocations of `assert` can be elided
 *  at compile time by providing the command line option `-Xdisable-assertions`,
 *  which raises `-Xelide-below` above `elidable.ASSERTION`, to the `scalac` command.
 *
 *  Variants of `assert` intended for use with static analysis tools are also
 *  provided: `assume`, `require` and `ensuring`. `require` and `ensuring` are
 *  intended for use as a means of design-by-contract style specification
 *  of pre- and post-conditions on functions, with the intention that these
 *  specifications could be consumed by a static analysis tool. For instance,
 *
 *  {{{
 *  def addNaturals(nats: List[Int]): Int = {
 *    require(nats forall (_ >= 0), "List contains negative numbers")
 *    nats.foldLeft(0)(_ + _)
 *  } ensuring(_ >= 0)
 *  }}}
 *
 *  The declaration of `addNaturals` states that the list of integers passed should
 *  only contain natural numbers (i.e. non-negative), and that the result returned
 *  will also be natural. `require` is distinct from `assert` in that if the
 *  condition fails, then the caller of the function is to blame rather than a
 *  logical error having been made within `addNaturals` itself. `ensuring` is a
 *  form of `assert` that declares the guarantee the function is providing with
 *  regards to its return value.
 *
 *  === Implicit Conversions ===
 *  A number of commonly applied implicit conversions are also defined here, and
 *  in the parent type [[scala.LowPriorityImplicits]]. Implicit conversions
 *  are provided for the "widening" of numeric values, for instance, converting a
 *  Short value to a Long value as required, and to add additional higher-order
 *  functions to Array values. These are described in more detail in the documentation of [[scala.Array]].
 *
 * @groupname utilities Utility Methods
 * @groupprio utilities 10
 *
 * @groupname assertions Assertions
 * @groupprio assertions 20
 * @groupdesc assertions These methods support program verification and runtime correctness.
 *
 * @groupname console-output Console Output
 * @groupprio console-output 30
 * @groupdesc console-output These methods provide output via the console.
 *
 * @groupname type-constraints Type Constraints
 * @groupprio type-constraints 40
 * @groupdesc type-constraints These entities allows constraints between types to be stipulated.
 *
 * @groupname aliases Aliases
 * @groupprio aliases 50
 * @groupdesc aliases These aliases bring selected immutable types into scope without any imports.
 *
 * @groupname conversions-string String Conversions
 * @groupprio conversions-string 60
 * @groupdesc conversions-string Conversions to and from String and StringOps.
 *
 * @groupname implicit-classes-any Implicit Classes
 * @groupprio implicit-classes-any 70
 * @groupdesc implicit-classes-any These implicit classes add useful extension methods to every type.
 *
 * @groupname implicit-classes-char CharSequence Conversions
 * @groupprio implicit-classes-char 80
 * @groupdesc implicit-classes-char These implicit classes add CharSequence methods to Array[Char] and IndexedSeq[Char] instances.
 *
 * @groupname conversions-java-to-anyval Java to Scala
 * @groupprio conversions-java-to-anyval 90
 * @groupdesc conversions-java-to-anyval Implicit conversion from Java primitive wrapper types to Scala equivalents.
 *
 * @groupname conversions-anyval-to-java Scala to Java
 * @groupprio conversions-anyval-to-java 100
 * @groupdesc conversions-anyval-to-java Implicit conversion from Scala AnyVals to Java primitive wrapper types equivalents.
 *
 * @groupname conversions-array-to-wrapped-array Array to WrappedArray
 * @groupprio conversions-array-to-wrapped-array 110
 * @groupdesc conversions-array-to-wrapped-array Conversions from Arrays to WrappedArrays.
 */
object Predef extends LowPriorityImplicits with DeprecatedPredef {
  /**
   * Retrieve the runtime representation of a class type. `classOf[T]` is equivalent to
   * the class literal `T.class` in Java.
   *
   * @example {{{
   * val listClass = classOf[List[_]]
   * // listClass is java.lang.Class[List[_]] = class scala.collection.immutable.List
   *
   * val mapIntString = classOf[Map[Int,String]]
   * // mapIntString is java.lang.Class[Map[Int,String]] = interface scala.collection.immutable.Map
   * }}}
   * @group utilities
   */
  def classOf[T]: Class[T] = null // This is a stub method. The actual implementation is filled in by the compiler.

  /** The `String` type in Scala has methods that come either from the underlying
   *  Java String (see the documentation corresponding to your Java version, for
   *  example [[http://docs.oracle.com/javase/8/docs/api/java/lang/String.html]]) or
   *  are added implicitly through [[scala.collection.immutable.StringOps]].
   *  @group aliases
   */
  type String        = java.lang.String
  /**  @group aliases */
  type Class[T]      = java.lang.Class[T]

  // miscellaneous -----------------------------------------------------
  scala.`package`                         // to force scala package object to be seen.
  scala.collection.immutable.List         // to force Nil, :: to be seen.

  /**  @group aliases */
  type Function[-A, +B] = Function1[A, B]

  /**  @group aliases */
  type Map[A, +B] = immutable.Map[A, B]
  /**  @group aliases */
  type Set[A]     = immutable.Set[A]
  /**  @group aliases */
  val Map         = immutable.Map
  /**  @group aliases */
  val Set         = immutable.Set

  // Manifest types, companions, and incantations for summoning
  @annotation.implicitNotFound(msg = "No ClassManifest available for ${T}.")
  @deprecated("use `scala.reflect.ClassTag` instead", "2.10.0")
  type ClassManifest[T] = scala.reflect.ClassManifest[T]
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  type OptManifest[T]   = scala.reflect.OptManifest[T]
  @annotation.implicitNotFound(msg = "No Manifest available for ${T}.")
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use `scala.reflect.ClassTag` (to capture erasures) or scala.reflect.runtime.universe.TypeTag (to capture types) or both instead", "2.10.0")
  type Manifest[T]      = scala.reflect.Manifest[T]
  @deprecated("use `scala.reflect.ClassTag` instead", "2.10.0")
  val ClassManifest     = scala.reflect.ClassManifest
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use `scala.reflect.ClassTag` (to capture erasures) or scala.reflect.runtime.universe.TypeTag (to capture types) or both instead", "2.10.0")
  val Manifest          = scala.reflect.Manifest
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  val NoManifest        = scala.reflect.NoManifest

  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("use scala.reflect.classTag[T] and scala.reflect.runtime.universe.typeTag[T] instead", "2.10.0")
  def manifest[T](implicit m: Manifest[T])           = m
  @deprecated("use scala.reflect.classTag[T] instead", "2.10.0")
  def classManifest[T](implicit m: ClassManifest[T]) = m
  // TODO undeprecated until Scala reflection becomes non-experimental
  // @deprecated("this notion doesn't have a corresponding concept in 2.10, because scala.reflect.runtime.universe.TypeTag can capture arbitrary types. Use type tags instead of manifests, and there will be no need in opt manifests.", "2.10.0")
  def optManifest[T](implicit m: OptManifest[T])     = m

  // Minor variations on identity functions
  /** @group utilities */
  @inline def identity[A](x: A): A         = x    // @see `conforms` for the implicit version
  /** @group utilities */
  @inline def implicitly[T](implicit e: T) = e    // for summoning implicit values from the nether world -- TODO: when dependent method types are on by default, give this result type `e.type`, so that inliner has better chance of knowing which method to inline in calls like `implicitly[MatchingStrategy[Option]].zero`
  /** @group utilities */
  @inline def locally[T](x: T): T  = x    // to communicate intent and avoid unmoored statements

  // assertions ---------------------------------------------------------

  /** Tests an expression, throwing an `AssertionError` if false.
   *  Calls to this method will not be generated if `-Xelide-below`
   *  is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assertion   the expression to test
   *  @group assertions
   */
  @elidable(ASSERTION)
  def assert(assertion: Boolean) {
    if (!assertion)
      throw new java.lang.AssertionError("assertion failed")
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  Calls to this method will not be generated if `-Xelide-below`
   *  is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assertion   the expression to test
   *  @param message     a String to include in the failure message
   *  @group assertions
   */
  @elidable(ASSERTION) @inline
  final def assert(assertion: Boolean, message: => Any) {
    if (!assertion)
      throw new java.lang.AssertionError("assertion failed: "+ message)
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  This method differs from assert only in the intent expressed:
   *  assert contains a predicate which needs to be proven, while
   *  assume contains an axiom for a static checker.  Calls to this method
   *  will not be generated if `-Xelide-below` is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assumption   the expression to test
   *  @group assertions
   */
  @elidable(ASSERTION)
  def assume(assumption: Boolean) {
    if (!assumption)
      throw new java.lang.AssertionError("assumption failed")
  }

  /** Tests an expression, throwing an `AssertionError` if false.
   *  This method differs from assert only in the intent expressed:
   *  assert contains a predicate which needs to be proven, while
   *  assume contains an axiom for a static checker.  Calls to this method
   *  will not be generated if `-Xelide-below` is greater than `ASSERTION`.
   *
   *  @see [[scala.annotation.elidable elidable]]
   *  @param assumption   the expression to test
   *  @param message      a String to include in the failure message
   *  @group assertions
   */
  @elidable(ASSERTION) @inline
  final def assume(assumption: Boolean, message: => Any) {
    if (!assumption)
      throw new java.lang.AssertionError("assumption failed: "+ message)
  }

  /** Tests an expression, throwing an `IllegalArgumentException` if false.
   *  This method is similar to `assert`, but blames the caller of the method
   *  for violating the condition.
   *
   *  @param requirement   the expression to test
   *  @group assertions
   */
  def require(requirement: Boolean) {
    if (!requirement)
      throw new IllegalArgumentException("requirement failed")
  }

  /** Tests an expression, throwing an `IllegalArgumentException` if false.
   *  This method is similar to `assert`, but blames the caller of the method
   *  for violating the condition.
   *
   *  @param requirement   the expression to test
   *  @param message       a String to include in the failure message
   *  @group assertions
   */
  @inline final def require(requirement: Boolean, message: => Any) {
    if (!requirement)
      throw new IllegalArgumentException("requirement failed: "+ message)
  }

  /** `???` can be used for marking methods that remain to be implemented.
   *  @throws NotImplementedError
   *  @group utilities
   */
  def ??? : Nothing = throw new NotImplementedError

  // tupling ------------------------------------------------------------

  @deprecated("use built-in tuple syntax or Tuple2 instead", "2.11.0")
  type Pair[+A, +B] = Tuple2[A, B]
  @deprecated("use built-in tuple syntax or Tuple2 instead", "2.11.0")
  object Pair {
    def apply[A, B](x: A, y: B) = Tuple2(x, y)
    def unapply[A, B](x: Tuple2[A, B]): Option[Tuple2[A, B]] = Some(x)
  }

  @deprecated("use built-in tuple syntax or Tuple3 instead", "2.11.0")
  type Triple[+A, +B, +C] = Tuple3[A, B, C]
  @deprecated("use built-in tuple syntax or Tuple3 instead", "2.11.0")
  object Triple {
    def apply[A, B, C](x: A, y: B, z: C) = Tuple3(x, y, z)
    def unapply[A, B, C](x: Tuple3[A, B, C]): Option[Tuple3[A, B, C]] = Some(x)
  }

  // implicit classes -----------------------------------------------------

  /** @group implicit-classes-any */
  implicit final class ArrowAssoc[A](private val self: A) extends AnyVal {
    @inline def -> [B](y: B): Tuple2[A, B] = Tuple2(self, y)
    def →[B](y: B): Tuple2[A, B] = ->(y)
  }

  /** @group implicit-classes-any */
  implicit final class Ensuring[A](private val self: A) extends AnyVal {
    def ensuring(cond: Boolean): A = { assert(cond); self }
    def ensuring(cond: Boolean, msg: => Any): A = { assert(cond, msg); self }
    def ensuring(cond: A => Boolean): A = { assert(cond(self)); self }
    def ensuring(cond: A => Boolean, msg: => Any): A = { assert(cond(self), msg); self }
  }

  /** @group implicit-classes-any */
  implicit final class StringFormat[A](private val self: A) extends AnyVal {
    /** Returns string formatted according to given `format` string.
     *  Format strings are as for `String.format`
     *  (@see java.lang.String.format).
     */
    @inline def formatted(fmtstr: String): String = fmtstr format self
  }

  // scala/bug#8229 retaining the pre 2.11 name for source compatibility in shadowing this implicit
  /** @group implicit-classes-any */
  implicit final class any2stringadd[A](private val self: A) extends AnyVal {
    def +(other: String): String = String.valueOf(self) + other
  }

  implicit final class RichException(private val self: Throwable) extends AnyVal {
    import scala.compat.Platform.EOL
    @deprecated("use Throwable#getStackTrace", "2.11.0") def getStackTraceString = self.getStackTrace().mkString("", EOL, EOL)
  }

  // Sadly we have to do `@deprecatedName(null, "2.12.0")` because
  // `@deprecatedName(since="2.12.0")` incurs a warning about
  //   Usage of named or default arguments transformed this annotation constructor call into a block.
  //   The corresponding AnnotationInfo will contain references to local values and default getters
  //   instead of the actual argument trees
  // and `@deprecatedName(Symbol("<none>"), "2.12.0")` crashes scalac with
  //   scala.reflect.internal.Symbols$CyclicReference: illegal cyclic reference involving object Symbol
  // in run/repl-no-imports-no-predef-power.scala.
  /** @group implicit-classes-char */
  implicit final class SeqCharSequence(@deprecated("will be made private", "2.12.0") @deprecatedName(null, "2.12.0") val __sequenceOfChars: scala.collection.IndexedSeq[Char]) extends CharSequence {
    def length: Int                                     = __sequenceOfChars.length
    def charAt(index: Int): Char                        = __sequenceOfChars(index)
    def subSequence(start: Int, end: Int): CharSequence = new SeqCharSequence(__sequenceOfChars.slice(start, end))
    override def toString                               = __sequenceOfChars mkString ""
  }

  /** @group implicit-classes-char */
  implicit final class ArrayCharSequence(@deprecated("will be made private", "2.12.0") @deprecatedName(null, "2.12.0") val __arrayOfChars: Array[Char]) extends CharSequence {
    def length: Int                                     = __arrayOfChars.length
    def charAt(index: Int): Char                        = __arrayOfChars(index)
    def subSequence(start: Int, end: Int): CharSequence = new runtime.ArrayCharSequence(__arrayOfChars, start, end)
    override def toString                               = __arrayOfChars mkString ""
  }

  implicit val StringCanBuildFrom: CanBuildFrom[String, Char, String] = new CanBuildFrom[String, Char, String] {
    def apply(from: String) = apply()
    def apply()             = mutable.StringBuilder.newBuilder
  }

  /** @group conversions-string */
  @inline implicit def augmentString(x: String): StringOps = new StringOps(x)
  /** @group conversions-string */
  @inline implicit def unaugmentString(x: StringOps): String = x.repr

  // printing -----------------------------------------------------------

  /** Prints an object to `out` using its `toString` method.
   *
   *  @param x the object to print; may be null.
   *  @group console-output
   */
  def print(x: Any) = Console.print(x)

  /** Prints a newline character on the default output.
   *  @group console-output
   */
  def println() = Console.println()

  /** Prints out an object to the default output, followed by a newline character.
   *
   *  @param x the object to print.
   *  @group console-output
   */
  def println(x: Any) = Console.println(x)

  /** Prints its arguments as a formatted string to the default output,
   *  based on a string pattern (in a fashion similar to printf in C).
   *
   *  The interpretation of the formatting patterns is described in
   *  [[java.util.Formatter]].
   *
   *  Consider using the [[scala.StringContext.f f interpolator]] as more type safe and idiomatic.
   *
   *  @param text the pattern for formatting the arguments.
   *  @param args the arguments used to instantiating the pattern.
   *  @throws java.lang.IllegalArgumentException if there was a problem with the format string or arguments
   *
   *  @see [[scala.StringContext.f StringContext.f]]
   *  @group console-output
   */
  def printf(text: String, xs: Any*) = Console.print(text.format(xs: _*))

  // views --------------------------------------------------------------

  implicit def tuple2ToZippedOps[T1, T2](x: (T1, T2))                           = new runtime.Tuple2Zipped.Ops(x)
  implicit def tuple3ToZippedOps[T1, T2, T3](x: (T1, T2, T3))                   = new runtime.Tuple3Zipped.Ops(x)

  implicit def genericArrayOps[T](xs: Array[T]): ArrayOps[T] = (xs match {
    case x: Array[AnyRef]  => refArrayOps[AnyRef](x)
    case x: Array[Boolean] => booleanArrayOps(x)
    case x: Array[Byte]    => byteArrayOps(x)
    case x: Array[Char]    => charArrayOps(x)
    case x: Array[Double]  => doubleArrayOps(x)
    case x: Array[Float]   => floatArrayOps(x)
    case x: Array[Int]     => intArrayOps(x)
    case x: Array[Long]    => longArrayOps(x)
    case x: Array[Short]   => shortArrayOps(x)
    case x: Array[Unit]    => unitArrayOps(x)
    case null              => null
  }).asInstanceOf[ArrayOps[T]]

  implicit def booleanArrayOps(xs: Array[Boolean]): ArrayOps.ofBoolean   = new ArrayOps.ofBoolean(xs)
  implicit def byteArrayOps(xs: Array[Byte]): ArrayOps.ofByte            = new ArrayOps.ofByte(xs)
  implicit def charArrayOps(xs: Array[Char]): ArrayOps.ofChar            = new ArrayOps.ofChar(xs)
  implicit def doubleArrayOps(xs: Array[Double]): ArrayOps.ofDouble      = new ArrayOps.ofDouble(xs)
  implicit def floatArrayOps(xs: Array[Float]): ArrayOps.ofFloat         = new ArrayOps.ofFloat(xs)
  implicit def intArrayOps(xs: Array[Int]): ArrayOps.ofInt               = new ArrayOps.ofInt(xs)
  implicit def longArrayOps(xs: Array[Long]): ArrayOps.ofLong            = new ArrayOps.ofLong(xs)
  implicit def refArrayOps[T <: AnyRef](xs: Array[T]): ArrayOps.ofRef[T] = new ArrayOps.ofRef[T](xs)
  implicit def shortArrayOps(xs: Array[Short]): ArrayOps.ofShort         = new ArrayOps.ofShort(xs)
  implicit def unitArrayOps(xs: Array[Unit]): ArrayOps.ofUnit            = new ArrayOps.ofUnit(xs)

  // "Autoboxing" and "Autounboxing" ---------------------------------------------------

  /** @group conversions-anyval-to-java */
  implicit def byte2Byte(x: Byte): java.lang.Byte             = x.asInstanceOf[java.lang.Byte]
  /** @group conversions-anyval-to-java */
  implicit def short2Short(x: Short): java.lang.Short         = x.asInstanceOf[java.lang.Short]
  /** @group conversions-anyval-to-java */
  implicit def char2Character(x: Char): java.lang.Character   = x.asInstanceOf[java.lang.Character]
  /** @group conversions-anyval-to-java */
  implicit def int2Integer(x: Int): java.lang.Integer         = x.asInstanceOf[java.lang.Integer]
  /** @group conversions-anyval-to-java */
  implicit def long2Long(x: Long): java.lang.Long             = x.asInstanceOf[java.lang.Long]
  /** @group conversions-anyval-to-java */
  implicit def float2Float(x: Float): java.lang.Float         = x.asInstanceOf[java.lang.Float]
  /** @group conversions-anyval-to-java */
  implicit def double2Double(x: Double): java.lang.Double     = x.asInstanceOf[java.lang.Double]
  /** @group conversions-anyval-to-java */
  implicit def boolean2Boolean(x: Boolean): java.lang.Boolean = x.asInstanceOf[java.lang.Boolean]

  /** @group conversions-java-to-anyval */
  implicit def Byte2byte(x: java.lang.Byte): Byte             = x.asInstanceOf[Byte]
  /** @group conversions-java-to-anyval */
  implicit def Short2short(x: java.lang.Short): Short         = x.asInstanceOf[Short]
  /** @group conversions-java-to-anyval */
  implicit def Character2char(x: java.lang.Character): Char   = x.asInstanceOf[Char]
  /** @group conversions-java-to-anyval */
  implicit def Integer2int(x: java.lang.Integer): Int         = x.asInstanceOf[Int]
  /** @group conversions-java-to-anyval */
  implicit def Long2long(x: java.lang.Long): Long             = x.asInstanceOf[Long]
  /** @group conversions-java-to-anyval */
  implicit def Float2float(x: java.lang.Float): Float         = x.asInstanceOf[Float]
  /** @group conversions-java-to-anyval */
  implicit def Double2double(x: java.lang.Double): Double     = x.asInstanceOf[Double]
  /** @group conversions-java-to-anyval */
  implicit def Boolean2boolean(x: java.lang.Boolean): Boolean = x.asInstanceOf[Boolean]

  // Type Constraints --------------------------------------------------------------

  /**
   * An instance of `A <:< B` witnesses that `A` is a subtype of `B`.
   * Requiring an implicit argument of the type `A <:< B` encodes
   * the generalized constraint `A <: B`.
   *
   * @note we need a new type constructor `<:<` and evidence `conforms`,
   * as reusing `Function1` and `identity` leads to ambiguities in
   * case of type errors (`any2stringadd` is inferred)
   *
   * To constrain any abstract type T that's in scope in a method's
   * argument list (not just the method's own type parameters) simply
   * add an implicit argument of type `T <:< U`, where `U` is the required
   * upper bound; or for lower-bounds, use: `L <:< T`, where `L` is the
   * required lower bound.
   *
   * In part contributed by Jason Zaugg.
   * @group type-constraints
   */
  @implicitNotFound(msg = "Cannot prove that ${From} <:< ${To}.")
  sealed abstract class <:<[-From, +To] extends (From => To) with Serializable
  private[this] final val singleton_<:< = new <:<[Any,Any] { def apply(x: Any): Any = x }
  // The dollar prefix is to dodge accidental shadowing of this method
  // by a user-defined method of the same name (scala/bug#7788).
  // The collections rely on this method.
  /** @group type-constraints */
  implicit def $conforms[A]: A <:< A = singleton_<:<.asInstanceOf[A <:< A]

  @deprecated("use `implicitly[T <:< U]` or `identity` instead.", "2.11.0")
  def conforms[A]: A <:< A = $conforms[A]

  /** An instance of `A =:= B` witnesses that the types `A` and `B` are equal.
   *
   * @see `<:<` for expressing subtyping constraints
   * @group type-constraints
   */
  @implicitNotFound(msg = "Cannot prove that ${From} =:= ${To}.")
  sealed abstract class =:=[From, To] extends (From => To) with Serializable
  private[this] final val singleton_=:= = new =:=[Any,Any] { def apply(x: Any): Any = x }
  /** @group type-constraints */
  object =:= {
     implicit def tpEquals[A]: A =:= A = singleton_=:=.asInstanceOf[A =:= A]
  }

  /** A type for which there is always an implicit value.
   *  @see [[scala.Array$]], method `fallbackCanBuildFrom`
   */
  class DummyImplicit

  object DummyImplicit {

    /** An implicit value yielding a `DummyImplicit`.
     *   @see [[scala.Array$]], method `fallbackCanBuildFrom`
     */
    implicit def dummyImplicit: DummyImplicit = new DummyImplicit
  }
}

private[scala] trait DeprecatedPredef {
  self: Predef.type =>

  // Deprecated stubs for any who may have been calling these methods directly.
  @deprecated("use `ArrowAssoc`", "2.11.0") def any2ArrowAssoc[A](x: A): ArrowAssoc[A]                                      = new ArrowAssoc(x)
  @deprecated("use `Ensuring`", "2.11.0") def any2Ensuring[A](x: A): Ensuring[A]                                            = new Ensuring(x)
  @deprecated("use `StringFormat`", "2.11.0") def any2stringfmt(x: Any): StringFormat[Any]                                  = new StringFormat(x)
  @deprecated("use `Throwable` directly", "2.11.0") def exceptionWrapper(exc: Throwable)                                    = new RichException(exc)
  @deprecated("use `SeqCharSequence`", "2.11.0") def seqToCharSequence(xs: scala.collection.IndexedSeq[Char]): CharSequence = new SeqCharSequence(xs)
  @deprecated("use `ArrayCharSequence`", "2.11.0") def arrayToCharSequence(xs: Array[Char]): CharSequence                   = new ArrayCharSequence(xs)

  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLine(): String                 = StdIn.readLine()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLine(text: String, args: Any*) = StdIn.readLine(text, args: _*)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readBoolean()                      = StdIn.readBoolean()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readByte()                         = StdIn.readByte()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readShort()                        = StdIn.readShort()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readChar()                         = StdIn.readChar()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readInt()                          = StdIn.readInt()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readLong()                         = StdIn.readLong()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readFloat()                        = StdIn.readFloat()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readDouble()                       = StdIn.readDouble()
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf(format: String)              = StdIn.readf(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf1(format: String)             = StdIn.readf1(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf2(format: String)             = StdIn.readf2(format)
  @deprecated("use the method in `scala.io.StdIn`", "2.11.0") def readf3(format: String)             = StdIn.readf3(format)
}

/** The `LowPriorityImplicits` class provides implicit values that
*  are valid in all Scala compilation units without explicit qualification,
*  but that are partially overridden by higher-priority conversions in object
*  `Predef`.
*
*  @author  Martin Odersky
*  @since 2.8
*/
// scala/bug#7335 Parents of Predef are defined in the same compilation unit to avoid
// cyclic reference errors compiling the standard library *without* a previously
// compiled copy on the classpath.
private[scala] abstract class LowPriorityImplicits {
  import mutable.WrappedArray
  import immutable.WrappedString

  /** We prefer the java.lang.* boxed types to these wrappers in
   *  any potential conflicts.  Conflicts do exist because the wrappers
   *  need to implement ScalaNumber in order to have a symmetric equals
   *  method, but that implies implementing java.lang.Number as well.
   *
   *  Note - these are inlined because they are value classes, but
   *  the call to xxxWrapper is not eliminated even though it does nothing.
   *  Even inlined, every call site does a no-op retrieval of Predef's MODULE$
   *  because maybe loading Predef has side effects!
   */
  @inline implicit def byteWrapper(x: Byte)       = new runtime.RichByte(x)
  @inline implicit def shortWrapper(x: Short)     = new runtime.RichShort(x)
  @inline implicit def intWrapper(x: Int)         = new runtime.RichInt(x)
  @inline implicit def charWrapper(c: Char)       = new runtime.RichChar(c)
  @inline implicit def longWrapper(x: Long)       = new runtime.RichLong(x)
  @inline implicit def floatWrapper(x: Float)     = new runtime.RichFloat(x)
  @inline implicit def doubleWrapper(x: Double)   = new runtime.RichDouble(x)
  @inline implicit def booleanWrapper(x: Boolean) = new runtime.RichBoolean(x)

  /** @group conversions-array-to-wrapped-array */
  implicit def genericWrapArray[T](xs: Array[T]): WrappedArray[T] =
    if (xs eq null) null
    else WrappedArray.make(xs)

  // Since the JVM thinks arrays are covariant, one 0-length Array[AnyRef]
  // is as good as another for all T <: AnyRef.  Instead of creating 100,000,000
  // unique ones by way of this implicit, let's share one.
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapRefArray[T <: AnyRef](xs: Array[T]): WrappedArray[T] = {
    if (xs eq null) null
    else if (xs.length == 0) WrappedArray.empty[T]
    else new WrappedArray.ofRef[T](xs)
  }

  /** @group conversions-array-to-wrapped-array */
  implicit def wrapIntArray(xs: Array[Int]): WrappedArray[Int] = if (xs ne null) new WrappedArray.ofInt(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapDoubleArray(xs: Array[Double]): WrappedArray[Double] = if (xs ne null) new WrappedArray.ofDouble(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapLongArray(xs: Array[Long]): WrappedArray[Long] = if (xs ne null) new WrappedArray.ofLong(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapFloatArray(xs: Array[Float]): WrappedArray[Float] = if (xs ne null) new WrappedArray.ofFloat(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapCharArray(xs: Array[Char]): WrappedArray[Char] = if (xs ne null) new WrappedArray.ofChar(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapByteArray(xs: Array[Byte]): WrappedArray[Byte] = if (xs ne null) new WrappedArray.ofByte(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapShortArray(xs: Array[Short]): WrappedArray[Short] = if (xs ne null) new WrappedArray.ofShort(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapBooleanArray(xs: Array[Boolean]): WrappedArray[Boolean] = if (xs ne null) new WrappedArray.ofBoolean(xs) else null
  /** @group conversions-array-to-wrapped-array */
  implicit def wrapUnitArray(xs: Array[Unit]): WrappedArray[Unit] = if (xs ne null) new WrappedArray.ofUnit(xs) else null

  /** @group conversions-string */
  implicit def wrapString(s: String): WrappedString = if (s ne null) new WrappedString(s) else null
  /** @group conversions-string */
  implicit def unwrapString(ws: WrappedString): String = if (ws ne null) ws.self else null

  implicit def fallbackStringCanBuildFrom[T]: CanBuildFrom[String, T, immutable.IndexedSeq[T]] =
    new CanBuildFrom[String, T, immutable.IndexedSeq[T]] {
      def apply(from: String) = immutable.IndexedSeq.newBuilder[T]
      def apply() = immutable.IndexedSeq.newBuilder[T]
    }
}

[0m2021.03.02 11:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:27:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:27:47 INFO  time: compiled root in 2.1s[0m
[0m2021.03.02 11:27:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:27:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:27:49 INFO  time: compiled root in 1.52s[0m
[0m2021.03.02 11:27:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:27:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:27:51 INFO  time: compiled root in 1.74s[0m
[0m2021.03.02 11:29:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:15 INFO  time: compiled root in 0.37s[0m
[0m2021.03.02 11:29:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:29:18 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 11:31:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:31:52 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 11:32:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:32:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:32:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:32:05 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 11:32:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:32:26 INFO  time: compiled root in 0.25s[0m
[0m2021.03.02 11:32:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:32:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:32:31 INFO  time: compiled root in 1.04s[0m
[0m2021.03.02 11:32:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:32:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 11:32:57 INFO  time: compiled root in 1.38s[0m
[0m2021.03.02 11:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:33 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 11:58:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:37 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 11:58:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:40 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:58:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:44 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:58:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:46 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:58:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:51 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 11:58:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:54 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 11:58:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:58 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 11:58:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:58:59 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 11:59:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 11:59:03 INFO  time: compiled root in 0.25s[0m
[0m2021.03.02 12:00:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:00:49 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 12:00:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:00:53 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 12:00:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:00:55 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 12:01:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:17 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 12:01:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:20 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 12:01:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 12:01:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 12:01:29 INFO  time: compiled root in 0.97s[0m
[0m2021.03.02 12:01:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:35 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 12:01:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:40 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 12:01:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:44 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 12:01:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:48 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 12:01:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:01:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 12:01:50 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 12:02:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:02:06 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 12:02:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:02:09 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 12:02:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:02:20 INFO  time: compiled root in 0.21s[0m
Mar 02, 2021 12:02:23 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: null
[0m2021.03.02 12:02:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:02:24 INFO  time: compiled root in 0.26s[0m
[0m2021.03.02 12:50:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:16 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 12:50:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:21 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 12:50:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:23 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 12:50:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:25 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 12:50:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:27 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 12:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:47 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 12:50:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:53 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 12:50:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:54 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 12:50:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 12:50:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 12:50:59 INFO  time: compiled root in 1.12s[0m
[0m2021.03.02 14:10:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:10:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:10:43 INFO  time: compiled root in 1.27s[0m
[0m2021.03.02 14:10:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:10:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:10:45 INFO  time: compiled root in 1.41s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala

import scala.collection.generic._
import scala.collection.{ mutable, immutable }
import mutable.{ ArrayBuilder, ArraySeq }
import scala.reflect.ClassTag
import scala.runtime.ScalaRunTime.{ array_apply, array_update }

/** Contains a fallback builder for arrays when the element type
 *  does not have a class tag. In that case a generic array is built.
 */
class FallbackArrayBuilding {

  /** A builder factory that generates a generic array.
   *  Called instead of `Array.newBuilder` if the element type of an array
   *  does not have a class tag. Note that fallbackBuilder factory
   *  needs an implicit parameter (otherwise it would not be dominated in
   *  implicit search by `Array.canBuildFrom`). We make sure that
   *  implicit search is always successful.
   */
  implicit def fallbackCanBuildFrom[T](implicit m: DummyImplicit): CanBuildFrom[Array[_], T, ArraySeq[T]] =
    new CanBuildFrom[Array[_], T, ArraySeq[T]] {
      def apply(from: Array[_]) = ArraySeq.newBuilder[T]
      def apply() = ArraySeq.newBuilder[T]
    }
}

/** Utility methods for operating on arrays.
 *  For example:
 *  {{{
 *  val a = Array(1, 2)
 *  val b = Array.ofDim[Int](2)
 *  val c = Array.concat(a, b)
 *  }}}
 *  where the array objects `a`, `b` and `c` have respectively the values
 *  `Array(1, 2)`, `Array(0, 0)` and `Array(1, 2, 0, 0)`.
 *
 *  @author Martin Odersky
 *  @since  1.0
 */
object Array extends FallbackArrayBuilding {
  val emptyBooleanArray = new Array[Boolean](0)
  val emptyByteArray    = new Array[Byte](0)
  val emptyCharArray    = new Array[Char](0)
  val emptyDoubleArray  = new Array[Double](0)
  val emptyFloatArray   = new Array[Float](0)
  val emptyIntArray     = new Array[Int](0)
  val emptyLongArray    = new Array[Long](0)
  val emptyShortArray   = new Array[Short](0)
  val emptyObjectArray  = new Array[Object](0)

  implicit def canBuildFrom[T](implicit tag: ClassTag[T]): CanBuildFrom[Array[_], T, Array[T]] = {
    val cls = tag.runtimeClass
    (if (cls.isPrimitive) {
      cls match {
        case java.lang.Integer.TYPE   => cbfIntArray
        case java.lang.Double.TYPE    => cbfDoubleArray
        case java.lang.Long.TYPE      => cbfLongArray
        case java.lang.Float.TYPE     => cbfFloatArray
        case java.lang.Character.TYPE => cbfCharArray
        case java.lang.Byte.TYPE      => cbfByteArray
        case java.lang.Short.TYPE     => cbfShortArray
        case java.lang.Boolean.TYPE   => cbfBooleanArray
        case java.lang.Void.TYPE      => cbfUnitArray
      }
    } else if (cls == ObjectClass) {
      cbfObjectArray
    } else {
      refCBF[T with AnyRef](tag.asInstanceOf[ClassTag[T with AnyRef]])
    }).asInstanceOf[CanBuildFrom[Array[_], T, Array[T]]]
  }
  private[this] val ObjectClass = classOf[Object]

  private[this] val cbfBooleanArray = new CanBuildFrom[Array[_], Boolean, Array[Boolean]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofBoolean()
    def apply() = new ArrayBuilder.ofBoolean()
  }

  private[this] val cbfByteArray    = new CanBuildFrom[Array[_], Byte, Array[Byte]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofByte()
    def apply() = new ArrayBuilder.ofByte()
  }

  private[this] val cbfCharArray    = new CanBuildFrom[Array[_], Char, Array[Char]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofChar()
    def apply() = new ArrayBuilder.ofChar()
  }

  private[this] val cbfDoubleArray  = new CanBuildFrom[Array[_], Double, Array[Double]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofDouble()
    def apply() = new ArrayBuilder.ofDouble()
  }

  private[this] val cbfFloatArray   = new CanBuildFrom[Array[_], Float, Array[Float]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofFloat()
    def apply() = new ArrayBuilder.ofFloat()
  }

  private[this] val cbfIntArray     = new CanBuildFrom[Array[_], Int, Array[Int]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofInt()
    def apply() = new ArrayBuilder.ofInt()
  }

  private[this] val cbfLongArray    = new CanBuildFrom[Array[_], Long, Array[Long]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofLong()
    def apply() = new ArrayBuilder.ofLong()
  }

  private[this] val cbfShortArray   = new CanBuildFrom[Array[_], Short, Array[Short]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofShort()
    def apply() = new ArrayBuilder.ofShort()
  }

  private[this] val cbfUnitArray    = new CanBuildFrom[Array[_], Unit, Array[Unit]] {
    def apply(from: Array[_]) = new ArrayBuilder.ofUnit()
    def apply() = new ArrayBuilder.ofUnit()
  }

  private[this] val cbfObjectArray  = refCBF[Object]
  private[this] def refCBF[T <: AnyRef](implicit t: ClassTag[T]): CanBuildFrom[Array[_], T, Array[T]] =
    new CanBuildFrom[Array[_], T, Array[T]] {
      def apply(from: Array[_]) = new ArrayBuilder.ofRef[T]()(t)
      def apply() = new ArrayBuilder.ofRef[T]()(t)
    }

  /**
   * Returns a new [[scala.collection.mutable.ArrayBuilder]].
   */
  def newBuilder[T](implicit t: ClassTag[T]): ArrayBuilder[T] = ArrayBuilder.make[T]()(t)

  private def slowcopy(src : AnyRef,
                       srcPos : Int,
                       dest : AnyRef,
                       destPos : Int,
                       length : Int) {
    var i = srcPos
    var j = destPos
    val srcUntil = srcPos + length
    while (i < srcUntil) {
      array_update(dest, j, array_apply(src, i))
      i += 1
      j += 1
    }
  }

  /** Copy one array to another.
   *  Equivalent to Java's
   *    `System.arraycopy(src, srcPos, dest, destPos, length)`,
   *  except that this also works for polymorphic and boxed arrays.
   *
   *  Note that the passed-in `dest` array will be modified by this call.
   *
   *  @param src the source array.
   *  @param srcPos  starting position in the source array.
   *  @param dest destination array.
   *  @param destPos starting position in the destination array.
   *  @param length the number of array elements to be copied.
   *
   *  @see `java.lang.System#arraycopy`
   */
  def copy(src: AnyRef, srcPos: Int, dest: AnyRef, destPos: Int, length: Int) {
    val srcClass = src.getClass
    if (srcClass.isArray && dest.getClass.isAssignableFrom(srcClass))
      java.lang.System.arraycopy(src, srcPos, dest, destPos, length)
    else
      slowcopy(src, srcPos, dest, destPos, length)
  }

  /** Returns an array of length 0 */
  def empty[T: ClassTag]: Array[T] = new Array[T](0)

  /** Creates an array with given elements.
   *
   *  @param xs the elements to put in the array
   *  @return an array containing all elements from xs.
   */
  // Subject to a compiler optimization in Cleanup.
  // Array(e0, ..., en) is translated to { val a = new Array(3); a(i) = ei; a }
  def apply[T: ClassTag](xs: T*): Array[T] = {
    val array = new Array[T](xs.length)
    var i = 0
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Boolean` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Boolean, xs: Boolean*): Array[Boolean] = {
    val array = new Array[Boolean](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Byte` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Byte, xs: Byte*): Array[Byte] = {
    val array = new Array[Byte](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Short` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Short, xs: Short*): Array[Short] = {
    val array = new Array[Short](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Char` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Char, xs: Char*): Array[Char] = {
    val array = new Array[Char](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Int` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Int, xs: Int*): Array[Int] = {
    val array = new Array[Int](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Long` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Long, xs: Long*): Array[Long] = {
    val array = new Array[Long](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Float` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Float, xs: Float*): Array[Float] = {
    val array = new Array[Float](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Double` objects */
  // Subject to a compiler optimization in Cleanup, see above.
  def apply(x: Double, xs: Double*): Array[Double] = {
    val array = new Array[Double](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates an array of `Unit` objects */
  def apply(x: Unit, xs: Unit*): Array[Unit] = {
    val array = new Array[Unit](xs.length + 1)
    array(0) = x
    var i = 1
    for (x <- xs.iterator) { array(i) = x; i += 1 }
    array
  }

  /** Creates array with given dimensions */
  def ofDim[T: ClassTag](n1: Int): Array[T] =
    new Array[T](n1)
  /** Creates a 2-dimensional array */
  def ofDim[T: ClassTag](n1: Int, n2: Int): Array[Array[T]] = {
    val arr: Array[Array[T]] = (new Array[Array[T]](n1): Array[Array[T]])
    for (i <- 0 until n1) arr(i) = new Array[T](n2)
    arr
    // tabulate(n1)(_ => ofDim[T](n2))
  }
  /** Creates a 3-dimensional array */
  def ofDim[T: ClassTag](n1: Int, n2: Int, n3: Int): Array[Array[Array[T]]] =
    tabulate(n1)(_ => ofDim[T](n2, n3))
  /** Creates a 4-dimensional array */
  def ofDim[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int): Array[Array[Array[Array[T]]]] =
    tabulate(n1)(_ => ofDim[T](n2, n3, n4))
  /** Creates a 5-dimensional array */
  def ofDim[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int, n5: Int): Array[Array[Array[Array[Array[T]]]]] =
    tabulate(n1)(_ => ofDim[T](n2, n3, n4, n5))

  /** Concatenates all arrays into a single array.
   *
   *  @param xss the given arrays
   *  @return   the array created from concatenating `xss`
   */
  def concat[T: ClassTag](xss: Array[T]*): Array[T] = {
    val b = newBuilder[T]
    b.sizeHint(xss.map(_.length).sum)
    for (xs <- xss) b ++= xs
    b.result()
  }

  /** Returns an array that contains the results of some element computation a number
   *  of times.
   *
   *  Note that this means that `elem` is computed a total of n times:
   *  {{{
   * scala> Array.fill(3){ math.random }
   * res3: Array[Double] = Array(0.365461167592537, 1.550395944913685E-4, 0.7907242137333306)
   *  }}}
   *
   *  @param   n  the number of elements desired
   *  @param   elem the element computation
   *  @return an Array of size n, where each element contains the result of computing
   *  `elem`.
   */
  def fill[T: ClassTag](n: Int)(elem: => T): Array[T] = {
    val b = newBuilder[T]
    b.sizeHint(n)
    var i = 0
    while (i < n) {
      b += elem
      i += 1
    }
    b.result()
  }

  /** Returns a two-dimensional array that contains the results of some element
   *  computation a number of times.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   elem the element computation
   */
  def fill[T: ClassTag](n1: Int, n2: Int)(elem: => T): Array[Array[T]] =
    tabulate(n1)(_ => fill(n2)(elem))

  /** Returns a three-dimensional array that contains the results of some element
   *  computation a number of times.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3nd dimension
   *  @param   elem the element computation
   */
  def fill[T: ClassTag](n1: Int, n2: Int, n3: Int)(elem: => T): Array[Array[Array[T]]] =
    tabulate(n1)(_ => fill(n2, n3)(elem))

  /** Returns a four-dimensional array that contains the results of some element
   *  computation a number of times.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3nd dimension
   *  @param   n4  the number of elements in the 4th dimension
   *  @param   elem the element computation
   */
  def fill[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int)(elem: => T): Array[Array[Array[Array[T]]]] =
    tabulate(n1)(_ => fill(n2, n3, n4)(elem))

  /** Returns a five-dimensional array that contains the results of some element
   *  computation a number of times.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3nd dimension
   *  @param   n4  the number of elements in the 4th dimension
   *  @param   n5  the number of elements in the 5th dimension
   *  @param   elem the element computation
   */
  def fill[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int, n5: Int)(elem: => T): Array[Array[Array[Array[Array[T]]]]] =
    tabulate(n1)(_ => fill(n2, n3, n4, n5)(elem))

  /** Returns an array containing values of a given function over a range of integer
   *  values starting from 0.
   *
   *  @param  n   The number of elements in the array
   *  @param  f   The function computing element values
   *  @return A traversable consisting of elements `f(0),f(1), ..., f(n - 1)`
   */
  def tabulate[T: ClassTag](n: Int)(f: Int => T): Array[T] = {
    val b = newBuilder[T]
    b.sizeHint(n)
    var i = 0
    while (i < n) {
      b += f(i)
      i += 1
    }
    b.result()
  }

  /** Returns a two-dimensional array containing values of a given function
   *  over ranges of integer values starting from `0`.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   f   The function computing element values
   */
  def tabulate[T: ClassTag](n1: Int, n2: Int)(f: (Int, Int) => T): Array[Array[T]] =
    tabulate(n1)(i1 => tabulate(n2)(f(i1, _)))

  /** Returns a three-dimensional array containing values of a given function
   *  over ranges of integer values starting from `0`.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3rd dimension
   *  @param   f   The function computing element values
   */
  def tabulate[T: ClassTag](n1: Int, n2: Int, n3: Int)(f: (Int, Int, Int) => T): Array[Array[Array[T]]] =
    tabulate(n1)(i1 => tabulate(n2, n3)(f(i1, _, _)))

  /** Returns a four-dimensional array containing values of a given function
   *  over ranges of integer values starting from `0`.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3rd dimension
   *  @param   n4  the number of elements in the 4th dimension
   *  @param   f   The function computing element values
   */
  def tabulate[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int)(f: (Int, Int, Int, Int) => T): Array[Array[Array[Array[T]]]] =
    tabulate(n1)(i1 => tabulate(n2, n3, n4)(f(i1, _, _, _)))

  /** Returns a five-dimensional array containing values of a given function
   *  over ranges of integer values starting from `0`.
   *
   *  @param   n1  the number of elements in the 1st dimension
   *  @param   n2  the number of elements in the 2nd dimension
   *  @param   n3  the number of elements in the 3rd dimension
   *  @param   n4  the number of elements in the 4th dimension
   *  @param   n5  the number of elements in the 5th dimension
   *  @param   f   The function computing element values
   */
  def tabulate[T: ClassTag](n1: Int, n2: Int, n3: Int, n4: Int, n5: Int)(f: (Int, Int, Int, Int, Int) => T): Array[Array[Array[Array[Array[T]]]]] =
    tabulate(n1)(i1 => tabulate(n2, n3, n4, n5)(f(i1, _, _, _, _)))

  /** Returns an array containing a sequence of increasing integers in a range.
   *
   *  @param start  the start value of the array
   *  @param end    the end value of the array, exclusive (in other words, this is the first value '''not''' returned)
   *  @return  the array with values in range `start, start + 1, ..., end - 1`
   *  up to, but excluding, `end`.
   */
  def range(start: Int, end: Int): Array[Int] = range(start, end, 1)

  /** Returns an array containing equally spaced values in some integer interval.
   *
   *  @param start the start value of the array
   *  @param end   the end value of the array, exclusive (in other words, this is the first value '''not''' returned)
   *  @param step  the increment value of the array (may not be zero)
   *  @return      the array with values in `start, start + step, ...` up to, but excluding `end`
   */
  def range(start: Int, end: Int, step: Int): Array[Int] = {
    if (step == 0) throw new IllegalArgumentException("zero step")
    val b = newBuilder[Int]
    b.sizeHint(immutable.Range.count(start, end, step, isInclusive = false))

    var i = start
    while (if (step < 0) end < i else i < end) {
      b += i
      i += step
    }
    b.result()
  }

  /** Returns an array containing repeated applications of a function to a start value.
   *
   *  @param start the start value of the array
   *  @param len   the number of elements returned by the array
   *  @param f     the function that is repeatedly applied
   *  @return      the array returning `len` values in the sequence `start, f(start), f(f(start)), ...`
   */
  def iterate[T: ClassTag](start: T, len: Int)(f: T => T): Array[T] = {
    val b = newBuilder[T]

    if (len > 0) {
      b.sizeHint(len)
      var acc = start
      var i = 1
      b += acc

      while (i < len) {
        acc = f(acc)
        i += 1
        b += acc
      }
    }
    b.result()
  }

  /** Called in a pattern match like `{ case Array(x,y,z) => println('3 elements')}`.
   *
   *  @param x the selector value
   *  @return  sequence wrapped in a [[scala.Some]], if `x` is a Seq, otherwise `None`
   */
  def unapplySeq[T](x: Array[T]): Option[IndexedSeq[T]] =
    if (x == null) None else Some(x.toIndexedSeq)
    // !!! the null check should to be necessary, but without it 2241 fails. Seems to be a bug
    // in pattern matcher.  @PP: I noted in #4364 I think the behavior is correct.
}

/** Arrays are mutable, indexed collections of values. `Array[T]` is Scala's representation
 *  for Java's `T[]`.
 *
 *  {{{
 *  val numbers = Array(1, 2, 3, 4)
 *  val first = numbers(0) // read the first element
 *  numbers(3) = 100 // replace the 4th array element with 100
 *  val biggerNumbers = numbers.map(_ * 2) // multiply all numbers by two
 *  }}}
 *
 *  Arrays make use of two common pieces of Scala syntactic sugar, shown on lines 2 and 3 of the above
 *  example code.
 *  Line 2 is translated into a call to `apply(Int)`, while line 3 is translated into a call to
 *  `update(Int, T)`.
 *
 *  Two implicit conversions exist in [[scala.Predef]] that are frequently applied to arrays: a conversion
 *  to [[scala.collection.mutable.ArrayOps]] (shown on line 4 of the example above) and a conversion
 *  to [[scala.collection.mutable.WrappedArray]] (a subtype of [[scala.collection.Seq]]).
 *  Both types make available many of the standard operations found in the Scala collections API.
 *  The conversion to `ArrayOps` is temporary, as all operations defined on `ArrayOps` return an `Array`,
 *  while the conversion to `WrappedArray` is permanent as all operations return a `WrappedArray`.
 *
 *  The conversion to `ArrayOps` takes priority over the conversion to `WrappedArray`. For instance,
 *  consider the following code:
 *
 *  {{{
 *  val arr = Array(1, 2, 3)
 *  val arrReversed = arr.reverse
 *  val seqReversed : Seq[Int] = arr.reverse
 *  }}}
 *
 *  Value `arrReversed` will be of type `Array[Int]`, with an implicit conversion to `ArrayOps` occurring
 *  to perform the `reverse` operation. The value of `seqReversed`, on the other hand, will be computed
 *  by converting to `WrappedArray` first and invoking the variant of `reverse` that returns another
 *  `WrappedArray`.
 *
 *  @author Martin Odersky
 *  @since  1.0
 *  @see [[http://www.scala-lang.org/files/archive/spec/2.12/ Scala Language Specification]], for in-depth information on the transformations the Scala compiler makes on Arrays (Sections 6.6 and 6.15 respectively.)
 *  @see [[http://docs.scala-lang.org/sips/completed/scala-2-8-arrays.html "Scala 2.8 Arrays"]] the Scala Improvement Document detailing arrays since Scala 2.8.
 *  @see [[http://docs.scala-lang.org/overviews/collections/arrays.html "The Scala 2.8 Collections' API"]] section on `Array` by Martin Odersky for more information.
 *  @hideImplicitConversion scala.Predef.booleanArrayOps
 *  @hideImplicitConversion scala.Predef.byteArrayOps
 *  @hideImplicitConversion scala.Predef.charArrayOps
 *  @hideImplicitConversion scala.Predef.doubleArrayOps
 *  @hideImplicitConversion scala.Predef.floatArrayOps
 *  @hideImplicitConversion scala.Predef.intArrayOps
 *  @hideImplicitConversion scala.Predef.longArrayOps
 *  @hideImplicitConversion scala.Predef.refArrayOps
 *  @hideImplicitConversion scala.Predef.shortArrayOps
 *  @hideImplicitConversion scala.Predef.unitArrayOps
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapRefArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapIntArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapDoubleArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapLongArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapFloatArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapCharArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapByteArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapShortArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapBooleanArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.wrapUnitArray
 *  @hideImplicitConversion scala.LowPriorityImplicits.genericWrapArray
 *  @define coll array
 *  @define Coll `Array`
 *  @define orderDependent
 *  @define orderDependentFold
 *  @define mayNotTerminateInf
 *  @define willNotTerminateInf
 *  @define collectExample
 *  @define undefinedorder
 *  @define thatinfo the class of the returned collection. In the standard library configuration,
 *    `That` is either `Array[B]` if an ClassTag is available for B or `ArraySeq[B]` otherwise.
 *  @define zipthatinfo $thatinfo
 *  @define bfinfo an implicit value of class `CanBuildFrom` which determines the result class `That` from the current
 *    representation type `Repr` and the new element type `B`.
 */
final class Array[T](_length: Int) extends java.io.Serializable with java.lang.Cloneable {

  /** The length of the array */
  def length: Int = throw new Error()

  /** The element at given index.
   *
   *  Indices start at `0`; `xs.apply(0)` is the first element of array `xs`.
   *  Note the indexing syntax `xs(i)` is a shorthand for `xs.apply(i)`.
   *
   *  @param    i   the index
   *  @return       the element at the given index
   *  @throws       ArrayIndexOutOfBoundsException if `i < 0` or `length <= i`
   */
  def apply(i: Int): T = throw new Error()

  /** Update the element at given index.
   *
   *  Indices start at `0`; `xs.update(i, x)` replaces the i^th^ element in the array.
   *  Note the syntax `xs(i) = x` is a shorthand for `xs.update(i, x)`.
   *
   *  @param    i   the index
   *  @param    x   the value to be written at index `i`
   *  @throws       ArrayIndexOutOfBoundsException if `i < 0` or `length <= i`
   */
  def update(i: Int, x: T) { throw new Error() }

  /** Clone the Array.
   *
   *  @return A clone of the Array.
   */
  override def clone(): Array[T] = throw new Error()
}

[0m2021.03.02 14:12:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:26 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 14:12:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:30 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 14:12:31 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123: error: ) expected but string constant found
          "inferSchema" -> "true",
          ^[0m
[0m2021.03.02 14:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:34 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 14:12:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:12:39 INFO  time: compiled root in 1.64s[0m
[0m2021.03.02 14:12:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:12:54 INFO  time: compiled root in 1.39s[0m
[0m2021.03.02 14:12:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:12:56 INFO  time: compiled root in 1.43s[0m
[0m2021.03.02 14:12:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:12:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:12:58 INFO  time: compiled root in 1.4s[0m
[0m2021.03.02 14:18:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:18:30 INFO  time: compiled root in 1.71s[0m
[0m2021.03.02 14:18:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:31 INFO  time: compiled root in 0.28s[0m
Exception in thread "pool-4-thread-1" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[0m2021.03.02 14:18:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:38 INFO  time: compiled root in 0.36s[0m
[0m2021.03.02 14:18:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:40 INFO  time: compiled root in 0.17s[0m
Mar 02, 2021 2:18:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6436
Mar 02, 2021 2:18:43 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.02 14:18:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:44 INFO  time: compiled root in 0.33s[0m
[0m2021.03.02 14:18:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:50 INFO  time: compiled root in 0.3s[0m
[0m2021.03.02 14:18:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:18:53 INFO  time: compiled root in 0.31s[0m
[0m2021.03.02 14:18:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 14:19:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 14:19:00 INFO  time: compiled root in 1.55s[0m
[0m2021.03.02 15:08:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:06 INFO  time: compiled root in 1.03s[0m
[0m2021.03.02 15:08:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:14 INFO  time: compiled root in 1.58s[0m
[0m2021.03.02 15:08:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:38 INFO  time: compiled root in 1s[0m
[0m2021.03.02 15:08:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:42 INFO  time: compiled root in 1.1s[0m
[0m2021.03.02 15:08:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:49 INFO  time: compiled root in 1s[0m
[0m2021.03.02 15:08:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:52 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 15:08:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:08:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:08:59 INFO  time: compiled root in 1.48s[0m
[0m2021.03.02 15:09:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:02 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 15:09:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:04 INFO  time: compiled root in 0.46s[0m
[0m2021.03.02 15:09:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:09 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 15:09:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:22 INFO  time: compiled root in 15ms[0m
[0m2021.03.02 15:09:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:23 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 15:09:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:09:31 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 15:09:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:09:34 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 15:09:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:36 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 15:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:38 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 15:09:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:41 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 15:09:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:09:48 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 15:09:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:50 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 15:09:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:52 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 15:09:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:09:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:09:59 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 15:10:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:10:00 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 15:10:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:10:06 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 15:10:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:10:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:10:09 INFO  time: compiled root in 0.94s[0m
[0m2021.03.02 15:21:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:21:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:21:39 INFO  time: compiled root in 0.9s[0m
[0m2021.03.02 15:22:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:22:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:22:45 INFO  time: compiled root in 1.28s[0m
[0m2021.03.02 15:25:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:25:05 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 15:25:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:25:08 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 15:25:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:25:11 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 15:25:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:25:15 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 15:25:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:25:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:25:23 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 15:26:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:26:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:26:41 INFO  time: compiled root in 1.03s[0m
[0m2021.03.02 15:26:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:26:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:26:42 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 15:27:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:27:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:27:19 INFO  time: compiled root in 1.39s[0m
[0m2021.03.02 15:28:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:28:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:28:55 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 15:29:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:29:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:29:08 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 15:29:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:29:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 15:29:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:29:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:29:23 INFO  time: compiled root in 1.43s[0m
[0m2021.03.02 15:31:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:31:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:31:33 INFO  time: compiled root in 1.13s[0m
[0m2021.03.02 15:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:36 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 15:40:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:38 INFO  time: compiled root in 0.14s[0m
[0m2021.03.02 15:40:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:40 INFO  time: compiled root in 99ms[0m
[0m2021.03.02 15:40:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:45 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 15:40:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:46 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 15:40:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:40:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:40:54 INFO  time: compiled root in 1.18s[0m
[0m2021.03.02 15:42:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:42:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:42:39 INFO  time: compiled root in 1s[0m
[0m2021.03.02 15:42:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:42:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:42:40 INFO  time: compiled root in 0.92s[0m
[0m2021.03.02 15:43:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:43:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:43:38 INFO  time: compiled root in 1.36s[0m
[0m2021.03.02 15:44:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:44:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:44:32 INFO  time: compiled root in 1.18s[0m
[0m2021.03.02 15:44:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:44:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:44:58 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 15:52:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:52:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:52:53 INFO  time: compiled root in 1.03s[0m
[0m2021.03.02 15:53:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:53:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:53:02 INFO  time: compiled root in 1.01s[0m
[0m2021.03.02 15:53:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:53:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:53:04 INFO  time: compiled root in 0.97s[0m
[0m2021.03.02 15:53:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:53:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:53:06 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 15:53:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:53:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:53:07 INFO  time: compiled root in 1.16s[0m
[0m2021.03.02 15:55:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:55:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:55:08 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 15:55:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:55:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:55:11 INFO  time: compiled root in 1.2s[0m
/*
 * Scala (https://www.scala-lang.org)
 *
 * Copyright EPFL and Lightbend, Inc.
 *
 * Licensed under Apache License 2.0
 * (http://www.apache.org/licenses/LICENSE-2.0).
 *
 * See the NOTICE file distributed with this work for
 * additional information regarding copyright ownership.
 */

package scala
package collection
package generic

import mutable.{Builder, MapBuilder}
import scala.language.higherKinds

/** A template for companion objects of `Map` and subclasses thereof.
 *
 *  @define coll map
 *  @define Coll `Map`
 *  @define factoryInfo
 *    This object provides a set of operations needed to create `$Coll` values.
 *    @author Martin Odersky
 *    @since 2.8
 *  @define canBuildFromInfo
 *    The standard `CanBuildFrom` instance for `$Coll` objects.
 *    @see CanBuildFrom
 *  @define mapCanBuildFromInfo
 *    The standard `CanBuildFrom` instance for `$Coll` objects.
 *    The created value is an instance of class `MapCanBuildFrom`.
 *    @see CanBuildFrom
 *    @see GenericCanBuildFrom
 */
abstract class GenMapFactory[CC[A, B] <: GenMap[A, B] with GenMapLike[A, B, CC[A, B]]] {

  /** The type constructor of the collection that can be built by this factory */
  type Coll = CC[_, _]

  /** An empty $Coll */
  def empty[A, B]: CC[A, B]

  /** A collection of type $Coll that contains given key/value bindings.
   *  @param elems   the key/value pairs that make up the $coll
   *  @tparam A      the type of the keys
   *  @tparam B      the type of the associated values
   *  @return        a new $coll consisting key/value pairs given by `elems`.
   */
  def apply[A, B](elems: (A, B)*): CC[A, B] = (newBuilder[A, B] ++= elems).result()

  /** The default builder for $Coll objects.
   *  @tparam A      the type of the keys
   *  @tparam B      the type of the associated values
   */
  def newBuilder[A, B]: Builder[(A, B), CC[A, B]] = new MapBuilder[A, B, CC[A, B]](empty[A, B])

  /** The standard `CanBuildFrom` class for maps.
   */
  class MapCanBuildFrom[A, B] extends CanBuildFrom[Coll, (A, B), CC[A, B]] {
    def apply(from: Coll) = newBuilder[A, B]
    def apply() = newBuilder
  }
}

[0m2021.03.02 15:55:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:55:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:29: stale bloop error: Invalid literal number
          "encoding" -> UTF-32BE,
                            ^[0m
[0m2021.03.02 15:55:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:29: stale bloop error: Invalid literal number
          "encoding" -> UTF-32BE,
                            ^[0m
[0m2021.03.02 15:55:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.02 15:55:22 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 15:55:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:29: stale bloop error: Invalid literal number
          "encoding" -> UTF-32BE,
                            ^[0m
[0m2021.03.02 15:55:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.02 15:55:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:29: stale bloop error: Invalid literal number
          "encoding" -> UTF-32BE,
                            ^[0m
[0m2021.03.02 15:55:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:154:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.02 15:55:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:55:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:55:27 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 15:55:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:55:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:55:32 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 15:57:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 15:57:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 15:57:12 INFO  time: compiled root in 1.18s[0m
[0m2021.03.02 16:00:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:00:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:00:36 INFO  time: compiled root in 1.57s[0m
something's wrong: no file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala in Array[String]RangePosition(file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala, 394, 394, 407)
something's wrong: no file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala in Array[String]RangePosition(file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala, 394, 394, 407)
[0m2021.03.02 16:00:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:00:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:00:45 INFO  time: compiled root in 1.59s[0m
[0m2021.03.02 16:00:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:00:54 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 16:00:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:00:59 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 16:01:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:01:01 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 16:01:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:01:50 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 16:01:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:01:53 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 16:02:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:02:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:02:01 INFO  time: compiled root in 1.27s[0m
[0m2021.03.02 16:07:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:07:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:07:04 INFO  time: compiled root in 1.72s[0m
Mar 02, 2021 4:07:06 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.02 16:07:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:07:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:07:11 INFO  time: compiled root in 1.44s[0m
[0m2021.03.02 16:07:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:07:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:07:14 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 16:07:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:07:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:07:17 INFO  time: compiled root in 1.21s[0m
[0m2021.03.02 16:47:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:47:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:47:03 INFO  time: compiled root in 1.29s[0m
[0m2021.03.02 16:47:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:47:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:47:13 INFO  time: compiled root in 1.24s[0m
[0m2021.03.02 16:47:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:47:13 INFO  time: compiled root in 0.26s[0m
[0m2021.03.02 16:47:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:47:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:47:16 INFO  time: compiled root in 1.16s[0m
[0m2021.03.02 16:47:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:47:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:47:21 INFO  time: compiled root in 1.28s[0m
Mar 02, 2021 4:49:29 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7613
[0m2021.03.02 16:49:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:49:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:49:47 INFO  time: compiled root in 1.29s[0m
[0m2021.03.02 16:49:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:49:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:49:49 INFO  time: compiled root in 1.19s[0m
[0m2021.03.02 16:50:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:50:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:50:22 INFO  time: compiled root in 1.57s[0m
[0m2021.03.02 16:50:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:50:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:50:39 INFO  time: compiled root in 1.45s[0m
[0m2021.03.02 16:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:50:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:50:48 INFO  time: compiled root in 1.15s[0m
[0m2021.03.02 16:53:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:53:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:53:06 INFO  time: compiled root in 1.31s[0m
[0m2021.03.02 16:53:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:53:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:53:36 INFO  time: compiled root in 1.42s[0m
Mar 02, 2021 4:55:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7738
[0m2021.03.02 16:56:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 16:56:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 16:56:03 INFO  time: compiled root in 1.35s[0m
Mar 02, 2021 5:08:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7765
Mar 02, 2021 5:08:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7774
[0m2021.03.02 17:09:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:09:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:09:02 INFO  time: compiled root in 1.44s[0m
[0m2021.03.02 17:09:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:09:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:09:08 INFO  time: compiled root in 1.65s[0m
[0m2021.03.02 17:09:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:09:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:09:12 INFO  time: compiled root in 2.02s[0m
[0m2021.03.02 17:11:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:11:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:11:32 INFO  time: compiled root in 1.47s[0m
Mar 02, 2021 5:18:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7852
[0m2021.03.02 17:18:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:18:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:18:14 INFO  time: compiled root in 1.86s[0m
[0m2021.03.02 17:18:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:18:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:18:19 INFO  time: compiled root in 2.53s[0m
[0m2021.03.02 17:20:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:20:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:20:37 INFO  time: compiled root in 2.77s[0m
[0m2021.03.02 17:21:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:21:11 INFO  time: compiled root in 0.35s[0m
Mar 02, 2021 5:21:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7920
[0m2021.03.02 17:21:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:21: stale bloop error: not enough arguments for method map: (f: Char => B)(implicit bf: scala.collection.generic.CanBuildFrom[String,B,That])That.
Unspecified value parameter f.
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 17:21:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:21: stale bloop error: not enough arguments for method map: (f: Char => B)(implicit bf: scala.collection.generic.CanBuildFrom[String,B,That])That.
Unspecified value parameter f.
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 17:21:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:21: stale bloop error: not enough arguments for method map: (f: Char => B)(implicit bf: scala.collection.generic.CanBuildFrom[String,B,That])That.
Unspecified value parameter f.
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map()
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 17:21:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:21:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:124: stale bloop error: Decimal integer literals may not have a leading zero. (Octal syntax is obsolete.)
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map(cc-index/collections/CC-MAIN-2021-04/indexes/cdx-)
                                                                                                                           ^[0m
[0m2021.03.02 17:21:49 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:21:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:124: stale bloop error: Decimal integer literals may not have a leading zero. (Octal syntax is obsolete.)
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map(cc-index/collections/CC-MAIN-2021-04/indexes/cdx-)
                                                                                                                           ^[0m
[0m2021.03.02 17:21:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:124: stale bloop error: Decimal integer literals may not have a leading zero. (Octal syntax is obsolete.)
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map(cc-index/collections/CC-MAIN-2021-04/indexes/cdx-)
                                                                                                                           ^[0m
[0m2021.03.02 17:21:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:21:57 INFO  time: compiled root in 0.34s[0m
[0m2021.03.02 17:22:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:06 INFO  time: compiled root in 0.44s[0m
[0m2021.03.02 17:22:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:90: stale bloop error: type mismatch;
 found   : String
 required: Char => ?
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map(s"cc-index/collections/CC-MAIN-2021-04/indexes/cdx-")
                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 17:22:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:192:90: stale bloop error: type mismatch;
 found   : String
 required: Char => ?
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/cc-index.paths.gz".map(s"cc-index/collections/CC-MAIN-2021-04/indexes/cdx-")
                                                                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^[0m
[0m2021.03.02 17:22:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:15 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 17:22:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:27 INFO  time: compiled root in 0.49s[0m
[0m2021.03.02 17:22:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:34 INFO  time: compiled root in 0.4s[0m
[0m2021.03.02 17:22:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:56 INFO  time: compiled root in 0.48s[0m
[0m2021.03.02 17:22:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:22:59 INFO  time: compiled root in 0.4s[0m
[0m2021.03.02 17:23:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:23:04 INFO  time: compiled root in 0.45s[0m
[0m2021.03.02 17:23:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:23:06 INFO  time: compiled root in 0.29s[0m
[0m2021.03.02 17:23:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:23:09 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 17:24:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:24:49 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 17:24:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:24:53 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 17:24:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:24:55 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:25:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:00 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 17:25:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:02 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 17:25:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:29 INFO  time: compiled root in 0.25s[0m
[0m2021.03.02 17:25:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:33 INFO  time: compiled root in 0.26s[0m
[0m2021.03.02 17:25:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:25:45 INFO  time: compiled root in 1.42s[0m
[0m2021.03.02 17:25:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:51 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 17:25:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:53 INFO  time: compiled root in 0.18s[0m
[0m2021.03.02 17:25:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:58 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 17:25:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:25:59 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 17:26:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:08 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 17:26:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:31 INFO  time: compiled root in 0.16s[0m
[0m2021.03.02 17:26:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:37 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 17:26:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:42 INFO  time: compiled root in 0.35s[0m
[0m2021.03.02 17:26:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:44 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 17:26:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:26:48 INFO  time: compiled root in 0.37s[0m
[0m2021.03.02 17:36:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:36:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 17:36:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:36:21 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:36:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:36:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:36:27 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 17:39:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:39:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:39:36 INFO  time: compiled root in 1.17s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.02 17:42:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:42:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:42:03 INFO  time: compiled root in 2.17s[0m
[0m2021.03.02 17:42:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:42:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:42:08 INFO  time: compiled root in 1.54s[0m
[0m2021.03.02 17:46:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:09 INFO  time: compiled root in 0.2s[0m
Mar 02, 2021 5:46:10 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.02 17:46:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:11 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:46:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:27 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 17:46:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:29 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 17:46:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:33 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 17:46:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:35 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 17:46:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:37 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:46:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:38 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 17:46:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:42 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 17:46:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:46:49 INFO  time: compiled root in 1.16s[0m
[0m2021.03.02 17:46:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:54 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 17:46:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:56 INFO  time: compiled root in 0.25s[0m
[0m2021.03.02 17:46:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:46:59 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 17:47:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:47:02 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 17:47:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:47:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:47:04 INFO  time: compiled root in 0.97s[0m
[0m2021.03.02 17:48:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:48:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:48:34 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 17:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:48:38 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 17:48:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:48:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:48:43 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 17:48:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:48:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:48:45 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 17:49:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:06 INFO  time: compiled root in 1.06s[0m
[0m2021.03.02 17:49:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:12 INFO  time: compiled root in 1.14s[0m
[0m2021.03.02 17:49:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:13 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 17:49:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:16 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 17:49:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:19 INFO  time: compiled root in 1.55s[0m
[0m2021.03.02 17:49:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:23 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 17:49:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:27 INFO  time: compiled root in 1.44s[0m
[0m2021.03.02 17:49:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:29 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 17:49:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:35 INFO  time: compiled root in 0.27s[0m
[0m2021.03.02 17:49:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:38 INFO  time: compiled root in 0.11s[0m
[0m2021.03.02 17:49:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:41 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 17:49:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:47 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 17:49:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:49:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:49:55 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 17:51:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:51:24 INFO  time: compiled root in 99ms[0m
[0m2021.03.02 17:51:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:51:31 INFO  time: compiled root in 0.15s[0m
[0m2021.03.02 17:51:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:51:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:51:34 INFO  time: compiled root in 0.9s[0m
[0m2021.03.02 17:54:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:54:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:54:29 INFO  time: compiled root in 1.19s[0m
[0m2021.03.02 17:54:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:54:40 INFO  time: compiled root in 0.19s[0m
[0m2021.03.02 17:54:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:54:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:54:47 INFO  time: compiled root in 0.93s[0m
[0m2021.03.02 17:54:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:54:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:54:51 INFO  time: compiled root in 0.91s[0m
[0m2021.03.02 17:55:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:55:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:55:01 INFO  time: compiled root in 0.87s[0m
[0m2021.03.02 17:55:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:55:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:199:57: stale bloop error: unclosed string literal
    .filter($"_c0" contains "/jobs" and $"_c0" contains "tech|technology|computer\")
                                                        ^[0m
[0m2021.03.02 17:55:03 INFO  time: compiled root in 0.17s[0m
[0m2021.03.02 17:55:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:199:57: stale bloop error: unclosed string literal
    .filter($"_c0" contains "/jobs" and $"_c0" contains "tech|technology|computer\")
                                                        ^[0m
[0m2021.03.02 17:55:04 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:199:57: stale bloop error: unclosed string literal
    .filter($"_c0" contains "/jobs" and $"_c0" contains "tech|technology|computer\")
                                                        ^[0m
[0m2021.03.02 17:55:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:55:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:55:12 INFO  time: compiled root in 0.9s[0m
[0m2021.03.02 17:56:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:56:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:56:09 INFO  time: compiled root in 0.92s[0m
[0m2021.03.02 17:56:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:56:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:56:10 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 17:56:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:56:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:56:22 INFO  time: compiled root in 0.93s[0m
[0m2021.03.02 17:56:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:56:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:56:27 INFO  time: compiled root in 1.21s[0m
[0m2021.03.02 17:58:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:15 INFO  time: compiled root in 0.92s[0m
[0m2021.03.02 17:58:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:19 INFO  time: compiled root in 1.04s[0m
[0m2021.03.02 17:58:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:20 INFO  time: compiled root in 0.96s[0m
[0m2021.03.02 17:58:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:29 INFO  time: compiled root in 1.04s[0m
[0m2021.03.02 17:58:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:32 INFO  time: compiled root in 1.01s[0m
[0m2021.03.02 17:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:33 INFO  time: compiled root in 0.98s[0m
[0m2021.03.02 17:58:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:38 INFO  time: compiled root in 0.95s[0m
[0m2021.03.02 17:58:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:39 INFO  time: compiled root in 0.95s[0m
[0m2021.03.02 17:58:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:58:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:58:55 INFO  time: compiled root in 0.92s[0m
[0m2021.03.02 17:58:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:00 INFO  time: compiled root in 1.08s[0m
[0m2021.03.02 17:59:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:01 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 17:59:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:04 INFO  time: compiled root in 1.04s[0m
[0m2021.03.02 17:59:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:06 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 17:59:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:08 INFO  time: compiled root in 0.95s[0m
[0m2021.03.02 17:59:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:21 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 17:59:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:23 INFO  time: compiled root in 0.98s[0m
[0m2021.03.02 17:59:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:30 INFO  time: compiled root in 1.09s[0m
[0m2021.03.02 17:59:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:36 INFO  time: compiled root in 1.02s[0m
[0m2021.03.02 17:59:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 17:59:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 17:59:39 INFO  time: compiled root in 1.17s[0m
[0m2021.03.02 18:33:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:33:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:33:36 INFO  time: compiled root in 0.99s[0m
[0m2021.03.02 18:33:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:33:46 INFO  time: compiled root in 0.12s[0m
[0m2021.03.02 18:33:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:33:49 INFO  time: compiled root in 0.1s[0m
[0m2021.03.02 18:33:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:33:51 INFO  time: compiled root in 0.13s[0m
[0m2021.03.02 18:34:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:34:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:34:35 INFO  time: compiled root in 0.98s[0m
[0m2021.03.02 18:34:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:34:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:34:38 INFO  time: compiled root in 1.17s[0m
[0m2021.03.02 18:35:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:35:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:35:32 INFO  time: compiled root in 1.01s[0m
[0m2021.03.02 18:42:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:42:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:42:46 INFO  time: compiled root in 1.12s[0m
[0m2021.03.02 18:42:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:42:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:42:50 INFO  time: compiled root in 1.22s[0m
[0m2021.03.02 18:42:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:42:51 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 18:42:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:42:53 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:42:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:42:55 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 18:43:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:43:00 INFO  time: compiled root in 0.28s[0m
[0m2021.03.02 18:43:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:43:03 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:43:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:43:06 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:43:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:43:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:43:13 INFO  time: compiled root in 1.03s[0m
[0m2021.03.02 18:48:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:17 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:48:18 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:15: error: ; expected but = found
spark.hadoop.parquet.enable.summary-metadata=false
                                            ^[0m
[0m2021.03.02 18:48:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:48:22 INFO  time: compiled root in 1.15s[0m
[0m2021.03.02 18:48:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:48:24 INFO  time: compiled root in 1.17s[0m
[0m2021.03.02 18:48:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:27 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:48:27 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:35: error: ; expected but = found
spark.hadoop.parquet.enable.summary-metadata=false
                                            ^[0m
[0m2021.03.02 18:48:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:32 INFO  time: compiled root in 0.45s[0m
[0m2021.03.02 18:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:38 INFO  time: compiled root in 0.24s[0m
[0m2021.03.02 18:48:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:39 INFO  time: compiled root in 0.3s[0m
[0m2021.03.02 18:48:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:44 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 18:48:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:48:48 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 18:49:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:13 INFO  time: compiled root in 0.22s[0m
[0m2021.03.02 18:49:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:15 INFO  time: compiled root in 0.32s[0m
[0m2021.03.02 18:49:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:33 INFO  time: compiled root in 0.2s[0m
[0m2021.03.02 18:49:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:44 INFO  time: compiled root in 0.21s[0m
[0m2021.03.02 18:49:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:47 INFO  time: compiled root in 0.23s[0m
[0m2021.03.02 18:49:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:49:54 INFO  time: compiled root in 0.25s[0m
[0m2021.03.02 18:50:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:50:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:50:01 INFO  time: compiled root in 1.07s[0m
[0m2021.03.02 18:50:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 18:50:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 18:50:02 INFO  time: compiled root in 1.25s[0m
[0m2021.03.02 20:13:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 20:13:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 20:13:39 INFO  time: compiled root in 2.79s[0m
[0m2021.03.02 20:16:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 20:16:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 20:16:29 INFO  time: compiled root in 1.87s[0m
[0m2021.03.02 20:16:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 20:16:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 20:16:31 INFO  time: compiled root in 1.42s[0m
[0m2021.03.02 20:16:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 20:16:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 20:16:42 INFO  time: compiled root in 1.23s[0m
[0m2021.03.02 20:17:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.02 20:17:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.02 20:17:06 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 02:34:01 INFO  shutting down Metals[0m
[0m2021.03.03 02:34:01 INFO  Shut down connection with build server.[0m
[0m2021.03.03 02:34:01 INFO  Shut down connection with build server.[0m
[0m2021.03.03 02:34:01 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.03 15:27:02 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.03 15:27:02 INFO  time: initialize in 0.37s[0m
[0m2021.03.03 15:27:02 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.03 15:27:02 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3446644082627206501/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.03 15:27:02 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.03 15:27:05 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.BooleanType
import org.apache.spark.sql.types.StringType

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)
    spark.sparkContext.hadoopConfiguration
      .set("fs.s3a.endpoint", "s3.amazonaws.com")

    //rddParser(spark)
    //dfParser(spark)
    urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "multiline" -> "true",
          "encoding" -> "UTF-16LE",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
          "inferSchema" -> "true"
        )
      )
      .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    df.show()

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val jobSiteIndex = df
    //   .select("url")
    //   .filter(
    //     ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and ($"url_path"
    //       .contains("/jobs|job-listing"))
    //       .as("Job Listing URLs")
    //   )

    // jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("JobSiteIndex.json")

    val df = spark.read
      .format("json")
      .options(
        Map(
          "header" -> "false",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
        )
      )
      .load()

    df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true"
        )
      )
      .load()

    val exampleFormat = df
      .filter(($"url_path" contains "job") and ($"content_languages" === "eng"))
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3446644082627206501/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3446644082627206501/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 15:27:07 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:27:07 INFO  time: code lens generation in 4.5s[0m
[0m2021.03.03 15:27:07 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1886253671060546524/bsp.socket'...
[0m2021.03.03 15:27:07 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6756982877911031133/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1886253671060546524/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1886253671060546524/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6756982877911031133/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6756982877911031133/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 15:27:08 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:27:08 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:27:08 INFO  time: Connected to build server in 5.16s[0m
[0m2021.03.03 15:27:08 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.03 15:27:08 INFO  time: Imported build in 0.29s[0m
[0m2021.03.03 15:27:10 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.03 15:27:10 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.03 15:27:10 INFO  time: indexed workspace in 2.81s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 03, 2021 3:27:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.03 15:27:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:27:20 INFO  time: compiled root in 3.25s[0m
[0m2021.03.03 15:27:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:27:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:27:23 INFO  time: compiled root in 3.45s[0m
[0m2021.03.03 15:27:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:27:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:27:33 INFO  time: compiled root in 2.05s[0m
[0m2021.03.03 15:27:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:27:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:27:36 INFO  time: compiled root in 1.58s[0m
[0m2021.03.03 15:28:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:28:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:28:45 INFO  time: compiled root in 1.89s[0m
[0m2021.03.03 15:28:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:28:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:28:47 INFO  time: compiled root in 2.04s[0m
[0m2021.03.03 15:29:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:04 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 15:29:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:06 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:29:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:08 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 15:29:09 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219: error: ) expected but string constant found
          "compression" -> "gzip",
          ^[0m
[0m2021.03.03 15:29:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:17 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 15:29:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:21 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 15:29:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:29:24 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 15:29:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:29:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:29:47 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 15:32:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:32:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:32:23 INFO  time: compiled root in 1.42s[0m
[0m2021.03.03 15:32:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:32:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:32:38 INFO  time: compiled root in 1.55s[0m
[0m2021.03.03 15:33:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:33:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:33:50 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 15:33:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:33:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:33:58 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 15:34:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:34:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:34:32 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 15:34:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:34:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:34:58 INFO  time: compiled root in 1.85s[0m
[0m2021.03.03 15:34:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:35:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:35:01 INFO  time: compiled root in 2.38s[0m
[0m2021.03.03 15:35:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:35:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:35:46 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 15:38:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:38:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:38:05 INFO  time: compiled root in 1.11s[0m
[0m2021.03.03 15:39:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:39:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:39:24 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 15:39:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:39:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:39:27 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 15:40:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:40:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:40:50 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 15:40:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:40:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:40:59 INFO  time: compiled root in 1.6s[0m
[0m2021.03.03 15:40:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:00 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing|)))
                    ^[0m
[0m2021.03.03 15:41:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing))
                    ^[0m
[0m2021.03.03 15:41:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing)))
                    ^[0m
[0m2021.03.03 15:41:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing)))
                    ^[0m
[0m2021.03.03 15:41:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:05 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:41:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing)))
                    ^[0m
[0m2021.03.03 15:41:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing)))
                    ^[0m
[0m2021.03.03 15:41:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|job-listing)))
                    ^[0m
[0m2021.03.03 15:41:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|/job-listing)))
                    ^[0m
[0m2021.03.03 15:41:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|/job-listing)))
                    ^[0m
[0m2021.03.03 15:41:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:08 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 15:41:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|/job-listing)))
                    ^[0m
[0m2021.03.03 15:41:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|/job-listing)))
                    ^[0m
[0m2021.03.03 15:41:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:174:21: stale bloop error: unclosed string literal
          .contains("(/jobs|/job-listing)))
                    ^[0m
[0m2021.03.03 15:41:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:41:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:11 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 15:41:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:41:35 INFO  time: compiled root in 1.65s[0m
[0m2021.03.03 15:41:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:41:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:41:40 INFO  time: compiled root in 1.62s[0m
[0m2021.03.03 15:43:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:43:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:43:22 INFO  time: compiled root in 1.66s[0m
[0m2021.03.03 15:43:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:43:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:43:24 INFO  time: compiled root in 1.35s[0m
[0m2021.03.03 15:43:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:43:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:43:27 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 15:44:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:38 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 15:44:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:40 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 15:44:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:42 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 15:44:42 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:165: error: ) expected but string constant found
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet",
          ^[0m
[0m2021.03.03 15:44:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:50 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 15:44:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:51 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:44:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:54 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 15:44:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:44:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:44:59 INFO  time: compiled root in 2.21s[0m
[0m2021.03.03 15:48:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:48:24 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 15:48:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:48:29 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 15:48:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:33 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 15:48:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:84: stale bloop error: missing argument list for method or in class Column
Unapplied methods are only converted to functions when a function type is expected.
You can make this conversion explicit by writing `or _` or `or(_)` instead of `or`.
> $"url_path"
>           .contains("/jobs") or[0m
[0m2021.03.03 15:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:38 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:48:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:29: stale bloop error: unclosed string literal
          .contains("/jobs" ")
                            ^[0m
[0m2021.03.03 15:48:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:41 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:48:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:48:44 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:49:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:49:21 INFO  time: compiled root in 0.44s[0m
[0m2021.03.03 15:50:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:50:03 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:50:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:50:07 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 15:50:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:50:10 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:50:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:50:27 INFO  time: compiled root in 0.62s[0m
[0m2021.03.03 15:50:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:50:51 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:51:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:01 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:51:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:11 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:51:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:25 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:51:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:29 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:51:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:32 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:51:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:35 INFO  time: compiled root in 0.25s[0m
[0m2021.03.03 15:51:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:51:57 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:52:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:02 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:52:06 INFO  time: compiled root in 1.3s[0m
[0m2021.03.03 15:52:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:50 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:52:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:53 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 15:52:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:52:58 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 15:53:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:03 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:53:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:06 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 15:53:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:09 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:53:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:11 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:53:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:13 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 15:53:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:16 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:53:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:18 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 15:53:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:28 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:53:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:31 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 15:53:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:46 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:53:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:53:52 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:54:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:01 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:54:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:54:05 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 15:54:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:10 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 15:54:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:54:13 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 15:54:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:41 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 15:54:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:45 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 15:54:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:47 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:54:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:54:59 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:55:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:55:08 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 15:55:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:55:12 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 15:55:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:55:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:55:15 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 15:55:52 INFO  shutting down Metals[0m
[0m2021.03.03 15:55:52 INFO  Shut down connection with build server.[0m
[0m2021.03.03 15:55:52 INFO  Shut down connection with build server.[0m
[0m2021.03.03 15:55:52 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.03 15:56:07 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.03 15:56:07 INFO  time: initialize in 0.43s[0m
[0m2021.03.03 15:56:07 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8365826410814333313/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.03 15:56:07 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.03 15:56:08 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.03 15:56:10 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.StructType
import org.apache.spark.sql.types.StructField
import org.apache.spark.sql.types.BooleanType
import org.apache.spark.sql.types.StringType

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    //dfParser(spark)
    urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wat/CC-MAIN-20210128134124-20210128164124-00799.warc.wat.gz",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed"
    //     )
    //   )
    //   .load()

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "multiline" -> "true",
          "encoding" -> "UTF-16LE",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
          "inferSchema" -> "true"
        )
      )
      .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    df.show()

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .contains("/jobs")) or $"url_path".contains("/job-listing"))
      )
      .select("url")
      .as("Job Listing URIs")

    jobSiteIndex.show(false)

    // jobSiteIndex.write
    //     .format("json")
    //     .mode(SaveMode.Append)
    //     .save("TechJobsiteIndex.json")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8365826410814333313/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8365826410814333313/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 15:56:13 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:56:13 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.03 15:56:13Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6468694138917258581/bsp.socket'... INFO 
 Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4039079038422717731/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6468694138917258581/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6468694138917258581/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4039079038422717731/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4039079038422717731/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.03 15:56:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:56:14 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.03 15:56:14 INFO  time: Connected to build server in 6.25s[0m
[0m2021.03.03 15:56:14 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.03 15:56:14 INFO  time: code lens generation in 6.33s[0m
[0m2021.03.03 15:56:14 INFO  time: Imported build in 0.36s[0m
[0m2021.03.03 15:56:18 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.03 15:56:18 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.03 15:56:19 INFO  time: indexed workspace in 4.5s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 03, 2021 3:56:21 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.03 15:58:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:58:15 INFO  time: compiled root in 2.1s[0m
[0m2021.03.03 15:58:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:58:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:58:25 INFO  time: compiled root in 4.92s[0m
[0m2021.03.03 15:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:58:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:58:29 INFO  time: compiled root in 2.51s[0m
[0m2021.03.03 15:59:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:04 INFO  time: compiled root in 1.51s[0m
[0m2021.03.03 15:59:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:11 INFO  time: compiled root in 1.64s[0m
[0m2021.03.03 15:59:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:13 INFO  time: compiled root in 0.52s[0m
[0m2021.03.03 15:59:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: overloaded method value - with alternatives:
  (x: Double)Double <and>
  (x: Float)Float <and>
  (x: Long)Long <and>
  (x: Int)Int <and>
  (x: Char)Int <and>
  (x: Short)Int <and>
  (x: Byte)Int
 cannot be applied to (Boolean)
    jobSiteIndex.show(1-false)
                      ^^[0m
[0m2021.03.03 15:59:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: overloaded method value - with alternatives:
  (x: Double)Double <and>
  (x: Float)Float <and>
  (x: Long)Long <and>
  (x: Int)Int <and>
  (x: Char)Int <and>
  (x: Short)Int <and>
  (x: Byte)Int
 cannot be applied to (Boolean)
    jobSiteIndex.show(1-false)
                      ^^[0m
[0m2021.03.03 15:59:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: overloaded method value - with alternatives:
  (x: Double)Double <and>
  (x: Float)Float <and>
  (x: Long)Long <and>
  (x: Int)Int <and>
  (x: Char)Int <and>
  (x: Short)Int <and>
  (x: Byte)Int
 cannot be applied to (Boolean)
    jobSiteIndex.show(1-false)
                      ^^[0m
[0m2021.03.03 15:59:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: overloaded method value - with alternatives:
  (x: Double)Double <and>
  (x: Float)Float <and>
  (x: Long)Long <and>
  (x: Int)Int <and>
  (x: Char)Int <and>
  (x: Short)Int <and>
  (x: Byte)Int
 cannot be applied to (Boolean)
    jobSiteIndex.show(1-false)
                      ^^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: overloaded method value - with alternatives:
  (x: Double)Double <and>
  (x: Float)Float <and>
  (x: Long)Long <and>
  (x: Int)Int <and>
  (x: Char)Int <and>
  (x: Short)Int <and>
  (x: Byte)Int
 cannot be applied to (Boolean)
    jobSiteIndex.show(1-false)
                      ^^[0m
[0m2021.03.03 15:59:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: Invalid literal number
    jobSiteIndex.show(100false)
                      ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: Invalid literal number
    jobSiteIndex.show(100false)
                      ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:208:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:59:16 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: Invalid literal number
    jobSiteIndex.show(100false)
                      ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:208:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: Invalid literal number
    jobSiteIndex.show(100false)
                      ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:208:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:180:23: stale bloop error: Invalid literal number
    jobSiteIndex.show(100false)
                      ^[0m
[0m2021.03.03 15:59:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:208:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.03 15:59:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:20 INFO  time: compiled root in 1.68s[0m
[0m2021.03.03 15:59:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:33 INFO  time: compiled root in 1.24s[0m
[0m2021.03.03 15:59:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:46 INFO  time: compiled root in 0.39s[0m
Mar 03, 2021 3:59:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 157
[0m2021.03.03 15:59:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:50 INFO  time: compiled root in 1.24s[0m
[0m2021.03.03 15:59:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:54 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 15:59:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 15:59:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 15:59:57 INFO  time: compiled root in 1.26s[0m
[0m2021.03.03 15:59:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:00:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:00:00 INFO  time: compiled root in 1.34s[0m
[0m2021.03.03 16:05:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:05:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:05:06 INFO  time: compiled root in 1.75s[0m
[0m2021.03.03 16:05:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:05:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:05:53 INFO  time: compiled root in 1.34s[0m
[0m2021.03.03 16:05:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:05:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:05:59 INFO  time: compiled root in 1.86s[0m
[0m2021.03.03 16:06:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:06:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:06:12 INFO  time: compiled root in 1.69s[0m
[0m2021.03.03 16:06:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:06:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:06:55 INFO  time: compiled root in 1.56s[0m
[0m2021.03.03 16:06:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:07:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:07:00 INFO  time: compiled root in 1.27s[0m
[0m2021.03.03 16:07:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:07:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:07:04 INFO  time: compiled root in 1.65s[0m
[0m2021.03.03 16:07:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:07:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:07:55 INFO  time: compiled root in 2.02s[0m
[0m2021.03.03 16:09:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:09:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:09:41 INFO  time: compiled root in 1.59s[0m
[0m2021.03.03 16:09:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:09:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:09:45 INFO  time: compiled root in 1.67s[0m
[0m2021.03.03 16:09:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:09:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:09:48 INFO  time: compiled root in 1.61s[0m
[0m2021.03.03 16:09:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:09:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:09:51 INFO  time: compiled root in 1.94s[0m
[0m2021.03.03 16:12:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:12:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed multi-line string literal
    val jobsRegex = """(/jobs)|(/job-listing)".r
                    ^[0m
[0m2021.03.03 16:12:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed multi-line string literal
    val jobsRegex = """(/jobs)|(/job-listing)".r
                    ^[0m
[0m2021.03.03 16:12:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:249:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.03 16:12:54 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed multi-line string literal
    val jobsRegex = """(/jobs)|(/job-listing)".r
                    ^[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:249:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed multi-line string literal
    val jobsRegex = """(/jobs)|(/job-listing)".r
                    ^[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:249:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed multi-line string literal
    val jobsRegex = """(/jobs)|(/job-listing)".r
                    ^[0m
[0m2021.03.03 16:12:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:249:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.03 16:12:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:12:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:12:59 INFO  time: compiled root in 1.37s[0m
[0m2021.03.03 16:15:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:44 INFO  time: compiled root in 1.18s[0m
[0m2021.03.03 16:15:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:48 INFO  time: compiled root in 1.89s[0m
[0m2021.03.03 16:15:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:51 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 16:15:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:54 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 16:15:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:56 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 16:15:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:15:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:15:59 INFO  time: compiled root in 1.17s[0m
[0m2021.03.03 16:16:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:16:56 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 16:16:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: ';' expected but string literal found.
    val jobsRegex = "[/jobs]|[/job-listing]""".r
                                            ^[0m
[0m2021.03.03 16:16:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: ';' expected but string literal found.
    val jobsRegex = "[/jobs]|[/job-listing]""".r
                                            ^[0m
[0m2021.03.03 16:16:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: ';' expected but string literal found.
    val jobsRegex = "[/jobs]|[/job-listing]""".r
                                            ^[0m
[0m2021.03.03 16:16:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:16:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing].r
                    ^[0m
[0m2021.03.03 16:16:57 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 16:16:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing].r
                    ^[0m
[0m2021.03.03 16:16:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing].r
                    ^[0m
[0m2021.03.03 16:17:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing].r
                    ^[0m
[0m2021.03.03 16:17:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:17:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing]"".r
                                            ^[0m
[0m2021.03.03 16:17:00 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 16:17:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing]"".r
                                            ^[0m
[0m2021.03.03 16:17:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:45: stale bloop error: unclosed string literal
    val jobsRegex = "[/jobs]|[/job-listing]"".r
                                            ^[0m
[0m2021.03.03 16:17:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:17:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:17:03 INFO  time: compiled root in 1.28s[0m
[0m2021.03.03 16:19:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:26 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:19:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:173:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:19:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:35 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:37 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.03 16:19:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "job\"
                    ^[0m
[0m2021.03.03 16:19:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:19:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "job\"
                    ^[0m
[0m2021.03.03 16:19:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:171:21: stale bloop error: unclosed string literal
    val jobsRegex = "job\"
                    ^[0m
[0m2021.03.03 16:19:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:19:42 INFO  time: compiled root in 1.16s[0m
[0m2021.03.03 16:19:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:19:45 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 16:19:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:19:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:19:49 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 16:22:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:22:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:22:57 INFO  time: compiled root in 1.19s[0m
[0m2021.03.03 16:24:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:24:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:24:13 INFO  time: compiled root in 1.08s[0m
[0m2021.03.03 16:24:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:24:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:24:16 INFO  time: compiled root in 1.08s[0m
[0m2021.03.03 16:24:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:24:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:24:35 INFO  time: compiled root in 1.86s[0m
[0m2021.03.03 16:25:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:25:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:25:12 INFO  time: compiled root in 1.82s[0m
[0m2021.03.03 16:28:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:28:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:28:26 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 16:32:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:32:19 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 16:32:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:32:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:32:21 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 16:35:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:00 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 16:35:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:02 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 16:35:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:06 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 16:35:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:08 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 16:35:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:13 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 16:35:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:16 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 16:35:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:35:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:28 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:35:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:35:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:35:36 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 16:44:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:44:59 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:45:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:45:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:172:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 16:45:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:45:02 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 16:45:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:05 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 16:45:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:61: stale bloop error: ';' expected but string literal found.
    val jobsRegex = """val pattern = """(jobs)|(job-listing)""".r"""
                                                            ^[0m
[0m2021.03.03 16:45:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:170:61: stale bloop error: ';' expected but string literal found.
    val jobsRegex = """val pattern = """(jobs)|(job-listing)""".r"""
                                                            ^[0m
[0m2021.03.03 16:45:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:45:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:15 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 16:45:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:18 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 16:45:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:21 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 16:45:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:45:39 INFO  time: compiled root in 1.69s[0m
[0m2021.03.03 16:45:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:50 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:45:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:53 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 16:45:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:55 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 16:45:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:45:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:45:58 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 16:46:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:46:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:46:07 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 16:52:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:52:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:52:48 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 16:55:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:55:48 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 16:55:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:55:53 INFO  time: compiled root in 0.3s[0m
[0m2021.03.03 16:55:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:55:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:55:58 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 16:56:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:56:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:56:00 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 16:56:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:56:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:56:05 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 16:59:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:59:15 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 16:59:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:59:18 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 16:59:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:34 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 16:59:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 16:59:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 16:59:37 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 17:01:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:01:56 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 17:01:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:01:58 INFO  time: compiled root in 0.36s[0m
[0m2021.03.03 17:02:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:02:01 INFO  time: compiled root in 0.31s[0m
[0m2021.03.03 17:02:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:02:02 INFO  time: compiled root in 0.26s[0m
[0m2021.03.03 17:02:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:02:05 INFO  time: compiled root in 0.28s[0m
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1294
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1300
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1299
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1296
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1306
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1305
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1307
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1309
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1311
Mar 03, 2021 5:02:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1308
[0m2021.03.03 17:02:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:02:10 INFO  time: compiled root in 0.32s[0m
[0m2021.03.03 17:03:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:03:33 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 17:03:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:33 INFO  time: compiled root in 0.3s[0m
[0m2021.03.03 17:03:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:35 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 17:03:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:40 INFO  time: compiled root in 0.25s[0m
[0m2021.03.03 17:03:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:03:44 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 17:03:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:03:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:03:45 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:07:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:02 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:07:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:09 INFO  time: compiled root in 1.17s[0m
[0m2021.03.03 17:07:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:11 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 17:07:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:12 INFO  time: compiled root in 1s[0m
[0m2021.03.03 17:07:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:15 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:07:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:19 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:07:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:23 INFO  time: compiled root in 1.09s[0m
[0m2021.03.03 17:07:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:24 INFO  time: compiled root in 1s[0m
[0m2021.03.03 17:07:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:27 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:07:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:36 INFO  time: compiled root in 1.17s[0m
[0m2021.03.03 17:07:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:38 INFO  time: compiled root in 0.26s[0m
[0m2021.03.03 17:07:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:07:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:07:43 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:08:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:04 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:08:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:10 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:08:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:16 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:08:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:17 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 17:08:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:19 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:08:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:22 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 17:08:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:27 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:08:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:33 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:08:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:36 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:08:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:41 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 17:08:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:44 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:08:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:08:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:08:50 INFO  time: compiled root in 1.72s[0m
[0m2021.03.03 17:12:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:12:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:12:54 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 17:12:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:12:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:12:58 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 17:13:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:06 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:13:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:16 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 17:13:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:21 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:13:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:26 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 17:13:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:31 INFO  time: compiled root in 0.91s[0m
[0m2021.03.03 17:13:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:34 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:34 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 17:13:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:37 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:13:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:13:40 INFO  time: compiled root in 0.93s[0m
[0m2021.03.03 17:13:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:13:42 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:14:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:14:10 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:14:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:12 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:14:12 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 17:14:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:14:13 INFO  time: compiled root in 0.92s[0m
[0m2021.03.03 17:14:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:14:19 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:14:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:14:23 INFO  time: compiled root in 0.91s[0m
[0m2021.03.03 17:17:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:01 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 17:17:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:03 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:17:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:13 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:17:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:15 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 17:17:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:17 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:17:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:18 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:17:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:20 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 17:17:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:21 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:17:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:23 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 17:17:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:27 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:17:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:29 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:17:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:38 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:17:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:45 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:17:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:17:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:17:52 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:18:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:18:05 INFO  time: compiled root in 1.62s[0m
[0m2021.03.03 17:18:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:09 INFO  time: compiled root in 0.26s[0m
[0m2021.03.03 17:18:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:10 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 17:18:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:18:14 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:18:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:25 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 17:18:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:28 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 17:18:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:18:36 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 17:18:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:18:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:18:57 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:19:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:19:02 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 17:20:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:20:47 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:21:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:21:24 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 17:21:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:21:32 INFO  time: compiled root in 1.22s[0m
Mar 03, 2021 5:21:32 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.03 17:21:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:21:36 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 17:21:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:21:40 INFO  time: compiled root in 0.92s[0m
[0m2021.03.03 17:21:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:21:46 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:21:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:21:47 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:22:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:00 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 17:22:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:02 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:22:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:07 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 17:22:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:11 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 17:22:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:13 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:22:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:16 INFO  time: compiled root in 1s[0m
[0m2021.03.03 17:22:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:22:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:22:19 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:24:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:24:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:24:28 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:24:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:24:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:24:43 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:29:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:01 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:29:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:07 INFO  time: compiled root in 1.07s[0m
[0m2021.03.03 17:29:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:09 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 17:29:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:11 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 17:29:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:17 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:29:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:27 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 17:29:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:30 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:30 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:29:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:36 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:29:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:38 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 17:29:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:45 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 17:29:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:29:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:29:52 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 17:30:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:30:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:30:31 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 17:32:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:32:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:32:39 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 17:34:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:34:46 INFO  time: compiled root in 0.28s[0m
[0m2021.03.03 17:34:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:34:49 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 17:42:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:42:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:42:29 INFO  time: compiled root in 1.08s[0m
[0m2021.03.03 17:42:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:42:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:42:41 INFO  time: compiled root in 1.17s[0m
[0m2021.03.03 17:42:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:42:46 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:42:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:42:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:42:50 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 17:43:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:43:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:43:54 INFO  time: compiled root in 1.91s[0m
[0m2021.03.03 17:43:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:43:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:43:56 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 17:44:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:44:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:44:00 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:44:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:44:07 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:44:08 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:44:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:44:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:44:11 INFO  time: compiled root in 0.92s[0m
[0m2021.03.03 17:44:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:44:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:44:28 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 17:44:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:44:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:44:33 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 17:47:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:47:41 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:47:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:47:51 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 17:47:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:47:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:47:53 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 17:48:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:48:11 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 17:48:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:48:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:48:14 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 17:48:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:48:33 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:48:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:48:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:48:37 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 17:50:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:50:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:50:11 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 17:51:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:51:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:51:49 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:52:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:52:09 INFO  time: compiled root in 0.24s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.03 17:52:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:52:24 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 17:52:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:52:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:52:27 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 17:52:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:52:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 17:52:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:52:33 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 17:55:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:55:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:55:31 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 17:58:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 17:58:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 17:58:36 INFO  time: compiled root in 1.1s[0m
[0m2021.03.03 18:01:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:01:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:01:41 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 18:01:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:01:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:01:44 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 18:01:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:01:54 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:01:54 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 18:01:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:01:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:01:57 INFO  time: compiled root in 1.06s[0m
[0m2021.03.03 18:07:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:07:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:07:23 INFO  time: compiled root in 1.19s[0m
[0m2021.03.03 18:15:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:14 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:15:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 18:15:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 18:15:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:205:5: stale bloop error: illegal start of simple expression
    val jobSiteIndex = df
    ^[0m
[0m2021.03.03 18:15:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:30 INFO  time: compiled root in 0.25s[0m
[0m2021.03.03 18:15:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:37 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 18:15:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:39 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 18:15:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:40 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 18:15:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:45 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:15:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:49 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 18:15:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:51 INFO  time: compiled root in 0.25s[0m
[0m2021.03.03 18:15:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:53 INFO  time: compiled root in 0.19s[0m
[0m2021.03.03 18:15:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:55 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:15:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:15:56 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 18:16:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:01 INFO  time: compiled root in 0.18s[0m
[0m2021.03.03 18:16:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:03 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:16:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:08 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 18:16:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:12 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 18:16:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:27 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 18:16:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:30 INFO  time: compiled root in 0.16s[0m
[0m2021.03.03 18:16:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:32 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 18:16:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:36 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 18:16:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:38 INFO  time: compiled root in 0.15s[0m
[0m2021.03.03 18:16:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 18:16:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:41 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 18:16:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:44 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:16:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:47 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 18:16:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:16:50 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 18:17:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:17:09 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 18:17:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:17:21 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 18:17:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:17:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:17:28 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 18:17:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:17:29 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 18:17:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:17:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:17:33 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 18:18:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:18:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:18:51 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 18:18:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:18:53 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:18:53 INFO  time: compiled root in 0.96s[0m
[0m2021.03.03 18:18:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:18:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:18:58 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 18:19:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:19:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:19:02 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 18:19:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:19:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:19:04 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 18:19:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:19:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:19:08 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 18:58:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:58:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:58:22 INFO  time: compiled root in 3.06s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.03 18:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:58:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:58:28 INFO  time: compiled root in 1.5s[0m
[0m2021.03.03 18:58:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:58:53 INFO  time: compiled root in 0.47s[0m
[0m2021.03.03 18:59:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:59:02 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 18:59:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:59:16 INFO  time: compiled root in 0.27s[0m
[0m2021.03.03 18:59:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 18:59:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 18:59:56 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 19:00:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:00:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:00:03 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 19:09:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:09:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:09:42 INFO  time: compiled root in 3.87s[0m
[0m2021.03.03 19:15:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:15:25 INFO  time: compiled root in 1.1s[0m
[0m2021.03.03 19:15:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:32 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 19:15:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:35 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 19:15:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:15:41 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 19:15:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:15:42 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 19:15:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:15:48 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 19:15:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:15:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:15:51 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 19:18:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:18:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:18:17 INFO  time: compiled root in 1.18s[0m
[0m2021.03.03 19:18:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:18:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:18:26 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 19:22:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:22:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:22:19 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 19:22:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:22:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:22:22 INFO  time: compiled root in 0.95s[0m
[0m2021.03.03 19:22:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:22:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:22:35 INFO  time: compiled root in 1.29s[0m
[0m2021.03.03 19:22:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:22:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:22:55 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 19:23:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:23:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:23:18 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 19:26:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:26:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:26:29 INFO  time: compiled root in 1.21s[0m
[0m2021.03.03 19:31:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:31:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:31:44 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 19:31:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:31:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:31:50 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 19:32:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:32:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:32:00 INFO  time: compiled root in 0.97s[0m
[0m2021.03.03 19:33:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:33:46 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 19:33:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:33:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:33:50 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 19:38:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:38:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:38:45 INFO  time: compiled root in 0.94s[0m
[0m2021.03.03 19:39:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:39:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:39:15 INFO  time: compiled root in 1.28s[0m
[0m2021.03.03 19:44:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:44:36 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:44:36 INFO  time: compiled root in 1.01s[0m
[0m2021.03.03 19:44:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:44:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:44:45 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 19:45:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:45:03 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 19:45:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:45:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:45:06 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 19:45:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:45:14 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 19:45:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:45:23 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 19:46:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:46:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:46:49 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 19:46:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:46:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:46:50 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 19:46:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:46:53 INFO  time: compiled root in 0.14s[0m
[0m2021.03.03 19:46:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:46:55 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 19:46:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:46:59 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 19:47:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:47:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:47:03 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 19:52:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:52:45 INFO  time: compiled root in 1.77s[0m
[0m2021.03.03 19:52:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:52:48 INFO  time: compiled root in 1.54s[0m
[0m2021.03.03 19:52:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:52 INFO  time: compiled root in 0.17s[0m
[0m2021.03.03 19:52:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:53 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 19:52:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:55 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 19:52:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:52:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:52:58 INFO  time: compiled root in 1.5s[0m
[0m2021.03.03 19:57:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:57:41 INFO  time: compiled root in 1.04s[0m
[0m2021.03.03 19:57:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:57:48 INFO  time: compiled root in 1.05s[0m
[0m2021.03.03 19:57:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 19:57:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:57:52 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 19:57:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:54 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 19:57:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:57:56 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 19:58:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:58:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:58:50 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 19:59:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 19:59:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 19:59:04 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 20:04:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:04:46 INFO  time: compiled root in 0.21s[0m
Mar 03, 2021 8:04:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4706
[0m2021.03.03 20:04:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:04:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 20:04:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:04:50 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 20:04:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:04:53 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 20:05:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:05:22 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 20:07:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:07:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:07:09 INFO  time: compiled root in 1.33s[0m
[0m2021.03.03 20:08:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:08:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:08:56 INFO  time: compiled root in 1.25s[0m
[0m2021.03.03 20:09:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:09:02 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 20:09:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:09:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:09:09 INFO  time: compiled root in 3.55s[0m
[0m2021.03.03 20:13:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:13:21 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:13:21 INFO  time: compiled root in 1.8s[0m
[0m2021.03.03 20:13:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:13:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:13:24 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 20:13:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:13:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:13:30 INFO  time: compiled root in 1s[0m
[0m2021.03.03 20:18:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:18:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:18:24 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 20:19:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:19:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:19:05 INFO  time: compiled root in 1.55s[0m
[0m2021.03.03 20:19:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:19:55 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 20:19:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:19:56 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 20:20:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:20:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:20:03 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 20:20:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:20:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:20:43 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 20:20:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:20:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:20:55 INFO  time: compiled root in 1.73s[0m
[0m2021.03.03 20:24:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:24:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:24:48 INFO  time: compiled root in 1.29s[0m
[0m2021.03.03 20:27:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:25 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 20:27:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:26 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 20:27:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:29 INFO  time: compiled root in 1.1s[0m
[0m2021.03.03 20:27:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:31 INFO  time: compiled root in 1.13s[0m
[0m2021.03.03 20:27:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:31 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 20:27:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:40 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:40 INFO  time: compiled root in 1.02s[0m
[0m2021.03.03 20:27:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:42 INFO  time: compiled root in 0.27s[0m
[0m2021.03.03 20:27:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:27:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:27:44 INFO  time: compiled root in 0.99s[0m
[0m2021.03.03 20:28:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:28:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:28:03 INFO  time: compiled root in 1.03s[0m
[0m2021.03.03 20:28:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:28:09 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 20:28:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:28:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:28:13 INFO  time: compiled root in 1.15s[0m
[0m2021.03.03 20:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:34:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:34:48 INFO  time: compiled root in 1.11s[0m
[0m2021.03.03 20:36:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:36:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:36:50 INFO  time: compiled root in 1.2s[0m
[0m2021.03.03 20:38:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:38:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:38:32 INFO  time: compiled root in 1.15s[0m
[0m2021.03.03 20:38:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:38:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:38:41 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 20:52:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:52:54 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 20:52:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 20:52:58 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 20:52:58 INFO  time: compiled root in 1.86s[0m
[0m2021.03.03 22:15:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 22:15:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 22:15:17 INFO  time: compiled root in 4.07s[0m
[0m2021.03.03 22:15:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 22:15:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 22:15:24 INFO  time: compiled root in 3.35s[0m
[0m2021.03.03 23:48:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:07 INFO  time: compiled root in 1.85s[0m
[0m2021.03.03 23:48:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:08 INFO  time: compiled root in 1.23s[0m
[0m2021.03.03 23:48:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:10 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 23:48:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:48:17 INFO  time: compiled root in 2.38s[0m
[0m2021.03.03 23:48:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:24 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 23:48:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:27 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 23:48:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:33 INFO  time: compiled root in 0.41s[0m
[0m2021.03.03 23:48:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:48:56 INFO  time: compiled root in 0.2s[0m
[0m2021.03.03 23:49:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:49:41 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 23:49:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:49:44 INFO  time: compiled root in 0.21s[0m
[0m2021.03.03 23:49:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:49:46 INFO  time: compiled root in 0.32s[0m
[0m2021.03.03 23:49:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:49:59 INFO  time: compiled root in 0.24s[0m
[0m2021.03.03 23:50:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:06 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 23:50:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:12 INFO  time: compiled root in 0.31s[0m
[0m2021.03.03 23:50:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:31 INFO  time: compiled root in 0.23s[0m
[0m2021.03.03 23:50:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:50 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 23:50:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:56 INFO  time: compiled root in 0.22s[0m
[0m2021.03.03 23:50:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:50:59 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 23:51:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:51:06 INFO  time: compiled root in 0.27s[0m
[0m2021.03.03 23:51:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:51:11 INFO  time: compiled root in 0.1s[0m
[0m2021.03.03 23:51:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:51:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:51:14 INFO  time: compiled root in 0.98s[0m
[0m2021.03.03 23:51:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:51:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:51:56 INFO  time: compiled root in 1.12s[0m
[0m2021.03.03 23:53:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:53:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:53:13 INFO  time: compiled root in 1.4s[0m
[0m2021.03.03 23:53:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:53:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:53:19 INFO  time: compiled root in 1.14s[0m
[0m2021.03.03 23:53:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:53:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:53:25 INFO  time: compiled root in 1.22s[0m
[0m2021.03.03 23:54:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:54:50 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 23:54:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:54:51 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 23:54:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:54:53 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 23:54:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:54:56 INFO  time: compiled root in 0.11s[0m
[0m2021.03.03 23:54:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:54:59 INFO  time: compiled root in 0.13s[0m
[0m2021.03.03 23:55:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:55:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.03 23:55:05 INFO  time: compiled root in 1.08s[0m
[0m2021.03.03 23:55:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:55:08 INFO  time: compiled root in 0.12s[0m
[0m2021.03.03 23:55:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.03 23:55:10 INFO  time: compiled root in 0.22s[0m
[0m2021.03.04 00:01:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 00:01:37 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 00:01:37 INFO  time: compiled root in 1.57s[0m
[0m2021.03.04 00:10:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 00:10:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 00:10:20 INFO  time: compiled root in 1.31s[0m
[0m2021.03.04 00:36:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 00:36:53 INFO  time: compiled root in 1.08s[0m
Mar 04, 2021 12:36:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5806
Mar 04, 2021 12:36:54 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5809
[0m2021.03.04 00:36:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 00:36:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 00:36:59 INFO  time: compiled root in 1.83s[0m
[0m2021.03.04 00:41:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 00:42:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 00:42:05 INFO  time: compiled root in 5.71s[0m
Mar 04, 2021 1:05:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5895
[0m2021.03.04 01:06:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:06:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:06:42 INFO  time: compiled root in 2.73s[0m
[0m2021.03.04 01:06:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:06:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:06:44 INFO  time: compiled root in 1.09s[0m
[0m2021.03.04 01:06:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:06:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:06:46 INFO  time: compiled root in 1.04s[0m
[0m2021.03.04 01:06:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:06:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:06:47 INFO  time: compiled root in 0.94s[0m
[0m2021.03.04 01:07:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:07:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:07:10 INFO  time: compiled root in 1.14s[0m
[0m2021.03.04 01:11:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:11:39 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:11:39 INFO  time: compiled root in 0.99s[0m
[0m2021.03.04 01:11:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:11:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:11:46 INFO  time: compiled root in 1.15s[0m
[0m2021.03.04 01:11:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:11:56 INFO  time: compiled root in 0.21s[0m
[0m2021.03.04 01:12:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:12:04 INFO  time: compiled root in 1.05s[0m
[0m2021.03.04 01:12:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:08 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 01:12:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:13 INFO  time: compiled root in 0.15s[0m
[0m2021.03.04 01:12:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:15 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:12:16 INFO  time: compiled root in 1s[0m
[0m2021.03.04 01:12:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 01:12:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:21 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 01:12:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:24 INFO  time: compiled root in 0.1s[0m
[0m2021.03.04 01:12:25 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79: error: ) expected but string constant found
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz",
          ^[0m
[0m2021.03.04 01:12:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:31 INFO  time: compiled root in 0.16s[0m
[0m2021.03.04 01:12:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:34 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 01:12:35 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:80: error: ) expected but string constant found
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz",
          ^[0m
[0m2021.03.04 01:12:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:41 INFO  time: compiled root in 0.1s[0m
[0m2021.03.04 01:12:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:12:45 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 01:13:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:13:08 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 01:14:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:14:03 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:14:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:13 INFO  time: compiled root in 0.11s[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:14:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:14:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:14:16 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 01:14:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:81:9: stale bloop error: illegal start of simple expression
        )
        ^[0m
[0m2021.03.04 01:14:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:156:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.04 01:14:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:14:21 INFO  time: compiled root in 0.13s[0m
[0m2021.03.04 01:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:14:24 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:14:24 INFO  time: compiled root in 1.13s[0m
[0m2021.03.04 01:24:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:24:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:24:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:24:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:24:20 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:79:24: stale bloop error: unclosed multi-line string literal
          "lineSep" -> """\r\n\r\n",
                       ^[0m
[0m2021.03.04 01:24:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:294:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.04 01:24:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:24:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:24:25 INFO  time: compiled root in 1.67s[0m
[0m2021.03.04 01:51:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 01:51:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 01:51:59 INFO  time: compiled root in 2.61s[0m
[0m2021.03.04 02:01:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 02:01:54 ERROR scalafmt: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:24: error: ; expected but . found
  .config("spark.executor.memory", "14g") \
  ^[0m
[0m2021.03.04 02:01:53 INFO  time: compiled root in 0.64s[0m
[0m2021.03.04 02:02:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 02:02:01 INFO  time: compiled root in 0.5s[0m
[0m2021.03.04 02:02:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 02:02:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 02:02:08 INFO  time: compiled root in 2.46s[0m
[0m2021.03.04 02:09:17 INFO  shutting down Metals[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.04 02:11:34 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.04 02:11:34 INFO  time: initialize in 0.48s[0m
[0m2021.03.04 02:11:35 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2788782690668595027/bsp.socket'...
[0m2021.03.04 02:11:35 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 02:11:34 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.04 02:11:38 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .config("spark.memory.fraction", 0.8)
      .config("spark.executor.memory", "14g")
      .config("spark.driver.memory", "12g")
      .config("spark.sql.shuffle.partitions" , "800")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed",
          "lineSep" -> """\r\n\r\n""",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
        )
      )
      .load()

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    df.show()

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting".toLowerCase

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    ).map(list => list.toLowerCase)

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select($"url", $"warc_filename", $"warc_record_offset", $"warc_record_length")

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2788782690668595027/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2788782690668595027/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 02:11:41 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 02:11:41 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6667842018214974739/bsp.socket'...
[0m2021.03.04 02:11:41 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3052666015385556260/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3052666015385556260/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3052666015385556260/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 02:11:42 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6667842018214974739/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6667842018214974739/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 02:11:42 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 02:11:42 INFO  time: Connected to build server in 6.77s[0m
[0m2021.03.04 02:11:42 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 02:11:42 INFO  time: code lens generation in 6.51s[0m
[0m2021.03.04 02:11:42 INFO  time: Imported build in 0.3s[0m
[0m2021.03.04 02:11:45 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.04 02:11:45 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 02:11:45 INFO  time: indexed workspace in 3.39s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 04, 2021 2:11:47 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.04 02:17:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 02:17:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 02:17:44 INFO  time: compiled root in 7.46s[0m
[0m2021.03.04 03:17:03 INFO  shutting down Metals[0m
[0m2021.03.04 03:17:03 INFO  Shut down connection with build server.[0m
[0m2021.03.04 03:17:03 INFO  Shut down connection with build server.[0m
[0m2021.03.04 03:17:03 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.04 12:07:07 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.04 12:07:07 INFO  time: initialize in 0.42s[0m
[0m2021.03.04 12:07:08 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.04 12:07:07 Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6109853743627067647/bsp.socket'...WARN  
no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 12:07:07 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.04 12:07:09 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .config("spark.memory.fraction", 0.8)
      .config("spark.sql.shuffle.partitions" , "800")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("json")
      .options(
        Map(
          "compression" -> "gzip",
          "inferSchema" -> "true",
          "mode" -> "dropMalformed",
          "lineSep" -> """\r\n\r\n""",
          "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
        )
      )
      .load()

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    df.show()

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting".toLowerCase

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    ).map(list => list.toLowerCase)

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select($"url", $"warc_filename", $"warc_record_offset", $"warc_record_length")

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6109853743627067647/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6109853743627067647/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 12:07:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 12:07:12 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6920117085067643556/bsp.socket'...
[0m2021.03.04 12:07:12 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher4334456312042248742/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6920117085067643556/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6920117085067643556/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher4334456312042248742/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher4334456312042248742/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.04 12:07:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 12:07:12 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.04 12:07:12 INFO  time: Connected to build server in 4.61s[0m
[0m2021.03.04 12:07:12 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.04 12:07:12 INFO  time: code lens generation in 2.83s[0m
[0m2021.03.04 12:07:13 INFO  time: Imported build in 0.26s[0m
[0m2021.03.04 12:07:15 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.04 12:07:15 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.04 12:07:15 INFO  time: indexed workspace in 3.39s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 04, 2021 12:07:17 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.04 12:07:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:07:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:07:26 INFO  time: compiled root in 6.16s[0m
[0m2021.03.04 12:07:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:07:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:07:55 INFO  time: compiled root in 2.72s[0m
[0m2021.03.04 12:09:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:09:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:09:59 INFO  time: compiled root in 1.83s[0m
[0m2021.03.04 12:25:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:25:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:25:06 INFO  time: compiled root in 1.96s[0m
[0m2021.03.04 12:28:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:09 INFO  time: compiled root in 0.58s[0m
[0m2021.03.04 12:28:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:18 INFO  time: compiled root in 0.57s[0m
[0m2021.03.04 12:28:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:20 INFO  time: compiled root in 0.47s[0m
[0m2021.03.04 12:28:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:21 INFO  time: compiled root in 0.72s[0m
[0m2021.03.04 12:28:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:39 INFO  time: compiled root in 0.55s[0m
[0m2021.03.04 12:28:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:28:40 INFO  time: compiled root in 0.8s[0m
[0m2021.03.04 12:29:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:29:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:29:31 INFO  time: compiled root in 1.98s[0m
[0m2021.03.04 12:31:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:31:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:31:52 INFO  time: compiled root in 1.8s[0m
[0m2021.03.04 12:32:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:32:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:32:10 INFO  time: compiled root in 2.15s[0m
[0m2021.03.04 12:34:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:34:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:34:09 INFO  time: compiled root in 1.45s[0m
[0m2021.03.04 12:34:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:34:11 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:34:11 INFO  time: compiled root in 1.61s[0m
[0m2021.03.04 12:34:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:34:14 INFO  time: compiled root in 0.36s[0m
[0m2021.03.04 12:34:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:34:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:34:23 INFO  time: compiled root in 1.38s[0m
[0m2021.03.04 12:34:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:34:29 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:34:29 INFO  time: compiled root in 1.59s[0m
[0m2021.03.04 12:40:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:40:02 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:40:02 INFO  time: compiled root in 1.26s[0m
[0m2021.03.04 12:44:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:44:45 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:44:45 INFO  time: compiled root in 1.28s[0m
[0m2021.03.04 12:44:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:44:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:44:51 INFO  time: compiled root in 1.32s[0m
[0m2021.03.04 12:45:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:45:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:45:08 INFO  time: compiled root in 1.35s[0m
[0m2021.03.04 12:45:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:45:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:45:18 INFO  time: compiled root in 1.5s[0m
[0m2021.03.04 12:49:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:49:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:49:14 INFO  time: compiled root in 1.04s[0m
[0m2021.03.04 12:49:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:49:17 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:49:17 INFO  time: compiled root in 1.28s[0m
[0m2021.03.04 12:49:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:49:18 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:49:18 INFO  time: compiled root in 1.25s[0m
[0m2021.03.04 12:49:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:49:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:49:22 INFO  time: compiled root in 1.34s[0m
[0m2021.03.04 12:49:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:49:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:49:28 INFO  time: compiled root in 1.05s[0m
[0m2021.03.04 12:50:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:32 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:50:32 INFO  time: compiled root in 1.86s[0m
[0m2021.03.04 12:50:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:42 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:50:42 INFO  time: compiled root in 1.16s[0m
[0m2021.03.04 12:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:50:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:50:48 INFO  time: compiled root in 1.04s[0m
[0m2021.03.04 12:50:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:51:00 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:51:00 INFO  time: compiled root in 1.36s[0m
[0m2021.03.04 12:55:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:55:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:55:23 INFO  time: compiled root in 0.98s[0m
[0m2021.03.04 12:55:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:55:48 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:55:48 INFO  time: compiled root in 0.96s[0m
[0m2021.03.04 12:56:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:16 INFO  time: compiled root in 0.2s[0m
[0m2021.03.04 12:56:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:18 INFO  time: compiled root in 0.29s[0m
[0m2021.03.04 12:56:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:22 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 12:56:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:26 INFO  time: compiled root in 0.34s[0m
[0m2021.03.04 12:56:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:27 INFO  time: compiled root in 0.28s[0m
[0m2021.03.04 12:56:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:56:33 INFO  time: compiled root in 1.16s[0m
[0m2021.03.04 12:56:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:36 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 12:56:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:42 INFO  time: compiled root in 0.27s[0m
[0m2021.03.04 12:56:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:56:50 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 12:57:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:01 INFO  time: compiled root in 0.33s[0m
[0m2021.03.04 12:57:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:31: stale bloop error: value mks is not a member of List[String]
      .filter($"value" rlike (techJobs.mks))
                              ^^^^^^^^^^^^[0m
[0m2021.03.04 12:57:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:31: stale bloop error: value mks is not a member of List[String]
      .filter($"value" rlike (techJobs.mks))
                              ^^^^^^^^^^^^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:31: stale bloop error: value mks is not a member of List[String]
      .filter($"value" rlike (techJobs.mks))
                              ^^^^^^^^^^^^[0m
[0m2021.03.04 12:57:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:49: stale bloop error: unclosed string literal
      .filter($"value" rlike (techJobs.mkString"))
                                                ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:49: stale bloop error: unclosed string literal
      .filter($"value" rlike (techJobs.mkString"))
                                                ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:40: stale bloop error: identifier expected but string literal found.
      .filter($"value" rlike (techJobs.mkString"))
                                       ^[0m
[0m2021.03.04 12:57:03 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:49: stale bloop error: unclosed string literal
      .filter($"value" rlike (techJobs.mkString"))
                                                ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:40: stale bloop error: identifier expected but string literal found.
      .filter($"value" rlike (techJobs.mkString"))
                                       ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:49: stale bloop error: unclosed string literal
      .filter($"value" rlike (techJobs.mkString"))
                                                ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:40: stale bloop error: identifier expected but string literal found.
      .filter($"value" rlike (techJobs.mkString"))
                                       ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:49: stale bloop error: unclosed string literal
      .filter($"value" rlike (techJobs.mkString"))
                                                ^[0m
[0m2021.03.04 12:57:03 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:40: stale bloop error: identifier expected but string literal found.
      .filter($"value" rlike (techJobs.mkString"))
                                       ^[0m
[0m2021.03.04 12:57:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:05 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 12:57:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:06 INFO  time: compiled root in 0.25s[0m
[0m2021.03.04 12:57:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:10 INFO  time: compiled root in 0.35s[0m
[0m2021.03.04 12:57:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:13 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:13 INFO  time: compiled root in 1.23s[0m
[0m2021.03.04 12:57:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:20 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:20 INFO  time: compiled root in 1.13s[0m
[0m2021.03.04 12:57:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:21 INFO  time: compiled root in 0.12s[0m
[0m2021.03.04 12:57:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:25 INFO  time: compiled root in 1.12s[0m
[0m2021.03.04 12:57:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:43 INFO  time: compiled root in 1.29s[0m
[0m2021.03.04 12:57:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:43 INFO  time: compiled root in 0.25s[0m
Mar 04, 2021 12:57:47 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.04 12:57:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:52 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:52 INFO  time: compiled root in 1.45s[0m
[0m2021.03.04 12:57:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 12:57:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 12:57:56 INFO  time: compiled root in 1.07s[0m
[0m2021.03.04 14:15:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 14:15:54 INFO  time: compiled root in 0.14s[0m
[0m2021.03.04 19:18:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 19:18:43 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 19:18:43 INFO  time: compiled root in 4.34s[0m
[0m2021.03.04 19:18:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 19:18:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 19:18:50 INFO  time: compiled root in 5.09s[0m
[0m2021.03.04 19:19:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.04 19:19:06 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.04 19:19:06 INFO  time: compiled root in 2.68s[0m
[0m2021.03.05 01:43:08 INFO  shutting down Metals[0m
[0m2021.03.05 01:43:08 INFO  Shut down connection with build server.[0m
[0m2021.03.05 01:43:08 INFO  Shut down connection with build server.[0m
[0m2021.03.05 01:43:08 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.05 10:46:04 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.05 10:46:04 INFO  time: initialize in 0.62s[0m
[0m2021.03.05 10:46:05 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5433706210103694189/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 10:46:04 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.05 10:46:05 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
[0m2021.03.05 10:46:07 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .text(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
      )
      .as[String]
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike (techJobs.mkString("|")))

    commonCrawlTechJobs.show(2, false)

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5433706210103694189/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5433706210103694189/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 10:46:10 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 10:46:11 INFO  time: code lens generation in 3.58s[0m
[0m2021.03.05 10:46:11 INFO  time: code lens generation in 3.66s[0m
[0m2021.03.05 10:46:11 INFO  time: code lens generation in 3.65s[0m
[0m2021.03.05 10:46:10 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 10:46:10 INFO  Attempting to connect to the build server...Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5225703734645747917/bsp.socket'...[0m

Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8075698971167735743/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5225703734645747917/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5225703734645747917/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8075698971167735743/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8075698971167735743/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 10:46:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 10:46:11 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 10:46:11 INFO  time: Connected to build server in 6.38s[0m
[0m2021.03.05 10:46:11 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 10:46:12 INFO  time: Imported build in 0.34s[0m
[0m2021.03.05 10:46:15 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.05 10:46:15 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 10:46:16 INFO  time: indexed workspace in 4.36s[0m
[0m2021.03.05 15:36:45 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.53.2.[0m
[0m2021.03.05 15:36:45 INFO  time: initialize in 0.42s[0m
[0m2021.03.05 15:36:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher123888789939019197/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 15:36:45 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.05 15:36:45 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher123888789939019197/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher123888789939019197/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 15:36:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 15:36:46 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 15:36:46Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher7471224916781174001/bsp.socket'... 
INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher2729543367275182485/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher7471224916781174001/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher7471224916781174001/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 15:36:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher2729543367275182485/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher2729543367275182485/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 15:36:46 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 15:36:46 INFO  time: Connected to build server in 0.66s[0m
[0m2021.03.05 15:36:46 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 15:36:48 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.05 15:36:48 INFO  time: Imported build in 0.16s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .text(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
      )
      .as[String]
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike (techJobs.mkString("|")))

    commonCrawlTechJobs.show(2, false)

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.05 15:36:51 INFO  no build target: using presentation compiler with only scala-library: 2.12.12[0m
[0m2021.03.05 15:36:53 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.05 15:36:53 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 15:36:54 INFO  time: indexed workspace in 5.83s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .text(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2019-51/segments/1575541319511.97/wet/CC-MAIN-20191216093448-20191216121448-00559.warc.wet.gz"
      )
      .as[String]
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike (techJobs.mkString("|")))

    commonCrawlTechJobs.show(2, false)

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 05, 2021 3:36:56 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.05 15:36:57 INFO  time: code lens generation in 11s[0m
[0m2021.03.05 15:37:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:37:35 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:37:35 INFO  time: compiled root in 5.45s[0m
[0m2021.03.05 15:37:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:37:43 INFO  time: compiled root in 0.36s[0m
[0m2021.03.05 15:37:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:37:44 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 15:37:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:37:49 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 15:41:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:25 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:41:25 INFO  time: compiled root in 2.83s[0m
[0m2021.03.05 15:41:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:27 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:41:27 INFO  time: compiled root in 1.78s[0m
[0m2021.03.05 15:41:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:41:33 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:41:33 INFO  time: compiled root in 1.56s[0m
[0m2021.03.05 15:41:39 INFO  shutting down Metals[0m
[0m2021.03.05 15:41:39 INFO  Shut down connection with build server.[0m
[0m2021.03.05 15:41:39 INFO  Shut down connection with build server.[0m
[0m2021.03.05 15:41:39 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 15:47:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:47:55 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:47:55 INFO  time: compiled root in 1.44s[0m
[0m2021.03.05 15:47:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:47:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:47:57 INFO  time: compiled root in 1.49s[0m
[0m2021.03.05 15:52:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:52:38 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:52:38 INFO  time: compiled root in 2.62s[0m
[0m2021.03.05 15:52:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:52:41 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:52:41 INFO  time: compiled root in 2.45s[0m
[0m2021.03.05 15:54:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:54:44 INFO  time: compiled root in 1.25s[0m
[0m2021.03.05 15:54:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:46 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 15:54:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 15:54:50 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 15:54:50 INFO  time: compiled root in 1.77s[0m
[0m2021.03.05 16:00:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:00:57 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:00:57 INFO  time: compiled root in 1.05s[0m
[0m2021.03.05 16:00:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:00:59 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:00:59 INFO  time: compiled root in 1.29s[0m
[0m2021.03.05 16:00:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:01:01 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:01:01 INFO  time: compiled root in 0.99s[0m
[0m2021.03.05 16:02:34 INFO  shutting down Metals[0m
[0m2021.03.05 16:02:34 INFO  Shut down connection with build server.[0m
[0m2021.03.05 16:02:34 INFO  Shut down connection with build server.[0m
[0m2021.03.05 16:02:34 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.05 16:03:11 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.54.1.[0m
[0m2021.03.05 16:03:12 INFO  time: initialize in 0.47s[0m
[0m2021.03.05 16:03:11 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
[0m2021.03.05 16:03:12 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher948363110451112207/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.05 16:03:12 INFO  skipping build import with status 'Installed'[0m
Waiting for the bsp connection to come up...
[0m2021.03.05 16:03:14 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
Waiting for the bsp connection to come up...
Waiting for the bsp connection to come up...
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .text(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
      )
      .as[String]
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))

    commonCrawlTechJobs.show(2, false)

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

Waiting for the bsp connection to come up...
No server running at 127.0.0.1:8212, let's fire one...
Resolving ch.epfl.scala:bloop-frontend_2.12:1.4.8...
Starting bloop server at 127.0.0.1:8212...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
Attempting a connection to the server...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Cache miss for scala instance org.scala-lang:scala-compiler:2.11.12.
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/jline/jline/2.14.3/jline-2.14.3.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-parser-combinators_2.11/1.0.4/scala-parser-combinators_2.11-1.0.4.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/modules/scala-xml_2.11/1.0.5/scala-xml_2.11-1.0.5.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-compiler/2.11.12/scala-compiler-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-library/2.11.12/scala-library-2.11.12.jar
[0m[32m[D][0m   => /home/skyler/.cache/coursier/v1/https/repo1.maven.org/maven2/org/scala-lang/scala-reflect/2.11.12/scala-reflect-2.11.12.jar
[0m[32m[D][0m Configured SemanticDB in projects 'root', 'root-test'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher948363110451112207/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher948363110451112207/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 16:03:17 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 16:03:17 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8951415317842064787/bsp.socket'...
[0m2021.03.05 16:03:17 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5994758586419933930/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5994758586419933930/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5994758586419933930/bsp.socket...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8951415317842064787/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8951415317842064787/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.05 16:03:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 16:03:18 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.05 16:03:18 INFO  time: Connected to build server in 5.96s[0m
[0m2021.03.05 16:03:18 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.05 16:03:19 INFO  time: Imported build in 0.55s[0m
[0m2021.03.05 16:03:19 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      .config("spark.master", "local[4]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .text(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
      )
      .as[String]
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))

    commonCrawlTechJobs.show(2, false)

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "compression" -> "gzip",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/part-00299-364a895c-5e5c-46bb-846e-75ec7de82b3b.c000.gz.parquet"
        )
      )
      .load()

    val exampleFormat = df
      .filter(
        ($"url_path" contains "jobs") and ($"content_languages" === "eng")
      )
      .select($"url_host_name", $"url_path" as "sample_path")
      .groupBy("url_host_name", "sample_path")
      .count()
      .orderBy($"count" desc)
      .as("n")

    exampleFormat.show(200, false)
  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

Mar 05, 2021 4:03:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1
[0m2021.03.05 16:03:21 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.05 16:03:21 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.05 16:03:21 INFO  time: indexed workspace in 3.3s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 05, 2021 4:03:23 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.05 16:07:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:07:49 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:07:49 INFO  time: compiled root in 4.99s[0m
[0m2021.03.05 16:16:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:16:16 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:16:16 INFO  time: compiled root in 2.29s[0m
[0m2021.03.05 16:18:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:18:51 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:18:51 INFO  time: compiled root in 1.95s[0m
[0m2021.03.05 16:22:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:22:46 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:22:46 INFO  time: compiled root in 1.76s[0m
[0m2021.03.05 16:25:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:25:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:25:19 INFO  time: compiled root in 1.86s[0m
[0m2021.03.05 16:25:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:25:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:25:23 INFO  time: compiled root in 2.51s[0m
[0m2021.03.05 16:25:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:25:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:25:26 INFO  time: compiled root in 2.57s[0m
[0m2021.03.05 16:28:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:28:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:28:08 INFO  time: compiled root in 1.32s[0m
[0m2021.03.05 16:59:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:59:22 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:59:22 INFO  time: compiled root in 2.54s[0m
[0m2021.03.05 16:59:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:59:28 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:59:28 INFO  time: compiled root in 2.49s[0m
[0m2021.03.05 16:59:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 16:59:31 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 16:59:31 INFO  time: compiled root in 1.88s[0m
[0m2021.03.05 17:01:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:01:44 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:01:44 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 17:01:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:01:47 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:01:47 INFO  time: compiled root in 1.39s[0m
[0m2021.03.05 17:02:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:02:50 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 17:02:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:02:56 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:02:56 INFO  time: compiled root in 1.09s[0m
[0m2021.03.05 17:03:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:03:03 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:03:03 INFO  time: compiled root in 1.9s[0m
[0m2021.03.05 17:04:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:05 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:05 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 17:04:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:08 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:08 INFO  time: compiled root in 1.18s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.util.{Locale, Properties}

import scala.collection.JavaConverters._

import com.fasterxml.jackson.databind.ObjectMapper
import com.univocity.parsers.csv.CsvParser

import org.apache.spark.Partition
import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst.json.{CreateJacksonParser, JacksonParser, JSONOptions}
import org.apache.spark.sql.catalyst.util.CaseInsensitiveMap
import org.apache.spark.sql.execution.command.DDLUtils
import org.apache.spark.sql.execution.datasources.{DataSource, FailureSafeParser}
import org.apache.spark.sql.execution.datasources.csv._
import org.apache.spark.sql.execution.datasources.jdbc._
import org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation
import org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils
import org.apache.spark.sql.sources.v2.{DataSourceOptions, DataSourceV2, ReadSupport}
import org.apache.spark.sql.types.{StringType, StructType}
import org.apache.spark.unsafe.types.UTF8String

/**
 * Interface used to load a [[Dataset]] from external storage systems (e.g. file systems,
 * key-value stores, etc). Use `SparkSession.read` to access this.
 *
 * @since 1.4.0
 */
@InterfaceStability.Stable
class DataFrameReader private[sql](sparkSession: SparkSession) extends Logging {

  /**
   * Specifies the input data source format.
   *
   * @since 1.4.0
   */
  def format(source: String): DataFrameReader = {
    this.source = source
    this
  }

  /**
   * Specifies the input schema. Some data sources (e.g. JSON) can infer the input schema
   * automatically from data. By specifying the schema here, the underlying data source can
   * skip the schema inference step, and thus speed up data loading.
   *
   * @since 1.4.0
   */
  def schema(schema: StructType): DataFrameReader = {
    this.userSpecifiedSchema = Option(schema)
    this
  }

  /**
   * Specifies the schema by using the input DDL-formatted string. Some data sources (e.g. JSON) can
   * infer the input schema automatically from data. By specifying the schema here, the underlying
   * data source can skip the schema inference step, and thus speed up data loading.
   *
   * {{{
   *   spark.read.schema("a INT, b STRING, c DOUBLE").csv("test.csv")
   * }}}
   *
   * @since 2.3.0
   */
  def schema(schemaString: String): DataFrameReader = {
    this.userSpecifiedSchema = Option(StructType.fromDDL(schemaString))
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def option(key: String, value: String): DataFrameReader = {
    this.extraOptions += (key -> value)
    this
  }

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Boolean): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Long): DataFrameReader = option(key, value.toString)

  /**
   * Adds an input option for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * @since 2.0.0
   */
  def option(key: String, value: Double): DataFrameReader = option(key, value.toString)

  /**
   * (Scala-specific) Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: scala.collection.Map[String, String]): DataFrameReader = {
    this.extraOptions ++= options
    this
  }

  /**
   * Adds input options for the underlying data source.
   *
   * All options are maintained in a case-insensitive way in terms of key names.
   * If a new option has the same key case-insensitively, it will override the existing option.
   *
   * You can set the following option(s):
   * <ul>
   * <li>`timeZone` (default session local timezone): sets the string that indicates a timezone
   * to be used to parse timestamps in the JSON/CSV datasources or partition values.</li>
   * </ul>
   *
   * @since 1.4.0
   */
  def options(options: java.util.Map[String, String]): DataFrameReader = {
    this.options(options.asScala)
    this
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that don't require a path (e.g. external
   * key-value stores).
   *
   * @since 1.4.0
   */
  def load(): DataFrame = {
    load(Seq.empty: _*) // force invocation of `load(...varargs...)`
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that require a path (e.g. data backed by
   * a local or distributed file system).
   *
   * @since 1.4.0
   */
  def load(path: String): DataFrame = {
    // force invocation of `load(...varargs...)`
    option(DataSourceOptions.PATH_KEY, path).load(Seq.empty: _*)
  }

  /**
   * Loads input in as a `DataFrame`, for data sources that support multiple paths.
   * Only works if the source is a HadoopFsRelationProvider.
   *
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def load(paths: String*): DataFrame = {
    if (source.toLowerCase(Locale.ROOT) == DDLUtils.HIVE_PROVIDER) {
      throw new AnalysisException("Hive data source can only be used with tables, you can not " +
        "read files of Hive data source directly.")
    }

    val cls = DataSource.lookupDataSource(source, sparkSession.sessionState.conf)
    if (classOf[DataSourceV2].isAssignableFrom(cls)) {
      val ds = cls.newInstance().asInstanceOf[DataSourceV2]
      if (ds.isInstanceOf[ReadSupport]) {
        val sessionOptions = DataSourceV2Utils.extractSessionConfigs(
          ds = ds, conf = sparkSession.sessionState.conf)
        val pathsOption = {
          val objectMapper = new ObjectMapper()
          DataSourceOptions.PATHS_KEY -> objectMapper.writeValueAsString(paths.toArray)
        }
        Dataset.ofRows(sparkSession, DataSourceV2Relation.create(
          ds, sessionOptions ++ extraOptions.toMap + pathsOption,
          userSpecifiedSchema = userSpecifiedSchema))
      } else {
        loadV1Source(paths: _*)
      }
    } else {
      loadV1Source(paths: _*)
    }
  }

  private def loadV1Source(paths: String*) = {
    // Code path for data source v1.
    sparkSession.baseRelationToDataFrame(
      DataSource.apply(
        sparkSession,
        paths = paths,
        userSpecifiedSchema = userSpecifiedSchema,
        className = source,
        options = extraOptions.toMap).resolveRelation())
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table and connection properties.
   *
   * @since 1.4.0
   */
  def jdbc(url: String, table: String, properties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // properties should override settings in extraOptions.
    this.extraOptions ++= properties.asScala
    // explicit url and dbtable should override all
    this.extraOptions ++= Seq(JDBCOptions.JDBC_URL -> url, JDBCOptions.JDBC_TABLE_NAME -> table)
    format("jdbc").load()
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table. Partitions of the table will be retrieved in parallel based on the parameters
   * passed to this function.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`.
   * @param table Name of the table in the external database.
   * @param columnName the name of a column of numeric, date, or timestamp type
   *                   that will be used for partitioning.
   * @param lowerBound the minimum value of `columnName` used to decide partition stride.
   * @param upperBound the maximum value of `columnName` used to decide partition stride.
   * @param numPartitions the number of partitions. This, along with `lowerBound` (inclusive),
   *                      `upperBound` (exclusive), form partition strides for generated WHERE
   *                      clause expressions used to split the column `columnName` evenly. When
   *                      the input is less than 1, the number is set to 1.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch and "queryTimeout" can be used to wait
   *                             for a Statement object to execute to the given number of seconds.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      columnName: String,
      lowerBound: Long,
      upperBound: Long,
      numPartitions: Int,
      connectionProperties: Properties): DataFrame = {
    // columnName, lowerBound, upperBound and numPartitions override settings in extraOptions.
    this.extraOptions ++= Map(
      JDBCOptions.JDBC_PARTITION_COLUMN -> columnName,
      JDBCOptions.JDBC_LOWER_BOUND -> lowerBound.toString,
      JDBCOptions.JDBC_UPPER_BOUND -> upperBound.toString,
      JDBCOptions.JDBC_NUM_PARTITIONS -> numPartitions.toString)
    jdbc(url, table, connectionProperties)
  }

  /**
   * Construct a `DataFrame` representing the database table accessible via JDBC URL
   * url named table using connection properties. The `predicates` parameter gives a list
   * expressions suitable for inclusion in WHERE clauses; each one defines one partition
   * of the `DataFrame`.
   *
   * Don't create too many partitions in parallel on a large cluster; otherwise Spark might crash
   * your external database systems.
   *
   * @param url JDBC database url of the form `jdbc:subprotocol:subname`
   * @param table Name of the table in the external database.
   * @param predicates Condition in the where clause for each partition.
   * @param connectionProperties JDBC database connection arguments, a list of arbitrary string
   *                             tag/value. Normally at least a "user" and "password" property
   *                             should be included. "fetchsize" can be used to control the
   *                             number of rows per fetch.
   * @since 1.4.0
   */
  def jdbc(
      url: String,
      table: String,
      predicates: Array[String],
      connectionProperties: Properties): DataFrame = {
    assertNoSpecifiedSchema("jdbc")
    // connectionProperties should override settings in extraOptions.
    val params = extraOptions ++ connectionProperties.asScala
    val options = new JDBCOptions(url, table, params)
    val parts: Array[Partition] = predicates.zipWithIndex.map { case (part, i) =>
      JDBCPartition(part, i) : Partition
    }
    val relation = JDBCRelation(parts, options)(sparkSession)
    sparkSession.baseRelationToDataFrame(relation)
  }

  /**
   * Loads a JSON file and returns the results as a `DataFrame`.
   *
   * See the documentation on the overloaded `json()` method with varargs for more details.
   *
   * @since 1.4.0
   */
  def json(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    json(Seq(path): _*)
  }

  /**
   * Loads JSON files and returns the results as a `DataFrame`.
   *
   * <a href="http://jsonlines.org/">JSON Lines</a> (newline-delimited JSON) is supported by
   * default. For JSON (one record per file), set the `multiLine` option to true.
   *
   * This function goes through the input once to determine the input schema. If you know the
   * schema in advance, use the version that specifies the schema to avoid the extra scan.
   *
   * You can set the following JSON-specific options to deal with non-standard JSON files:
   * <ul>
   * <li>`primitivesAsString` (default `false`): infers all primitive values as a string type</li>
   * <li>`prefersDecimal` (default `false`): infers all floating-point values as a decimal
   * type. If the values do not fit in decimal, then it infers them as doubles.</li>
   * <li>`allowComments` (default `false`): ignores Java/C++ style comment in JSON records</li>
   * <li>`allowUnquotedFieldNames` (default `false`): allows unquoted JSON field names</li>
   * <li>`allowSingleQuotes` (default `true`): allows single quotes in addition to double quotes
   * </li>
   * <li>`allowNumericLeadingZeros` (default `false`): allows leading zeros in numbers
   * (e.g. 00012)</li>
   * <li>`allowBackslashEscapingAnyCharacter` (default `false`): allows accepting quoting of all
   * character using backslash quoting mechanism</li>
   * <li>`allowUnquotedControlChars` (default `false`): allows JSON Strings to contain unquoted
   * control characters (ASCII characters with value less than 32, including tab and line feed
   * characters) or not.</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   * during parsing.
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To
   *     keep corrupt records, an user can set a string type field named
   *     `columnNameOfCorruptRecord` in an user-defined schema. If a schema does not have the
   *     field, it drops corrupt records during parsing. When inferring a schema, it implicitly
   *     adds a `columnNameOfCorruptRecord` field in an output schema.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines,
   * per file</li>
   * <li>`encoding` (by default it is not set): allows to forcibly set one of standard basic
   * or extended encoding for the JSON files. For example UTF-16BE, UTF-32LE. If the encoding
   * is not specified and `multiLine` is set to `true`, it will be detected automatically.</li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of input JSON objects used
   * for schema inferring.</li>
   * <li>`dropFieldIfAllNull` (default `false`): whether to ignore column of all null values or
   * empty array/struct during schema inference.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def json(paths: String*): DataFrame = format("json").load(paths : _*)

  /**
   * Loads a `JavaRDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON
   * Lines text format or newline-delimited JSON</a>) and returns the result as
   * a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: JavaRDD[String]): DataFrame = json(jsonRDD.rdd)

  /**
   * Loads an `RDD[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonRDD input RDD with one JSON object per record
   * @since 1.4.0
   */
  @deprecated("Use json(Dataset[String]) instead.", "2.2.0")
  def json(jsonRDD: RDD[String]): DataFrame = {
    json(sparkSession.createDataset(jsonRDD)(Encoders.STRING))
  }

  /**
   * Loads a `Dataset[String]` storing JSON objects (<a href="http://jsonlines.org/">JSON Lines
   * text format or newline-delimited JSON</a>) and returns the result as a `DataFrame`.
   *
   * Unless the schema is specified using `schema` function, this function goes through the
   * input once to determine the input schema.
   *
   * @param jsonDataset input Dataset with one JSON object per record
   * @since 2.2.0
   */
  def json(jsonDataset: Dataset[String]): DataFrame = {
    val parsedOptions = new JSONOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.sessionLocalTimeZone,
      sparkSession.sessionState.conf.columnNameOfCorruptRecord)

    val schema = userSpecifiedSchema.getOrElse {
      TextInputJsonDataSource.inferFromDataset(jsonDataset, parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val createParser = CreateJacksonParser.string _
    val parsed = jsonDataset.rdd.mapPartitions { iter =>
      val rawParser = new JacksonParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => rawParser.parse(input, createParser, UTF8String.fromString),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = jsonDataset.isStreaming)
  }

  /**
   * Loads a CSV file and returns the result as a `DataFrame`. See the documentation on the
   * other overloaded `csv()` method for more details.
   *
   * @since 2.0.0
   */
  def csv(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    csv(Seq(path): _*)
  }

  /**
   * Loads an `Dataset[String]` storing CSV rows and returns the result as a `DataFrame`.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is enabled,
   * this function goes through the input once to determine the input schema.
   *
   * If the schema is not specified using `schema` function and `inferSchema` option is disabled,
   * it determines the columns as string types and it reads only the first line to determine the
   * names and the number of fields.
   *
   * If the enforceSchema is set to `false`, only the CSV header in the first line is checked
   * to conform specified or inferred schema.
   *
   * @param csvDataset input Dataset with one CSV row per record
   * @since 2.2.0
   */
  def csv(csvDataset: Dataset[String]): DataFrame = {
    val parsedOptions: CSVOptions = new CSVOptions(
      extraOptions.toMap,
      sparkSession.sessionState.conf.csvColumnPruning,
      sparkSession.sessionState.conf.sessionLocalTimeZone)
    val filteredLines: Dataset[String] =
      CSVUtils.filterCommentAndEmpty(csvDataset, parsedOptions)
    val maybeFirstLine: Option[String] = filteredLines.take(1).headOption

    val schema = userSpecifiedSchema.getOrElse {
      TextInputCSVDataSource.inferFromDataset(
        sparkSession,
        csvDataset,
        maybeFirstLine,
        parsedOptions)
    }

    verifyColumnNameOfCorruptRecord(schema, parsedOptions.columnNameOfCorruptRecord)
    val actualSchema =
      StructType(schema.filterNot(_.name == parsedOptions.columnNameOfCorruptRecord))

    val linesWithoutHeader = if (parsedOptions.headerFlag && maybeFirstLine.isDefined) {
      val firstLine = maybeFirstLine.get
      val parser = new CsvParser(parsedOptions.asParserSettings)
      val columnNames = parser.parseLine(firstLine)
      CSVDataSource.checkHeaderColumnNames(
        actualSchema,
        columnNames,
        csvDataset.getClass.getCanonicalName,
        parsedOptions.enforceSchema,
        sparkSession.sessionState.conf.caseSensitiveAnalysis)
      filteredLines.rdd.mapPartitions(CSVUtils.filterHeaderLine(_, firstLine, parsedOptions))
    } else {
      filteredLines.rdd
    }

    val parsed = linesWithoutHeader.mapPartitions { iter =>
      val rawParser = new UnivocityParser(actualSchema, parsedOptions)
      val parser = new FailureSafeParser[String](
        input => Seq(rawParser.parse(input)),
        parsedOptions.parseMode,
        schema,
        parsedOptions.columnNameOfCorruptRecord)
      iter.flatMap(parser.parse)
    }
    sparkSession.internalCreateDataFrame(parsed, schema, isStreaming = csvDataset.isStreaming)
  }

  /**
   * Loads CSV files and returns the result as a `DataFrame`.
   *
   * This function will go through the input once to determine the input schema if `inferSchema`
   * is enabled. To avoid going through the entire data once, disable `inferSchema` option or
   * specify the schema explicitly using `schema`.
   *
   * You can set the following CSV-specific options to deal with CSV files:
   * <ul>
   * <li>`sep` (default `,`): sets a single character as a separator for each
   * field and value.</li>
   * <li>`encoding` (default `UTF-8`): decodes the CSV files by the given encoding
   * type.</li>
   * <li>`quote` (default `"`): sets a single character used for escaping quoted values where
   * the separator can be part of the value. If you would like to turn off quotations, you need to
   * set not `null` but an empty string. This behaviour is different from
   * `com.databricks.spark.csv`.</li>
   * <li>`escape` (default `\`): sets a single character used for escaping quotes inside
   * an already quoted value.</li>
   * <li>`charToEscapeQuoteEscaping` (default `escape` or `\0`): sets a single character used for
   * escaping the escape for the quote character. The default value is escape character when escape
   * and quote characters are different, `\0` otherwise.</li>
   * <li>`comment` (default empty string): sets a single character used for skipping lines
   * beginning with this character. By default, it is disabled.</li>
   * <li>`header` (default `false`): uses the first line as names of columns.</li>
   * <li>`enforceSchema` (default `true`): If it is set to `true`, the specified or inferred schema
   * will be forcibly applied to datasource files, and headers in CSV files will be ignored.
   * If the option is set to `false`, the schema will be validated against all headers in CSV files
   * in the case when the `header` option is set to `true`. Field names in the schema
   * and column names in CSV headers are checked by their positions taking into account
   * `spark.sql.caseSensitive`. Though the default value is true, it is recommended to disable
   * the `enforceSchema` option to avoid incorrect results.</li>
   * <li>`inferSchema` (default `false`): infers the input schema automatically from data. It
   * requires one extra pass over the data.</li>
   * <li>`samplingRatio` (default is 1.0): defines fraction of rows used for schema inferring.</li>
   * <li>`ignoreLeadingWhiteSpace` (default `false`): a flag indicating whether or not leading
   * whitespaces from values being read should be skipped.</li>
   * <li>`ignoreTrailingWhiteSpace` (default `false`): a flag indicating whether or not trailing
   * whitespaces from values being read should be skipped.</li>
   * <li>`nullValue` (default empty string): sets the string representation of a null value. Since
   * 2.0.1, this applies to all supported types including the string type.</li>
   * <li>`emptyValue` (default empty string): sets the string representation of an empty value.</li>
   * <li>`nanValue` (default `NaN`): sets the string representation of a non-number" value.</li>
   * <li>`positiveInf` (default `Inf`): sets the string representation of a positive infinity
   * value.</li>
   * <li>`negativeInf` (default `-Inf`): sets the string representation of a negative infinity
   * value.</li>
   * <li>`dateFormat` (default `yyyy-MM-dd`): sets the string that indicates a date format.
   * Custom date formats follow the formats at `java.text.SimpleDateFormat`. This applies to
   * date type.</li>
   * <li>`timestampFormat` (default `yyyy-MM-dd'T'HH:mm:ss.SSSXXX`): sets the string that
   * indicates a timestamp format. Custom date formats follow the formats at
   * `java.text.SimpleDateFormat`. This applies to timestamp type.</li>
   * <li>`maxColumns` (default `20480`): defines a hard limit of how many columns
   * a record can have.</li>
   * <li>`maxCharsPerColumn` (default `-1`): defines the maximum number of characters allowed
   * for any given value being read. By default, it is -1 meaning unlimited length</li>
   * <li>`mode` (default `PERMISSIVE`): allows a mode for dealing with corrupt records
   *    during parsing. It supports the following case-insensitive modes. Note that Spark tries
   *    to parse only required columns in CSV under column pruning. Therefore, corrupt records
   *    can be different based on required set of fields. This behavior can be controlled by
   *    `spark.sql.csv.parser.columnPruning.enabled` (enabled by default).
   *   <ul>
   *     <li>`PERMISSIVE` : when it meets a corrupted record, puts the malformed string into a
   *     field configured by `columnNameOfCorruptRecord`, and sets other fields to `null`. To keep
   *     corrupt records, an user can set a string type field named `columnNameOfCorruptRecord`
   *     in an user-defined schema. If a schema does not have the field, it drops corrupt records
   *     during parsing. A record with less/more tokens than schema is not a corrupted record to
   *     CSV. When it meets a record having fewer tokens than the length of the schema, sets
   *     `null` to extra fields. When the record has more tokens than the length of the schema,
   *     it drops extra tokens.</li>
   *     <li>`DROPMALFORMED` : ignores the whole corrupted records.</li>
   *     <li>`FAILFAST` : throws an exception when it meets corrupted records.</li>
   *   </ul>
   * </li>
   * <li>`columnNameOfCorruptRecord` (default is the value specified in
   * `spark.sql.columnNameOfCorruptRecord`): allows renaming the new field having malformed string
   * created by `PERMISSIVE` mode. This overrides `spark.sql.columnNameOfCorruptRecord`.</li>
   * <li>`multiLine` (default `false`): parse one record, which may span multiple lines.</li>
   * </ul>
   *
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def csv(paths: String*): DataFrame = format("csv").load(paths : _*)

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`. See the documentation
   * on the other overloaded `parquet()` method for more details.
   *
   * @since 2.0.0
   */
  def parquet(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    parquet(Seq(path): _*)
  }

  /**
   * Loads a Parquet file, returning the result as a `DataFrame`.
   *
   * You can set the following Parquet-specific option(s) for reading Parquet files:
   * <ul>
   * <li>`mergeSchema` (default is the value specified in `spark.sql.parquet.mergeSchema`): sets
   * whether we should merge schemas collected from all Parquet part-files. This will override
   * `spark.sql.parquet.mergeSchema`.</li>
   * </ul>
   * @since 1.4.0
   */
  @scala.annotation.varargs
  def parquet(paths: String*): DataFrame = {
    format("parquet").load(paths: _*)
  }

  /**
   * Loads an ORC file and returns the result as a `DataFrame`.
   *
   * @param path input path
   * @since 1.5.0
   */
  def orc(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    orc(Seq(path): _*)
  }

  /**
   * Loads ORC files and returns the result as a `DataFrame`.
   *
   * @param paths input paths
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orc(paths: String*): DataFrame = format("orc").load(paths: _*)

  /**
   * Returns the specified table as a `DataFrame`.
   *
   * @since 1.4.0
   */
  def table(tableName: String): DataFrame = {
    assertNoSpecifiedSchema("table")
    sparkSession.table(tableName)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any. See the documentation on
   * the other overloaded `text()` method for more details.
   *
   * @since 2.0.0
   */
  def text(path: String): DataFrame = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    text(Seq(path): _*)
  }

  /**
   * Loads text files and returns a `DataFrame` whose schema starts with a string column named
   * "value", and followed by partitioned columns if there are any.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.text("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().text("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following text-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input paths
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def text(paths: String*): DataFrame = format("text").load(paths : _*)

  /**
   * Loads text files and returns a [[Dataset]] of String. See the documentation on the
   * other overloaded `textFile()` method for more details.
   * @since 2.0.0
   */
  def textFile(path: String): Dataset[String] = {
    // This method ensures that calls that explicit need single argument works, see SPARK-16009
    textFile(Seq(path): _*)
  }

  /**
   * Loads text files and returns a [[Dataset]] of String. The underlying schema of the Dataset
   * contains a single string column named "value".
   *
   * If the directory structure of the text files contains partitioning information, those are
   * ignored in the resulting Dataset. To include partitioning information as columns, use `text`.
   *
   * By default, each line in the text files is a new row in the resulting DataFrame. For example:
   * {{{
   *   // Scala:
   *   spark.read.textFile("/path/to/spark/README.md")
   *
   *   // Java:
   *   spark.read().textFile("/path/to/spark/README.md")
   * }}}
   *
   * You can set the following textFile-specific option(s) for reading text files:
   * <ul>
   * <li>`wholetext` (default `false`): If true, read a file as a single row and not split by "\n".
   * </li>
   * <li>`lineSep` (default covers all `\r`, `\r\n` and `\n`): defines the line separator
   * that should be used for parsing.</li>
   * </ul>
   *
   * @param paths input path
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def textFile(paths: String*): Dataset[String] = {
    assertNoSpecifiedSchema("textFile")
    text(paths : _*).select("value").as[String](sparkSession.implicits.newStringEncoder)
  }

  /**
   * A convenient function for schema validation in APIs.
   */
  private def assertNoSpecifiedSchema(operation: String): Unit = {
    if (userSpecifiedSchema.nonEmpty) {
      throw new AnalysisException(s"User specified schema not supported with `$operation`")
    }
  }

  /**
   * A convenient function for schema validation in datasources supporting
   * `columnNameOfCorruptRecord` as an option.
   */
  private def verifyColumnNameOfCorruptRecord(
      schema: StructType,
      columnNameOfCorruptRecord: String): Unit = {
    schema.getFieldIndex(columnNameOfCorruptRecord).foreach { corruptFieldIndex =>
      val f = schema(corruptFieldIndex)
      if (f.dataType != StringType || !f.nullable) {
        throw new AnalysisException(
          "The field for corrupt records must be string type and nullable")
      }
    }
  }

  ///////////////////////////////////////////////////////////////////////////////////////
  // Builder pattern config options
  ///////////////////////////////////////////////////////////////////////////////////////

  private var source: String = sparkSession.sessionState.conf.defaultDataSourceName

  private var userSpecifiedSchema: Option[StructType] = None

  private var extraOptions = CaseInsensitiveMap[String](Map.empty)

}

[0m2021.03.05 17:04:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:14 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:14 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 17:04:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:19 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:20 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 17:04:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:23 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:23 INFO  time: compiled root in 1.11s[0m
[0m2021.03.05 17:04:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:04:26 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:04:26 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 17:06:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:06:07 INFO  time: compiled root in 1.02s[0m
[0m2021.03.05 17:06:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:06:11 INFO  time: compiled root in 1.74s[0m
[0m2021.03.05 17:08:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:08:58 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 17:09:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:09:04 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:09:04 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 17:09:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:09:09 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:09:09 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 17:09:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:09:10 WARN  there was one feature warning; re-run with -feature for details[0m
[0m2021.03.05 17:09:10 INFO  time: compiled root in 1.06s[0m
[0m2021.03.05 17:09:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:09:19 INFO  time: compiled root in 0.99s[0m
Mar 05, 2021 5:10:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 468
Mar 05, 2021 5:10:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 465
[0m2021.03.05 17:10:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:10:15 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 17:10:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:10:22 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:10:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:10:24 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 17:10:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:10:26 INFO  time: compiled root in 0.98s[0m
[0m2021.03.05 17:12:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:18 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 17:12:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:23 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 17:12:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:27 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 17:12:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:28 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:12:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:31 INFO  time: compiled root in 0.17s[0m
Mar 05, 2021 5:12:37 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 578
[0m2021.03.05 17:12:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:38 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 17:12:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:40 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 17:12:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:41 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:12:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:44 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 17:12:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:12:47 INFO  time: compiled root in 1.16s[0m
Mar 05, 2021 5:14:23 PM org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint notify
INFO: Unsupported notification method: $/setTraceNotification
[0m2021.03.05 17:26:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:26:45 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:26:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:26:58 INFO  time: compiled root in 1.05s[0m
[0m2021.03.05 17:27:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:27:02 INFO  time: compiled root in 0.95s[0m
[0m2021.03.05 17:27:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:27:23 INFO  time: compiled root in 1.15s[0m
[0m2021.03.05 17:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:27:46 INFO  time: compiled root in 1.71s[0m
[0m2021.03.05 17:27:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:27:48 INFO  time: compiled root in 1.4s[0m
[0m2021.03.05 17:31:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:31:32 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 17:31:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:31:34 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 17:37:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:37:01 INFO  time: compiled root in 1.05s[0m
[0m2021.03.05 17:37:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:37:05 INFO  time: compiled root in 1.08s[0m
[0m2021.03.05 17:37:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:37:22 INFO  time: compiled root in 1.02s[0m
[0m2021.03.05 17:37:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:37:26 INFO  time: compiled root in 1.58s[0m
[0m2021.03.05 17:42:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:42:13 INFO  time: compiled root in 1.34s[0m
[0m2021.03.05 17:43:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:43:53 INFO  time: compiled root in 1.14s[0m
[0m2021.03.05 17:46:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:46:23 INFO  time: compiled root in 0.93s[0m
[0m2021.03.05 17:46:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:46:27 INFO  time: compiled root in 0.94s[0m
[0m2021.03.05 17:47:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:47:45 INFO  time: compiled root in 0.96s[0m
[0m2021.03.05 17:47:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:47:54 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 17:48:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:48:01 INFO  time: compiled root in 0.19s[0m
[0m2021.03.05 17:48:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:48:16 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 17:48:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:48:19 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 17:50:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:50:20 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 17:50:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:50:25 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 17:50:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:50:29 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 17:50:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:50:34 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 17:50:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:50:44 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 17:51:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:01 INFO  time: compiled root in 0.22s[0m
[0m2021.03.05 17:51:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:07 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:51:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:09 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:51:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:13 INFO  time: compiled root in 0.24s[0m
[0m2021.03.05 17:51:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:48 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 17:51:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:49 INFO  time: compiled root in 0.23s[0m
[0m2021.03.05 17:51:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:54 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 17:51:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:51:58 INFO  time: compiled root in 0.21s[0m
Mar 05, 2021 5:52:00 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: null
Mar 05, 2021 5:52:00 PM scala.meta.internal.pc.CompletionProvider expected$1
WARNING: null
[0m2021.03.05 17:52:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:01 INFO  time: compiled root in 0.25s[0m
[0m2021.03.05 17:52:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:04 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 17:52:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:09 INFO  time: compiled root in 0.22s[0m
[0m2021.03.05 17:52:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:12 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 17:52:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:15 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 17:52:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:52:23 INFO  time: compiled root in 0.96s[0m
[0m2021.03.05 17:54:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:54:32 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 17:54:33 INFO  compiling root (1 scala source)[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.Closeable
import java.util.concurrent.atomic.AtomicReference

import scala.collection.JavaConverters._
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.spark.{SPARK_VERSION, SparkConf, SparkContext, TaskContext}
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.internal.Logging
import org.apache.spark.rdd.RDD
import org.apache.spark.scheduler.{SparkListener, SparkListenerApplicationEnd}
import org.apache.spark.sql.catalog.Catalog
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis.UnresolvedRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions.AttributeReference
import org.apache.spark.sql.catalyst.plans.logical.{LocalRelation, Range}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.internal._
import org.apache.spark.sql.internal.StaticSQLConf.CATALOG_IMPLEMENTATION
import org.apache.spark.sql.sources.BaseRelation
import org.apache.spark.sql.streaming._
import org.apache.spark.sql.types.{DataType, StructType}
import org.apache.spark.sql.util.ExecutionListenerManager
import org.apache.spark.util.{CallSite, Utils}


/**
 * The entry point to programming Spark with the Dataset and DataFrame API.
 *
 * In environments that this has been created upfront (e.g. REPL, notebooks), use the builder
 * to get an existing session:
 *
 * {{{
 *   SparkSession.builder().getOrCreate()
 * }}}
 *
 * The builder can also be used to create a new session:
 *
 * {{{
 *   SparkSession.builder
 *     .master("local")
 *     .appName("Word Count")
 *     .config("spark.some.config.option", "some-value")
 *     .getOrCreate()
 * }}}
 *
 * @param sparkContext The Spark context associated with this Spark session.
 * @param existingSharedState If supplied, use the existing shared state
 *                            instead of creating a new one.
 * @param parentSessionState If supplied, inherit all session state (i.e. temporary
 *                            views, SQL config, UDFs etc) from parent.
 */
@InterfaceStability.Stable
class SparkSession private(
    @transient val sparkContext: SparkContext,
    @transient private val existingSharedState: Option[SharedState],
    @transient private val parentSessionState: Option[SessionState],
    @transient private[sql] val extensions: SparkSessionExtensions)
  extends Serializable with Closeable with Logging { self =>

  // The call site where this SparkSession was constructed.
  private val creationSite: CallSite = Utils.getCallSite()

  private[sql] def this(sc: SparkContext) {
    this(sc, None, None, new SparkSessionExtensions)
  }

  sparkContext.assertNotStopped()

  // If there is no active SparkSession, uses the default SQL conf. Otherwise, use the session's.
  SQLConf.setSQLConfGetter(() => {
    SparkSession.getActiveSession.filterNot(_.sparkContext.isStopped).map(_.sessionState.conf)
      .getOrElse(SQLConf.getFallbackConf)
  })

  /**
   * The version of Spark on which this application is running.
   *
   * @since 2.0.0
   */
  def version: String = SPARK_VERSION

  /* ----------------------- *
   |  Session-related state  |
   * ----------------------- */

  /**
   * State shared across sessions, including the `SparkContext`, cached data, listener,
   * and a catalog that interacts with external systems.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sharedState: SharedState = {
    existingSharedState.getOrElse(new SharedState(sparkContext))
  }

  /**
   * Initial options for session. This options are applied once when sessionState is created.
   */
  @transient
  private[sql] val initialSessionOptions = new scala.collection.mutable.HashMap[String, String]

  /**
   * State isolated across sessions, including SQL configurations, temporary tables, registered
   * functions, and everything else that accepts a [[org.apache.spark.sql.internal.SQLConf]].
   * If `parentSessionState` is not null, the `SessionState` will be a copy of the parent.
   *
   * This is internal to Spark and there is no guarantee on interface stability.
   *
   * @since 2.2.0
   */
  @InterfaceStability.Unstable
  @transient
  lazy val sessionState: SessionState = {
    parentSessionState
      .map(_.clone(this))
      .getOrElse {
        val state = SparkSession.instantiateSessionState(
          SparkSession.sessionStateClassName(sparkContext.conf),
          self)
        initialSessionOptions.foreach { case (k, v) => state.conf.setConfString(k, v) }
        state
      }
  }

  /**
   * A wrapped version of this session in the form of a [[SQLContext]], for backward compatibility.
   *
   * @since 2.0.0
   */
  @transient
  val sqlContext: SQLContext = new SQLContext(this)

  /**
   * Runtime configuration interface for Spark.
   *
   * This is the interface through which the user can get and set all Spark and Hadoop
   * configurations that are relevant to Spark SQL. When getting the value of a config,
   * this defaults to the value set in the underlying `SparkContext`, if any.
   *
   * @since 2.0.0
   */
  @transient lazy val conf: RuntimeConfig = new RuntimeConfig(sessionState.conf)

  /**
   * :: Experimental ::
   * An interface to register custom [[org.apache.spark.sql.util.QueryExecutionListener]]s
   * that listen for execution metrics.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def listenerManager: ExecutionListenerManager = sessionState.listenerManager

  /**
   * :: Experimental ::
   * A collection of methods that are considered experimental, but can be used to hook into
   * the query planner for advanced functionality.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def experimental: ExperimentalMethods = sessionState.experimentalMethods

  /**
   * A collection of methods for registering user-defined functions (UDF).
   *
   * The following example registers a Scala closure as UDF:
   * {{{
   *   sparkSession.udf.register("myUDF", (arg1: Int, arg2: String) => arg2 + arg1)
   * }}}
   *
   * The following example registers a UDF in Java:
   * {{{
   *   sparkSession.udf().register("myUDF",
   *       (Integer arg1, String arg2) -> arg2 + arg1,
   *       DataTypes.StringType);
   * }}}
   *
   * @note The user-defined functions must be deterministic. Due to optimization,
   * duplicate invocations may be eliminated or the function may even be invoked more times than
   * it is present in the query.
   *
   * @since 2.0.0
   */
  def udf: UDFRegistration = sessionState.udfRegistration

  /**
   * :: Experimental ::
   * Returns a `StreamingQueryManager` that allows managing all the
   * `StreamingQuery`s active on `this`.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Unstable
  def streams: StreamingQueryManager = sessionState.streamingQueryManager

  /**
   * Start a new session with isolated SQL configurations, temporary tables, registered
   * functions are isolated, but sharing the underlying `SparkContext` and cached data.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   *
   * @since 2.0.0
   */
  def newSession(): SparkSession = {
    new SparkSession(sparkContext, Some(sharedState), parentSessionState = None, extensions)
  }

  /**
   * Create an identical copy of this `SparkSession`, sharing the underlying `SparkContext`
   * and shared state. All the state of this session (i.e. SQL configurations, temporary tables,
   * registered functions) is copied over, and the cloned session is set up with the same shared
   * state as this session. The cloned session is independent of this session, that is, any
   * non-global change in either session is not reflected in the other.
   *
   * @note Other than the `SparkContext`, all shared state is initialized lazily.
   * This method will force the initialization of the shared state to ensure that parent
   * and child sessions are set up with the same shared state. If the underlying catalog
   * implementation is Hive, this will initialize the metastore, which may take some time.
   */
  private[sql] def cloneSession(): SparkSession = {
    val result = new SparkSession(sparkContext, Some(sharedState), Some(sessionState), extensions)
    result.sessionState // force copy of SessionState
    result
  }


  /* --------------------------------- *
   |  Methods for creating DataFrames  |
   * --------------------------------- */

  /**
   * Returns a `DataFrame` with no rows or columns.
   *
   * @since 2.0.0
   */
  @transient
  lazy val emptyDataFrame: DataFrame = {
    createDataFrame(sparkContext.emptyRDD[Row].setName("empty"), StructType(Nil))
  }

  /**
   * :: Experimental ::
   * Creates a new [[Dataset]] of type T containing zero elements.
   *
   * @return 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def emptyDataset[T: Encoder]: Dataset[T] = {
    val encoder = implicitly[Encoder[T]]
    new Dataset(self, LocalRelation(encoder.schema.toAttributes), encoder)
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from an RDD of Product (e.g. case classes, tuples).
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](rdd: RDD[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val encoder = Encoders.product[A]
    Dataset.ofRows(self, ExternalRDD(rdd, self)(encoder))
  }

  /**
   * :: Experimental ::
   * Creates a `DataFrame` from a local Seq of Product.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataFrame[A <: Product : TypeTag](data: Seq[A]): DataFrame = {
    SparkSession.setActiveSession(this)
    val schema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]
    val attributeSeq = schema.toAttributes
    Dataset.ofRows(self, LocalRelation.fromProduct(attributeSeq, data))
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from an `RDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   * Example:
   * {{{
   *  import org.apache.spark.sql._
   *  import org.apache.spark.sql.types._
   *  val sparkSession = new org.apache.spark.sql.SparkSession(sc)
   *
   *  val schema =
   *    StructType(
   *      StructField("name", StringType, false) ::
   *      StructField("age", IntegerType, true) :: Nil)
   *
   *  val people =
   *    sc.textFile("examples/src/main/resources/people.txt").map(
   *      _.split(",")).map(p => Row(p(0), p(1).trim.toInt))
   *  val dataFrame = sparkSession.createDataFrame(people, schema)
   *  dataFrame.printSchema
   *  // root
   *  // |-- name: string (nullable = false)
   *  // |-- age: integer (nullable = true)
   *
   *  dataFrame.createOrReplaceTempView("people")
   *  sparkSession.sql("select name from people").collect.foreach(println)
   * }}}
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: RDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD, schema, needsConversion = true)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `JavaRDD` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided RDD matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rowRDD: JavaRDD[Row], schema: StructType): DataFrame = {
    createDataFrame(rowRDD.rdd, schema)
  }

  /**
   * :: DeveloperApi ::
   * Creates a `DataFrame` from a `java.util.List` containing [[Row]]s using the given schema.
   * It is important to make sure that the structure of every [[Row]] of the provided List matches
   * the provided schema. Otherwise, there will be runtime exception.
   *
   * @since 2.0.0
   */
  @DeveloperApi
  @InterfaceStability.Evolving
  def createDataFrame(rows: java.util.List[Row], schema: StructType): DataFrame = {
    Dataset.ofRows(self, LocalRelation.fromExternalRows(schema.toAttributes, rows.asScala))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: RDD[_], beanClass: Class[_]): DataFrame = {
    val attributeSeq: Seq[AttributeReference] = getSchema(beanClass)
    val className = beanClass.getName
    val rowRdd = rdd.mapPartitions { iter =>
    // BeanInfo is not serializable so we must rediscover it remotely for each partition.
      SQLContext.beansToRows(iter, Utils.classForName(className), attributeSeq)
    }
    Dataset.ofRows(self, LogicalRDD(attributeSeq, rowRdd.setName(rdd.name))(self))
  }

  /**
   * Applies a schema to an RDD of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   * SELECT * queries will return the columns in an undefined order.
   *
   * @since 2.0.0
   */
  def createDataFrame(rdd: JavaRDD[_], beanClass: Class[_]): DataFrame = {
    createDataFrame(rdd.rdd, beanClass)
  }

  /**
   * Applies a schema to a List of Java Beans.
   *
   * WARNING: Since there is no guaranteed ordering for fields in a Java Bean,
   *          SELECT * queries will return the columns in an undefined order.
   * @since 1.6.0
   */
  def createDataFrame(data: java.util.List[_], beanClass: Class[_]): DataFrame = {
    val attrSeq = getSchema(beanClass)
    val rows = SQLContext.beansToRows(data.asScala.iterator, beanClass, attrSeq)
    Dataset.ofRows(self, LocalRelation(attrSeq, rows.toSeq))
  }

  /**
   * Convert a `BaseRelation` created for external data sources into a `DataFrame`.
   *
   * @since 2.0.0
   */
  def baseRelationToDataFrame(baseRelation: BaseRelation): DataFrame = {
    Dataset.ofRows(self, LogicalRelation(baseRelation))
  }

  /* ------------------------------- *
   |  Methods for creating DataSets  |
   * ------------------------------- */

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a local Seq of data of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Example ==
   *
   * {{{
   *
   *   import spark.implicits._
   *   case class Person(name: String, age: Long)
   *   val data = Seq(Person("Michael", 29), Person("Andy", 30), Person("Justin", 19))
   *   val ds = spark.createDataset(data)
   *
   *   ds.show()
   *   // +-------+---+
   *   // |   name|age|
   *   // +-------+---+
   *   // |Michael| 29|
   *   // |   Andy| 30|
   *   // | Justin| 19|
   *   // +-------+---+
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: Seq[T]): Dataset[T] = {
    // `ExpressionEncoder` is not thread-safe, here we create a new encoder.
    val enc = encoderFor[T].copy()
    val attributes = enc.schema.toAttributes
    val encoded = data.map(d => enc.toRow(d).copy())
    val plan = new LocalRelation(attributes, encoded)
    Dataset[T](self, plan)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from an RDD of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: RDD[T]): Dataset[T] = {
    Dataset[T](self, ExternalRDD(data, self))
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] from a `java.util.List` of a given type. This method requires an
   * encoder (to convert a JVM object of type `T` to and from the internal Spark SQL representation)
   * that is generally created automatically through implicits from a `SparkSession`, or can be
   * created explicitly by calling static methods on [[Encoders]].
   *
   * == Java Example ==
   *
   * {{{
   *     List<String> data = Arrays.asList("hello", "world");
   *     Dataset<String> ds = spark.createDataset(data, Encoders.STRING());
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def createDataset[T : Encoder](data: java.util.List[T]): Dataset[T] = {
    createDataset(data.asScala)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from 0 to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(end: Long): Dataset[java.lang.Long] = range(0, end)

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with step value 1.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long): Dataset[java.lang.Long] = {
    range(start, end, step = 1, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long): Dataset[java.lang.Long] = {
    range(start, end, step, numPartitions = sparkContext.defaultParallelism)
  }

  /**
   * :: Experimental ::
   * Creates a [[Dataset]] with a single `LongType` column named `id`, containing elements
   * in a range from `start` to `end` (exclusive) with a step value, with partition number
   * specified.
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def range(start: Long, end: Long, step: Long, numPartitions: Int): Dataset[java.lang.Long] = {
    new Dataset(self, Range(start, end, step, numPartitions), Encoders.LONG)
  }

  /**
   * Creates a `DataFrame` from an `RDD[InternalRow]`.
   */
  private[sql] def internalCreateDataFrame(
      catalystRows: RDD[InternalRow],
      schema: StructType,
      isStreaming: Boolean = false): DataFrame = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val logicalPlan = LogicalRDD(
      schema.toAttributes,
      catalystRows,
      isStreaming = isStreaming)(self)
    Dataset.ofRows(self, logicalPlan)
  }

  /**
   * Creates a `DataFrame` from an `RDD[Row]`.
   * User can specify whether the input rows should be converted to Catalyst rows.
   */
  private[sql] def createDataFrame(
      rowRDD: RDD[Row],
      schema: StructType,
      needsConversion: Boolean) = {
    // TODO: use MutableProjection when rowRDD is another DataFrame and the applied
    // schema differs from the existing schema on any field data type.
    val catalystRows = if (needsConversion) {
      val encoder = RowEncoder(schema)
      rowRDD.map(encoder.toRow)
    } else {
      rowRDD.map { r: Row => InternalRow.fromSeq(r.toSeq) }
    }
    internalCreateDataFrame(catalystRows.setName(rowRDD.name), schema)
  }


  /* ------------------------- *
   |  Catalog-related methods  |
   * ------------------------- */

  /**
   * Interface through which the user may create, drop, alter or query underlying
   * databases, tables, functions etc.
   *
   * @since 2.0.0
   */
  @transient lazy val catalog: Catalog = new CatalogImpl(self)

  /**
   * Returns the specified table/view as a `DataFrame`.
   *
   * @param tableName is either a qualified or unqualified name that designates a table or view.
   *                  If a database is specified, it identifies the table/view from the database.
   *                  Otherwise, it first attempts to find a temporary view with the given name
   *                  and then match the table/view from the current database.
   *                  Note that, the global temporary view database is also valid here.
   * @since 2.0.0
   */
  def table(tableName: String): DataFrame = {
    table(sessionState.sqlParser.parseTableIdentifier(tableName))
  }

  private[sql] def table(tableIdent: TableIdentifier): DataFrame = {
    Dataset.ofRows(self, UnresolvedRelation(tableIdent))
  }

  /* ----------------- *
   |  Everything else  |
   * ----------------- */

  /**
   * Executes a SQL query using Spark, returning the result as a `DataFrame`.
   * The dialect that is used for SQL parsing can be configured with 'spark.sql.dialect'.
   *
   * @since 2.0.0
   */
  def sql(sqlText: String): DataFrame = {
    Dataset.ofRows(self, sessionState.sqlParser.parsePlan(sqlText))
  }

  /**
   * Returns a [[DataFrameReader]] that can be used to read non-streaming data in as a
   * `DataFrame`.
   * {{{
   *   sparkSession.read.parquet("/path/to/file.parquet")
   *   sparkSession.read.schema(schema).json("/path/to/file.json")
   * }}}
   *
   * @since 2.0.0
   */
  def read: DataFrameReader = new DataFrameReader(self)

  /**
   * Returns a `DataStreamReader` that can be used to read streaming data in as a `DataFrame`.
   * {{{
   *   sparkSession.readStream.parquet("/path/to/directory/of/parquet/files")
   *   sparkSession.readStream.schema(schema).json("/path/to/directory/of/json/files")
   * }}}
   *
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def readStream: DataStreamReader = new DataStreamReader(self)

  /**
   * Executes some code block and prints to stdout the time taken to execute the block. This is
   * available in Scala only and is used primarily for interactive testing and debugging.
   *
   * @since 2.1.0
   */
  def time[T](f: => T): T = {
    val start = System.nanoTime()
    val ret = f
    val end = System.nanoTime()
    // scalastyle:off println
    println(s"Time taken: ${(end - start) / 1000 / 1000} ms")
    // scalastyle:on println
    ret
  }

  // scalastyle:off
  // Disable style checker so "implicits" object can start with lowercase i
  /**
   * :: Experimental ::
   * (Scala-specific) Implicit methods available in Scala for converting
   * common Scala objects into `DataFrame`s.
   *
   * {{{
   *   val sparkSession = SparkSession.builder.getOrCreate()
   *   import sparkSession.implicits._
   * }}}
   *
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  object implicits extends SQLImplicits with Serializable {
    protected override def _sqlContext: SQLContext = SparkSession.this.sqlContext
  }
  // scalastyle:on

  /**
   * Stop the underlying `SparkContext`.
   *
   * @since 2.0.0
   */
  def stop(): Unit = {
    sparkContext.stop()
  }

  /**
   * Synonym for `stop()`.
   *
   * @since 2.1.0
   */
  override def close(): Unit = stop()

  /**
   * Parses the data type in our internal string representation. The data type string should
   * have the same format as the one generated by `toString` in scala.
   * It is only used by PySpark.
   */
  protected[sql] def parseDataType(dataTypeString: String): DataType = {
    DataType.fromJson(dataTypeString)
  }

  /**
   * Apply a schema defined by the schemaString to an RDD. It is only used by PySpark.
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schemaString: String): DataFrame = {
    val schema = DataType.fromJson(schemaString).asInstanceOf[StructType]
    applySchemaToPythonRDD(rdd, schema)
  }

  /**
   * Apply `schema` to an RDD.
   *
   * @note Used by PySpark only
   */
  private[sql] def applySchemaToPythonRDD(
      rdd: RDD[Array[Any]],
      schema: StructType): DataFrame = {
    val rowRdd = rdd.mapPartitions { iter =>
      val fromJava = python.EvaluatePython.makeFromJava(schema)
      iter.map(r => fromJava(r).asInstanceOf[InternalRow])
    }
    internalCreateDataFrame(rowRdd, schema)
  }

  /**
   * Returns a Catalyst Schema for the given java bean class.
   */
  private def getSchema(beanClass: Class[_]): Seq[AttributeReference] = {
    val (dataType, _) = JavaTypeInference.inferDataType(beanClass)
    dataType.asInstanceOf[StructType].fields.map { f =>
      AttributeReference(f.name, f.dataType, f.nullable)()
    }
  }

}


@InterfaceStability.Stable
object SparkSession extends Logging {

  /**
   * Builder for [[SparkSession]].
   */
  @InterfaceStability.Stable
  class Builder extends Logging {

    private[this] val options = new scala.collection.mutable.HashMap[String, String]

    private[this] val extensions = new SparkSessionExtensions

    private[this] var userSuppliedContext: Option[SparkContext] = None

    private[spark] def sparkContext(sparkContext: SparkContext): Builder = synchronized {
      userSuppliedContext = Option(sparkContext)
      this
    }

    /**
     * Sets a name for the application, which will be shown in the Spark web UI.
     * If no application name is set, a randomly generated name will be used.
     *
     * @since 2.0.0
     */
    def appName(name: String): Builder = config("spark.app.name", name)

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: String): Builder = synchronized {
      options += key -> value
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Long): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Double): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a config option. Options set using this method are automatically propagated to
     * both `SparkConf` and SparkSession's own configuration.
     *
     * @since 2.0.0
     */
    def config(key: String, value: Boolean): Builder = synchronized {
      options += key -> value.toString
      this
    }

    /**
     * Sets a list of config options based on the given `SparkConf`.
     *
     * @since 2.0.0
     */
    def config(conf: SparkConf): Builder = synchronized {
      conf.getAll.foreach { case (k, v) => options += k -> v }
      this
    }

    /**
     * Sets the Spark master URL to connect to, such as "local" to run locally, "local[4]" to
     * run locally with 4 cores, or "spark://master:7077" to run on a Spark standalone cluster.
     *
     * @since 2.0.0
     */
    def master(master: String): Builder = config("spark.master", master)

    /**
     * Enables Hive support, including connectivity to a persistent Hive metastore, support for
     * Hive serdes, and Hive user-defined functions.
     *
     * @since 2.0.0
     */
    def enableHiveSupport(): Builder = synchronized {
      if (hiveClassesArePresent) {
        config(CATALOG_IMPLEMENTATION.key, "hive")
      } else {
        throw new IllegalArgumentException(
          "Unable to instantiate SparkSession with Hive support because " +
            "Hive classes are not found.")
      }
    }

    /**
     * Inject extensions into the [[SparkSession]]. This allows a user to add Analyzer rules,
     * Optimizer rules, Planning Strategies or a customized parser.
     *
     * @since 2.2.0
     */
    def withExtensions(f: SparkSessionExtensions => Unit): Builder = synchronized {
      f(extensions)
      this
    }

    /**
     * Gets an existing [[SparkSession]] or, if there is no existing one, creates a new
     * one based on the options set in this builder.
     *
     * This method first checks whether there is a valid thread-local SparkSession,
     * and if yes, return that one. It then checks whether there is a valid global
     * default SparkSession, and if yes, return that one. If no valid global default
     * SparkSession exists, the method creates a new SparkSession and assigns the
     * newly created SparkSession as the global default.
     *
     * In case an existing SparkSession is returned, the non-static config options specified in
     * this builder will be applied to the existing SparkSession.
     *
     * @since 2.0.0
     */
    def getOrCreate(): SparkSession = synchronized {
      assertOnDriver()
      // Get the session from current thread's active session.
      var session = activeThreadSession.get()
      if ((session ne null) && !session.sparkContext.isStopped) {
        applyModifiableSettings(session)
        return session
      }

      // Global synchronization so we will only set the default session once.
      SparkSession.synchronized {
        // If the current thread does not have an active session, get it from the global session.
        session = defaultSession.get()
        if ((session ne null) && !session.sparkContext.isStopped) {
          applyModifiableSettings(session)
          return session
        }

        // No active nor global default session. Create a new one.
        val sparkContext = userSuppliedContext.getOrElse {
          val sparkConf = new SparkConf()
          options.foreach { case (k, v) => sparkConf.set(k, v) }

          // set a random app name if not given.
          if (!sparkConf.contains("spark.app.name")) {
            sparkConf.setAppName(java.util.UUID.randomUUID().toString)
          }

          SparkContext.getOrCreate(sparkConf)
          // Do not update `SparkConf` for existing `SparkContext`, as it's shared by all sessions.
        }

        // Initialize extensions if the user has defined a configurator class.
        val extensionConfOption = sparkContext.conf.get(StaticSQLConf.SPARK_SESSION_EXTENSIONS)
        if (extensionConfOption.isDefined) {
          val extensionConfClassName = extensionConfOption.get
          try {
            val extensionConfClass = Utils.classForName(extensionConfClassName)
            val extensionConf = extensionConfClass.newInstance()
              .asInstanceOf[SparkSessionExtensions => Unit]
            extensionConf(extensions)
          } catch {
            // Ignore the error if we cannot find the class or when the class has the wrong type.
            case e @ (_: ClassCastException |
                      _: ClassNotFoundException |
                      _: NoClassDefFoundError) =>
              logWarning(s"Cannot use $extensionConfClassName to configure session extensions.", e)
          }
        }

        session = new SparkSession(sparkContext, None, None, extensions)
        options.foreach { case (k, v) => session.initialSessionOptions.put(k, v) }
        setDefaultSession(session)
        setActiveSession(session)

        // Register a successfully instantiated context to the singleton. This should be at the
        // end of the class definition so that the singleton is updated only if there is no
        // exception in the construction of the instance.
        sparkContext.addSparkListener(new SparkListener {
          override def onApplicationEnd(applicationEnd: SparkListenerApplicationEnd): Unit = {
            defaultSession.set(null)
          }
        })
      }

      return session
    }

    private def applyModifiableSettings(session: SparkSession): Unit = {
      val (staticConfs, otherConfs) =
        options.partition(kv => SQLConf.staticConfKeys.contains(kv._1))

      otherConfs.foreach { case (k, v) => session.sessionState.conf.setConfString(k, v) }

      if (staticConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; the static sql configurations will not take" +
          " effect.")
      }
      if (otherConfs.nonEmpty) {
        logWarning("Using an existing SparkSession; some spark core configurations may not take" +
          " effect.")
      }
    }
  }

  /**
   * Creates a [[SparkSession.Builder]] for constructing a [[SparkSession]].
   *
   * @since 2.0.0
   */
  def builder(): Builder = new Builder

  /**
   * Changes the SparkSession that will be returned in this thread and its children when
   * SparkSession.getOrCreate() is called. This can be used to ensure that a given thread receives
   * a SparkSession with an isolated session, instead of the global (first created) context.
   *
   * @since 2.0.0
   */
  def setActiveSession(session: SparkSession): Unit = {
    activeThreadSession.set(session)
  }

  /**
   * Clears the active SparkSession for current thread. Subsequent calls to getOrCreate will
   * return the first created context instead of a thread-local override.
   *
   * @since 2.0.0
   */
  def clearActiveSession(): Unit = {
    activeThreadSession.remove()
  }

  /**
   * Sets the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def setDefaultSession(session: SparkSession): Unit = {
    defaultSession.set(session)
  }

  /**
   * Clears the default SparkSession that is returned by the builder.
   *
   * @since 2.0.0
   */
  def clearDefaultSession(): Unit = {
    defaultSession.set(null)
  }

  /**
   * Returns the active SparkSession for the current thread, returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getActiveSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(activeThreadSession.get)
    }
  }

  /**
   * Returns the default SparkSession that is returned by the builder.
   *
   * @note Return None, when calling this function on executors
   *
   * @since 2.2.0
   */
  def getDefaultSession: Option[SparkSession] = {
    if (TaskContext.get != null) {
      // Return None when running on executors.
      None
    } else {
      Option(defaultSession.get)
    }
  }

  /**
   * Returns the currently active SparkSession, otherwise the default one. If there is no default
   * SparkSession, throws an exception.
   *
   * @since 2.4.0
   */
  def active: SparkSession = {
    getActiveSession.getOrElse(getDefaultSession.getOrElse(
      throw new IllegalStateException("No active or default Spark session found")))
  }

  ////////////////////////////////////////////////////////////////////////////////////////
  // Private methods from now on
  ////////////////////////////////////////////////////////////////////////////////////////

  /** The active SparkSession for the current thread. */
  private val activeThreadSession = new InheritableThreadLocal[SparkSession]

  /** Reference to the root SparkSession. */
  private val defaultSession = new AtomicReference[SparkSession]

  private val HIVE_SESSION_STATE_BUILDER_CLASS_NAME =
    "org.apache.spark.sql.hive.HiveSessionStateBuilder"

  private def sessionStateClassName(conf: SparkConf): String = {
    conf.get(CATALOG_IMPLEMENTATION) match {
      case "hive" => HIVE_SESSION_STATE_BUILDER_CLASS_NAME
      case "in-memory" => classOf[SessionStateBuilder].getCanonicalName
    }
  }

  private def assertOnDriver(): Unit = {
    if (Utils.isTesting && TaskContext.get != null) {
      // we're accessing it during task execution, fail.
      throw new IllegalStateException(
        "SparkSession should only be created and accessed on the driver.")
    }
  }

  /**
   * Helper method to create an instance of `SessionState` based on `className` from conf.
   * The result is either `SessionState` or a Hive based `SessionState`.
   */
  private def instantiateSessionState(
      className: String,
      sparkSession: SparkSession): SessionState = {
    try {
      // invoke `new [Hive]SessionStateBuilder(SparkSession, Option[SessionState])`
      val clazz = Utils.classForName(className)
      val ctor = clazz.getConstructors.head
      ctor.newInstance(sparkSession, None).asInstanceOf[BaseSessionStateBuilder].build()
    } catch {
      case NonFatal(e) =>
        throw new IllegalArgumentException(s"Error while instantiating '$className':", e)
    }
  }

  /**
   * @return true if Hive classes can be loaded, otherwise false.
   */
  private[spark] def hiveClassesArePresent: Boolean = {
    try {
      Utils.classForName(HIVE_SESSION_STATE_BUILDER_CLASS_NAME)
      Utils.classForName("org.apache.hadoop.hive.conf.HiveConf")
      true
    } catch {
      case _: ClassNotFoundException | _: NoClassDefFoundError => false
    }
  }

  private[spark] def cleanupAnyExistingSession(): Unit = {
    val session = getActiveSession.orElse(getDefaultSession)
    if (session.isDefined) {
      logWarning(
        s"""An existing Spark session exists as the active or default session.
           |This probably means another suite leaked it. Attempting to stop it before continuing.
           |This existing Spark session was created at:
           |
           |${session.get.creationSite.longForm}
           |
         """.stripMargin)
      session.get.stop()
      SparkSession.clearActiveSession()
      SparkSession.clearDefaultSession()
    }
  }
}

[0m2021.03.05 17:54:35 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 17:54:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:55:00 INFO  time: compiled root in 1.13s[0m
[0m2021.03.05 17:55:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:55:07 INFO  time: compiled root in 1.18s[0m
[0m2021.03.05 17:58:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:58:25 INFO  time: compiled root in 1.07s[0m
[0m2021.03.05 17:58:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:58:28 INFO  time: compiled root in 0.91s[0m
[0m2021.03.05 17:58:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:58:31 INFO  time: compiled root in 0.96s[0m
[0m2021.03.05 17:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 17:58:33 INFO  time: compiled root in 0.97s[0m
[0m2021.03.05 18:12:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:12:46 INFO  time: compiled root in 1.1s[0m
[0m2021.03.05 18:12:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:12:59 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 18:13:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:13:04 INFO  time: compiled root in 1.08s[0m
Mar 05, 2021 6:13:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1380
[0m2021.03.05 18:13:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:13:31 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 18:18:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:18:08 INFO  time: compiled root in 1.2s[0m
Mar 05, 2021 6:18:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1416
[0m2021.03.05 18:23:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:23:59 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 18:24:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:24:01 INFO  time: compiled root in 1.01s[0m
[0m2021.03.05 18:27:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:27:36 INFO  time: compiled root in 1.04s[0m
[0m2021.03.05 18:28:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:28:51 INFO  time: compiled root in 1.16s[0m
[0m2021.03.05 18:32:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:32:12 INFO  time: compiled root in 0.26s[0m
[0m2021.03.05 18:32:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:32:15 INFO  time: compiled root in 0.21s[0m
[0m2021.03.05 18:32:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:32:22 INFO  time: compiled root in 1.3s[0m
[0m2021.03.05 18:32:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:32:22 INFO  time: compiled root in 0.17s[0m
[0m2021.03.05 18:32:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:32:28 INFO  time: compiled root in 1.16s[0m
[0m2021.03.05 18:35:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:35:15 INFO  time: compiled root in 2.74s[0m
[0m2021.03.05 18:35:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:35:32 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 18:35:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:35:41 INFO  time: compiled root in 1.07s[0m
[0m2021.03.05 18:35:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:35:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:35:54 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 18:36:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:05 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:06 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:12 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:14 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 18:36:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:14 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021commonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common CrawlcommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:20 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 18:36:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:34: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl iscommonCrawlTechJobs)
                                 ^[0m
[0m2021.03.05 18:36:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:22 INFO  time: compiled root in 0.13s[0m
[0m2021.03.05 18:36:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:29 INFO  time: compiled root in 0.11s[0m
[0m2021.03.05 18:36:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:123: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs)
                                                                                                                          ^[0m
[0m2021.03.05 18:36:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:124: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs"")
                                                                                                                           ^[0m
[0m2021.03.05 18:36:34 INFO  time: compiled root in 0.12s[0m
[0m2021.03.05 18:36:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:124: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs"")
                                                                                                                           ^[0m
[0m2021.03.05 18:36:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:124: stale bloop error: unclosed string literal
    val techJobCount = println(s"The number of computer tech jobs in the January 2021 Common Crawl is $commonCrawlTechJobs"")
                                                                                                                           ^[0m
[0m2021.03.05 18:36:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:38 INFO  time: compiled root in 1.11s[0m
[0m2021.03.05 18:36:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:41 INFO  time: compiled root in 1.12s[0m
[0m2021.03.05 18:36:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 18:36:50 INFO  time: compiled root in 1.39s[0m
Mar 05, 2021 7:31:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1754
[0m2021.03.05 19:45:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:45:06 INFO  time: compiled root in 3.52s[0m
[0m2021.03.05 19:45:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:45:09 INFO  time: compiled root in 1.87s[0m
[0m2021.03.05 19:49:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:49:47 INFO  time: compiled root in 1.74s[0m
[0m2021.03.05 19:49:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:49:49 INFO  time: compiled root in 1.37s[0m
[0m2021.03.05 19:49:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:49:52 INFO  time: compiled root in 0.46s[0m
[0m2021.03.05 19:49:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:49:56 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 19:50:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:50:07 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 19:50:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:50:38 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 19:50:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 19:50:43 INFO  time: compiled root in 2.26s[0m
[0m2021.03.05 20:04:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:04:20 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 20:04:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:04:24 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:05:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:03 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:05:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:06 INFO  time: compiled root in 0.2s[0m
[0m2021.03.05 20:05:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: identifier expected but 'val' found.
    val warcSchema = StructType(
    ^[0m
[0m2021.03.05 20:05:07 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: identifier expected but 'val' found.
    val warcSchema = StructType(
    ^[0m
[0m2021.03.05 20:05:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: identifier expected but 'val' found.
    val warcSchema = StructType(
    ^[0m
[0m2021.03.05 20:05:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:08 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:05:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:11 INFO  time: compiled root in 0.17s[0m
[0m2021.03.05 20:05:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers
                         ^[0m
[0m2021.03.05 20:05:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:16 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/
                         ^[0m
[0m2021.03.05 20:05:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:24 INFO  time: compiled root in 0.16s[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:27 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-uri-
                         ^[0m
[0m2021.03.05 20:05:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:29 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:141:26: stale bloop error: unclosed string literal
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count
                         ^[0m
[0m2021.03.05 20:05:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:42 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:42 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:05:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:7: stale bloop error: unclosed string literal
      "
      ^[0m
[0m2021.03.05 20:05:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:50 INFO  time: compiled root in 1.48s[0m
[0m2021.03.05 20:05:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:54 INFO  time: compiled root in 0.39s[0m
[0m2021.03.05 20:05:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:05:56 INFO  time: compiled root in 0.33s[0m
[0m2021.03.05 20:06:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:06:47 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 20:06:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:06:51 INFO  time: compiled root in 0.35s[0m
[0m2021.03.05 20:06:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:06:56 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:07:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:01 INFO  time: compiled root in 0.19s[0m
[0m2021.03.05 20:07:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:35 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 20:07:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:38 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:07:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:43 INFO  time: compiled root in 0.34s[0m
[0m2021.03.05 20:07:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:48 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 20:07:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:49 INFO  time: compiled root in 0.31s[0m
[0m2021.03.05 20:07:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:55 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:07:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: ')' expected but 'val' found.
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:07:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: ')' expected but 'val' found.
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:07:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: ')' expected but 'val' found.
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:07:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:07:57 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:07:57 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:08:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:09 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:09 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:08:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count()
                               ^[0m
[0m2021.03.05 20:08:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:11 INFO  time: compiled root in 0.16s[0m
[0m2021.03.05 20:08:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:19 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:21 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:32 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:32: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: commonCrawlJobs.count
                               ^[0m
[0m2021.03.05 20:08:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:33 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:93: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: $commonCrawlJobs.count
                                                                                            ^[0m
[0m2021.03.05 20:08:33 INFO  time: compiled root in 0.16s[0m
[0m2021.03.05 20:08:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:93: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: $commonCrawlJobs.count
                                                                                            ^[0m
[0m2021.03.05 20:08:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:34 INFO  time: compiled root in 0.16s[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:39 INFO  time: compiled root in 0.18s[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:216:4: stale bloop error: unclosed string literal
  }
   ^[0m
[0m2021.03.05 20:08:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:5: stale bloop error: Missing closing brace `}' assumed here
    val s3OutputBucket = "s3a://bigdatasamyers/job-webpage-count"
    ^[0m
[0m2021.03.05 20:08:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:102: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: ${commonCrawlJobs.count}""
                                                                                                     ^[0m
[0m2021.03.05 20:08:41 INFO  time: compiled root in 0.14s[0m
[0m2021.03.05 20:08:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:102: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: ${commonCrawlJobs.count}""
                                                                                                     ^[0m
[0m2021.03.05 20:08:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:102: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: ${commonCrawlJobs.count}""
                                                                                                     ^[0m
[0m2021.03.05 20:08:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:135:102: stale bloop error: unclosed string literal
      val jobCount = println(s"The total number of job posting websites is: ${commonCrawlJobs.count}""
                                                                                                     ^[0m
[0m2021.03.05 20:08:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:44 INFO  time: compiled root in 0.15s[0m
[0m2021.03.05 20:08:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:46 INFO  time: compiled root in 0.27s[0m
[0m2021.03.05 20:08:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:08:59 INFO  time: compiled root in 0.29s[0m
[0m2021.03.05 20:09:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:01 INFO  time: compiled root in 0.32s[0m
[0m2021.03.05 20:09:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:03 INFO  time: compiled root in 0.3s[0m
[0m2021.03.05 20:09:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:09 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 20:09:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:12 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 20:09:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:34 INFO  time: compiled root in 0.28s[0m
[0m2021.03.05 20:09:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:37 INFO  time: compiled root in 1.86s[0m
[0m2021.03.05 20:09:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:09:44 INFO  time: compiled root in 2.39s[0m
[0m2021.03.05 20:14:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.05 20:14:21 INFO  time: compiled root in 1.52s[0m
[0m2021.03.06 00:16:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 00:16:27 INFO  time: compiled root in 1.57s[0m
[0m2021.03.06 00:16:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 00:16:44 INFO  time: compiled root in 1.6s[0m
[0m2021.03.06 00:24:16 INFO  shutting down Metals[0m
No more data in the server stdin, exiting...
[0m2021.03.06 00:24:16 INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...[0m2021.03.06 00:24:16 
INFO  Shut down connection with build server.[0m
No more data in the server stdin, exiting...
[0m2021.03.06 00:24:16 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.06 01:04:23 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.54.1.[0m
[0m2021.03.06 01:04:24 INFO  time: initialize in 0.84s[0m
[0m2021.03.06 01:04:25 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6486866953414155351/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.06 01:04:24 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.06 01:04:26 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6486866953414155351/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6486866953414155351/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 01:04:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 01:04:26 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher1864940572885814817/bsp.socket'...
[0m2021.03.06 01:04:26 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher5977843772699536642/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher1864940572885814817/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher1864940572885814817/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher5977843772699536642/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher5977843772699536642/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.06 01:04:27 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 01:04:26 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.06 01:04:26 INFO  time: Connected to build server in 1.43s[0m
[0m2021.03.06 01:04:26 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.06 01:04:29 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.06 01:04:29 INFO  time: Imported build in 0.2s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-*.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))
      .count

    val techJobCount = println(s"The total number of computer tech jobs in the January 2021 Common Crawl is: $commonCrawlTechJobs")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz").coalesce(1, true).saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.06 01:04:35 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-*.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))
      .count

    val techJobCount = println(s"The total number of computer tech jobs in the January 2021 Common Crawl is: $commonCrawlTechJobs")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz").coalesce(1, true).saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.06 01:04:36 INFO  time: code lens generation in 10s[0m
[0m2021.03.06 01:04:38 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.06 01:04:38 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.06 01:04:40 INFO  time: indexed workspace in 11s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 06, 2021 1:04:42 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.06 01:06:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:06:16 INFO  time: compiled root in 3.94s[0m
[0m2021.03.06 01:06:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:06:27 INFO  time: compiled root in 3.14s[0m
Mar 06, 2021 1:06:37 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 49
[0m2021.03.06 01:06:37 INFO  compiling root (1 scala source)[0m
Exception in thread "pool-5-thread-1" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[0m2021.03.06 01:06:43 INFO  time: compiled root in 5.47s[0m
[0m2021.03.06 01:06:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:06:48 INFO  time: compiled root in 2.54s[0m
[0m2021.03.06 01:18:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:18:04 INFO  time: compiled root in 3.91s[0m
[0m2021.03.06 01:20:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:20:49 INFO  time: compiled root in 0.52s[0m
[0m2021.03.06 01:20:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:20:53 INFO  time: compiled root in 2.61s[0m
[0m2021.03.06 01:26:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:26:04 INFO  time: compiled root in 0.26s[0m
[0m2021.03.06 01:26:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:26:12 INFO  time: compiled root in 2.88s[0m
[0m2021.03.06 01:29:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.06 01:29:46 INFO  time: compiled root in 3.74s[0m
[0m2021.03.06 01:54:27 INFO  shutting down Metals[0m
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
No more data in the server stdin, exiting...
[0m2021.03.06 01:54:28 INFO  Shut down connection with build server.[0m
[0m2021.03.06 01:54:28 INFO  Shut down connection with build server.[0m
[0m2021.03.06 01:54:28 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
[0m2021.03.07 16:30:14 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.54.1.[0m
[0m2021.03.07 16:30:15 INFO  time: initialize in 0.48s[0m
[0m2021.03.07 16:30:15 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher6456865045575576994/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.07 16:30:15 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.07 16:30:15 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Loading 2 projects from '/home/skyler/project3/s3data/s3dataget/.bloop'...
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root.json'
[0m[32m[D][0m Loading project from '/home/skyler/project3/s3data/s3dataget/.bloop/root-test.json'
[0m[32m[D][0m Configured SemanticDB in projects 'root-test', 'root'
[0m[32m[D][0m Missing analysis file for project 'root-test'
[0m[32m[D][0m Loading previous analysis for 'root' from '/home/skyler/project3/s3data/s3dataget/target/streams/compile/bloopAnalysisOut/_global/streams/inc_compile_2.11.zip'.
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher6456865045575576994/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher6456865045575576994/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.07 16:30:16 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.07 16:30:16 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher9048600401026322160/bsp.socket'...
[0m2021.03.07 16:30:16 INFO  Attempting to connect to the build server...[0m
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher319341542432032698/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher319341542432032698/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher319341542432032698/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher9048600401026322160/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher9048600401026322160/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
[0m2021.03.07 16:30:16 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.07 16:30:16 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.07 16:30:16 INFO  time: Connected to build server in 0.57s[0m
[0m2021.03.07 16:30:16 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.07 16:30:17 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.07 16:30:17 INFO  time: Imported build in 0.1s[0m
[0m2021.03.07 16:30:18 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))
      .show(false)

    //val techJobCount = println(s"The total number of computer tech jobs in the January 2021 Common Crawl is: $commonCrawlTechJobs")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz").coalesce(1, true).saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.attempts.maximum", "30")
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val commonCrawlJobs = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .filter($"value" rlike jobsRegex)

    val commonCrawlTechJobs = commonCrawlJobs
      .filter($"value" rlike techJobs.mkString("|"))
      .show(false)

    //val techJobCount = println(s"The total number of computer tech jobs in the January 2021 Common Crawl is: $commonCrawlTechJobs")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext.textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz").coalesce(1, true).saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.07 16:30:20 INFO  time: code lens generation in 4.39s[0m
[0m2021.03.07 16:30:21 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.07 16:30:21 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
[0m2021.03.07 16:30:21 INFO  time: indexed workspace in 5.16s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Mar 07, 2021 4:30:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "build/publishDiagnostics",
  "params": {
    "textDocument": {
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala"
    },
    "buildTarget": {
      "uri": "file:/home/skyler/project3/s3data/s3dataget/?id\u003droot"
    },
    "diagnostics": [
      {
        "range": {
          "start": {
            "line": 0,
            "character": 0
          },
          "end": {
            "line": 0,
            "character": 0
          }
        },
        "severity": 2,
        "code": "package `com.revature.scala`",
        "source": "bloop",
        "message": "\nFound names but no class, trait or object is defined in the compilation unit.\nThe incremental compiler cannot record the dependency information in such case.\nSome errors like unused import referring to a non-existent class might not be reported.\n    "
      }
    ],
    "reset": true
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.nio.file.NoSuchFileException: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/RDD.scala
	at sun.nio.fs.UnixException.translateToIOException(UnixException.java:86)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:102)
	at sun.nio.fs.UnixException.rethrowAsIOException(UnixException.java:107)
	at sun.nio.fs.UnixFileSystemProvider.newByteChannel(UnixFileSystemProvider.java:214)
	at java.nio.file.Files.newByteChannel(Files.java:361)
	at java.nio.file.Files.newByteChannel(Files.java:407)
	at java.nio.file.Files.readAllBytes(Files.java:3152)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.Diagnostics.onPublishDiagnostics(Diagnostics.scala:141)
	at scala.meta.internal.metals.Diagnostics.onBuildPublishDiagnostics(Diagnostics.scala:121)
	at scala.meta.internal.metals.ForwardingMetalsBuildClient.onBuildPublishDiagnostics(ForwardingMetalsBuildClient.scala:99)
	... 16 more

[0m2021.03.07 16:30:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:39 INFO  time: compiled root in 3.53s[0m
[0m2021.03.07 16:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:42 INFO  time: compiled root in 0.17s[0m
[0m2021.03.07 16:30:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:49 INFO  time: compiled root in 0.19s[0m
[0m2021.03.07 16:30:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:55 INFO  time: compiled root in 0.16s[0m
[0m2021.03.07 16:30:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:56 INFO  time: compiled root in 0.17s[0m
[0m2021.03.07 16:30:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:30:59 INFO  time: compiled root in 0.42s[0m
[0m2021.03.07 16:31:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:03 INFO  time: compiled root in 1.84s[0m
Mar 07, 2021 4:31:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 87
Mar 07, 2021 4:31:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 90
[0m2021.03.07 16:31:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:05 INFO  time: compiled root in 0.43s[0m
[0m2021.03.07 16:31:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:12 INFO  time: compiled root in 1.59s[0m
[0m2021.03.07 16:31:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:15 INFO  time: compiled root in 1.36s[0m
[0m2021.03.07 16:31:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:44 INFO  time: compiled root in 0.34s[0m
[0m2021.03.07 16:31:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:31:57 INFO  time: compiled root in 0.3s[0m
[0m2021.03.07 16:32:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:04 INFO  time: compiled root in 0.13s[0m
[0m2021.03.07 16:32:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:09 INFO  time: compiled root in 0.35s[0m
[0m2021.03.07 16:32:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:13 INFO  time: compiled root in 0.36s[0m
[0m2021.03.07 16:32:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:14 INFO  time: compiled root in 0.28s[0m
[0m2021.03.07 16:32:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:17 INFO  time: compiled root in 0.35s[0m
[0m2021.03.07 16:32:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:18 INFO  time: compiled root in 0.34s[0m
[0m2021.03.07 16:32:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:32:20 INFO  time: compiled root in 0.33s[0m
[0m2021.03.07 16:34:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:11 INFO  time: compiled root in 0.26s[0m
[0m2021.03.07 16:34:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:16 INFO  time: compiled root in 0.12s[0m
Mar 07, 2021 4:34:24 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: String index out of range: -1
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
	at java.lang.String.<init>(String.java:196)
	at scala.tools.nsc.interactive.Global.typeCompletions$1(Global.scala:1229)
	at scala.tools.nsc.interactive.Global.completionsAt(Global.scala:1252)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:375)
	at scala.meta.internal.pc.SignatureHelpProvider$$anonfun$8.apply(SignatureHelpProvider.scala:373)
	at scala.Option.map(Option.scala:146)
	at scala.meta.internal.pc.SignatureHelpProvider.treeSymbol(SignatureHelpProvider.scala:373)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCall$.unapply(SignatureHelpProvider.scala:180)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.visit(SignatureHelpProvider.scala:309)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.traverse(SignatureHelpProvider.scala:303)
	at scala.meta.internal.pc.SignatureHelpProvider$MethodCallTraverser.fromTree(SignatureHelpProvider.scala:272)
	at scala.meta.internal.pc.SignatureHelpProvider.signatureHelp(SignatureHelpProvider.scala:27)

[0m2021.03.07 16:34:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:26 INFO  time: compiled root in 0.13s[0m
[0m2021.03.07 16:34:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:29 INFO  time: compiled root in 0.16s[0m
[0m2021.03.07 16:34:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:37 INFO  time: compiled root in 0.13s[0m
[0m2021.03.07 16:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:21: stale bloop error: identifier expected but string literal found.
    commonCrawlJobs["0"]
                    ^[0m
[0m2021.03.07 16:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:1: stale bloop error: ']' expected but ';' found.
    val commonCrawlTechJobs = commonCrawlJobs
^[0m
[0m2021.03.07 16:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:21: stale bloop error: identifier expected but string literal found.
    commonCrawlJobs["0"]
                    ^[0m
[0m2021.03.07 16:34:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:139:1: stale bloop error: ']' expected but ';' found.
    val commonCrawlTechJobs = commonCrawlJobs
^[0m
[0m2021.03.07 16:34:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:34:47 INFO  time: compiled root in 0.29s[0m
[0m2021.03.07 16:35:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:35:12 INFO  time: compiled root in 1.44s[0m
Mar 07, 2021 4:35:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 383
[0m2021.03.07 16:35:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:35:17 INFO  time: compiled root in 1.27s[0m
[0m2021.03.07 16:35:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:35:21 INFO  time: compiled root in 1.46s[0m
[0m2021.03.07 16:40:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:05 INFO  time: compiled root in 0.26s[0m
[0m2021.03.07 16:40:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:10 INFO  time: compiled root in 0.56s[0m
[0m2021.03.07 16:40:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:14 INFO  time: compiled root in 2.41s[0m
[0m2021.03.07 16:40:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:18 INFO  time: compiled root in 2.75s[0m
[0m2021.03.07 16:40:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:25 INFO  time: compiled root in 2.44s[0m
[0m2021.03.07 16:40:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:33 INFO  time: compiled root in 1.51s[0m
[0m2021.03.07 16:40:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:36 INFO  time: compiled root in 0.33s[0m
[0m2021.03.07 16:40:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:38 INFO  time: compiled root in 0.36s[0m
[0m2021.03.07 16:40:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:40:56 INFO  time: compiled root in 0.41s[0m
[0m2021.03.07 16:41:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:00 INFO  time: compiled root in 0.33s[0m
[0m2021.03.07 16:41:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:04 INFO  time: compiled root in 0.32s[0m
[0m2021.03.07 16:41:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:23 INFO  time: compiled root in 0.29s[0m
[0m2021.03.07 16:41:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:30 INFO  time: compiled root in 0.38s[0m
[0m2021.03.07 16:41:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:34 INFO  time: compiled root in 0.43s[0m
[0m2021.03.07 16:41:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:35 INFO  time: compiled root in 0.7s[0m
[0m2021.03.07 16:41:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:40 INFO  time: compiled root in 0.35s[0m
[0m2021.03.07 16:41:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:44 INFO  time: compiled root in 1.84s[0m
[0m2021.03.07 16:41:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:41:49 INFO  time: compiled root in 1.37s[0m
[0m2021.03.07 16:42:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:42:03 INFO  time: compiled root in 1.42s[0m
[0m2021.03.07 16:42:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:42:05 INFO  time: compiled root in 1.5s[0m
[0m2021.03.07 16:42:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:42:08 INFO  time: compiled root in 1.49s[0m
[0m2021.03.07 16:45:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:45:23 INFO  time: compiled root in 2.61s[0m
[0m2021.03.07 16:50:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:50:38 INFO  time: compiled root in 1.96s[0m
[0m2021.03.07 16:50:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:50:42 INFO  time: compiled root in 2.52s[0m
[0m2021.03.07 16:50:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:50:49 INFO  time: compiled root in 2.22s[0m
[0m2021.03.07 16:50:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:50:56 INFO  time: compiled root in 2.54s[0m
[0m2021.03.07 16:51:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:51:05 INFO  time: compiled root in 2.67s[0m
[0m2021.03.07 16:51:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:51:28 INFO  time: compiled root in 3.03s[0m
[0m2021.03.07 16:54:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:54:48 INFO  time: compiled root in 2.66s[0m
[0m2021.03.07 16:59:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 16:59:08 INFO  time: compiled root in 2.31s[0m
[0m2021.03.07 17:02:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:02:13 INFO  time: compiled root in 1.56s[0m
[0m2021.03.07 17:02:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:02:20 INFO  time: compiled root in 1.72s[0m
[0m2021.03.07 17:05:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:05:08 INFO  time: compiled root in 1.46s[0m
[0m2021.03.07 17:10:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:10:51 INFO  time: compiled root in 1.55s[0m
[0m2021.03.07 17:15:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:15:44 INFO  time: compiled root in 2.56s[0m
[0m2021.03.07 17:15:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:15:58 INFO  time: compiled root in 2.05s[0m
[0m2021.03.07 17:18:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:18:59 INFO  time: compiled root in 1.9s[0m
[0m2021.03.07 17:23:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:23:03 INFO  time: compiled root in 1.01s[0m
[0m2021.03.07 17:23:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:23:06 INFO  time: compiled root in 1.02s[0m
[0m2021.03.07 17:23:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:23:12 INFO  time: compiled root in 1.04s[0m
[0m2021.03.07 17:23:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:23:17 INFO  time: compiled root in 1.06s[0m
[0m2021.03.07 17:26:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:26:16 INFO  time: compiled root in 1.01s[0m
[0m2021.03.07 17:26:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:26:19 INFO  time: compiled root in 1.08s[0m
[0m2021.03.07 17:26:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:26:22 INFO  time: compiled root in 1s[0m
[0m2021.03.07 17:26:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:26:25 INFO  time: compiled root in 1.16s[0m
[0m2021.03.07 17:27:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:27:39 INFO  time: compiled root in 0.24s[0m
[0m2021.03.07 17:27:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:27:42 INFO  time: compiled root in 0.28s[0m
[0m2021.03.07 17:28:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:28:31 INFO  time: compiled root in 1.01s[0m
[0m2021.03.07 17:28:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:28:32 INFO  time: compiled root in 1.03s[0m
[0m2021.03.07 17:28:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:28:35 INFO  time: compiled root in 1.08s[0m
[0m2021.03.07 17:28:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:28:40 INFO  time: compiled root in 1.04s[0m
[0m2021.03.07 17:28:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:28:43 INFO  time: compiled root in 1.12s[0m
[0m2021.03.07 17:30:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:30:25 INFO  time: compiled root in 1.15s[0m
[0m2021.03.07 17:39:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:39:13 INFO  time: compiled root in 1.13s[0m
[0m2021.03.07 17:39:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:39:14 INFO  time: compiled root in 1.19s[0m
[0m2021.03.07 17:39:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:39:17 INFO  time: compiled root in 1.12s[0m
[0m2021.03.07 17:39:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:39:25 INFO  time: compiled root in 1.11s[0m
[0m2021.03.07 17:39:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:39:42 INFO  time: compiled root in 1.02s[0m
[0m2021.03.07 17:40:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:40:18 INFO  time: compiled root in 1.14s[0m
[0m2021.03.07 17:41:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:41:01 INFO  time: compiled root in 1.08s[0m
[0m2021.03.07 17:41:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:41:06 INFO  time: compiled root in 1.08s[0m
[0m2021.03.07 17:41:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:41:15 INFO  time: compiled root in 1.16s[0m
[0m2021.03.07 17:41:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:41:19 INFO  time: compiled root in 1.5s[0m
[0m2021.03.07 17:42:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:42:27 INFO  time: compiled root in 1.06s[0m
[0m2021.03.07 17:45:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:45:34 INFO  time: compiled root in 1.15s[0m
[0m2021.03.07 17:48:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:48:50 INFO  time: compiled root in 1.18s[0m
[0m2021.03.07 17:49:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:49:07 INFO  time: compiled root in 2.04s[0m
[0m2021.03.07 17:57:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:00 INFO  time: compiled root in 1s[0m
[0m2021.03.07 17:58:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:03 INFO  time: compiled root in 0.24s[0m
[0m2021.03.07 17:58:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:06 INFO  time: compiled root in 1.05s[0m
[0m2021.03.07 17:58:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:14 INFO  time: compiled root in 0.16s[0m
[0m2021.03.07 17:58:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:17 INFO  time: compiled root in 1.13s[0m
[0m2021.03.07 17:58:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:32 INFO  time: compiled root in 1.08s[0m
[0m2021.03.07 17:58:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 17:58:35 INFO  time: compiled root in 1.03s[0m
[0m2021.03.07 18:04:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:04:01 INFO  time: compiled root in 1.16s[0m
[0m2021.03.07 18:04:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:04:12 INFO  time: compiled root in 1.18s[0m
[0m2021.03.07 18:06:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:07:00 INFO  time: compiled root in 1.26s[0m
[0m2021.03.07 18:10:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:10:55 INFO  time: compiled root in 1.12s[0m
[0m2021.03.07 18:34:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:34:40 INFO  time: compiled root in 1.42s[0m
[0m2021.03.07 18:37:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 18:37:38 INFO  time: compiled root in 1.5s[0m
[0m2021.03.07 19:09:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:09:01 INFO  time: compiled root in 1.38s[0m
[0m2021.03.07 19:09:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:09:04 INFO  time: compiled root in 1.41s[0m
[0m2021.03.07 19:13:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:13:59 INFO  time: compiled root in 1.46s[0m
[0m2021.03.07 19:14:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:01 INFO  time: compiled root in 0.3s[0m
[0m2021.03.07 19:14:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:10 INFO  time: compiled root in 0.25s[0m
[0m2021.03.07 19:14:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:12 INFO  time: compiled root in 0.23s[0m
[0m2021.03.07 19:14:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:18 INFO  time: compiled root in 0.31s[0m
[0m2021.03.07 19:14:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:21 INFO  time: compiled root in 0.31s[0m
[0m2021.03.07 19:14:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:40 INFO  time: compiled root in 0.23s[0m
[0m2021.03.07 19:14:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:43 INFO  time: compiled root in 0.23s[0m
Mar 07, 2021 7:14:48 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 1853
[0m2021.03.07 19:14:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:48 INFO  time: compiled root in 0.35s[0m
[0m2021.03.07 19:14:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:49 INFO  time: compiled root in 0.25s[0m
[0m2021.03.07 19:14:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:51 INFO  time: compiled root in 0.24s[0m
[0m2021.03.07 19:14:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:14:55 INFO  time: compiled root in 0.26s[0m
[0m2021.03.07 19:15:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:15:04 INFO  time: compiled root in 0.29s[0m
[0m2021.03.07 19:15:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:15:07 INFO  time: compiled root in 0.22s[0m
[0m2021.03.07 19:19:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:19:02 INFO  time: compiled root in 0.27s[0m
[0m2021.03.07 19:19:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:19:15 INFO  time: compiled root in 0.25s[0m
[0m2021.03.07 19:19:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:19:17 INFO  time: compiled root in 0.26s[0m
something's wrong: no file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala, 3615, 3615, 3630)
something's wrong: no file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala in org.apache.spark.sql.Dataset[String]RangePosition(file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala, 3615, 3615, 3630)
[0m2021.03.07 19:21:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:11 INFO  time: compiled root in 0.27s[0m
[0m2021.03.07 19:21:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:21 INFO  time: compiled root in 0.27s[0m
[0m2021.03.07 19:21:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:33 INFO  time: compiled root in 1.13s[0m
[0m2021.03.07 19:21:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:37 INFO  time: compiled root in 1.16s[0m
[0m2021.03.07 19:21:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:40 INFO  time: compiled root in 0.26s[0m
[0m2021.03.07 19:21:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:42 INFO  time: compiled root in 1.12s[0m
[0m2021.03.07 19:21:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:21:53 INFO  time: compiled root in 1.29s[0m
Mar 07, 2021 7:27:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2191
Mar 07, 2021 7:27:08 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 07, 2021 7:27:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 2200
[0m2021.03.07 19:27:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:27:12 INFO  time: compiled root in 1.88s[0m
[0m2021.03.07 19:27:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:27:25 INFO  time: compiled root in 1.11s[0m
[0m2021.03.07 19:27:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:27:26 INFO  time: compiled root in 0.14s[0m
[0m2021.03.07 19:27:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:27:29 INFO  time: compiled root in 1.25s[0m
[0m2021.03.07 19:34:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 19:34:25 INFO  time: compiled root in 1.6s[0m
[0m2021.03.07 20:29:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 20:29:52 INFO  time: compiled root in 1.52s[0m
[0m2021.03.07 20:30:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 20:30:43 INFO  time: compiled root in 1.61s[0m
[0m2021.03.07 20:30:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 20:30:44 INFO  time: compiled root in 0.28s[0m
[0m2021.03.07 20:30:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.07 20:30:47 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 11:28:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:28:59 INFO  time: compiled root in 1.92s[0m
[0m2021.03.08 11:29:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:29:01 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 11:30:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:30:07 INFO  time: compiled root in 0.33s[0m
[0m2021.03.08 11:30:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:30:14 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:30:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:30:39 INFO  time: compiled root in 0.32s[0m
[0m2021.03.08 11:30:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:30:43 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 11:30:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:30:57 INFO  time: compiled root in 0.3s[0m
[0m2021.03.08 11:31:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:31:05 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:31: stale bloop error: not enough arguments for method withColumn: (colName: String, col: org.apache.spark.sql.Column)org.apache.spark.sql.DataFrame.
Unspecified value parameter col.
> spark.read
>       .option("lineSep", "WARC/1.0")
>       .textFile(
>         "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz"
>       )
>       .map(str => str.substring(str.indexOf("\n") + 1))
>       .withColumnRenamed("value", "Header")
>       .withColumn($"Header")[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:31:13 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:31:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:31:19 INFO  time: compiled root in 3.82s[0m
[0m2021.03.08 11:31:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:31:44 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 11:31:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:31:47 INFO  time: compiled root in 0.31s[0m
[0m2021.03.08 11:31:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:31:59 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:32:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:02 INFO  time: compiled root in 0.33s[0m
[0m2021.03.08 11:32:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:07 INFO  time: compiled root in 0.3s[0m
[0m2021.03.08 11:32:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:13 INFO  time: compiled root in 0.17s[0m
[0m2021.03.08 11:32:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:17 INFO  time: compiled root in 0.41s[0m
[0m2021.03.08 11:32:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: illegal start of simple expression
      .withColumn("Text", split($"Header", ))
                                           ^[0m
[0m2021.03.08 11:32:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: illegal start of simple expression
      .withColumn("Text", split($"Header", ))
                                           ^[0m
[0m2021.03.08 11:32:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: illegal start of simple expression
      .withColumn("Text", split($"Header", ))
                                           ^[0m
[0m2021.03.08 11:32:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:25 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:44: stale bloop error: unclosed string literal
      .withColumn("Text", split($"Header", "\"))
                                           ^[0m
[0m2021.03.08 11:32:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:219:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 11:32:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:27 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:32:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:32:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:138:31: stale bloop error: not found: value commonCrawlJobs
    val commonCrawlTechJobs = commonCrawlJobs
                              ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 11:32:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:146:5: stale bloop error: not found: value display
    display()
    ^^^^^^^[0m
[0m2021.03.08 11:32:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:30 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:32:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:32:58 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:33:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:33:13 INFO  time: compiled root in 1.64s[0m
[0m2021.03.08 11:33:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:33:16 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 11:33:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:33:28 INFO  time: compiled root in 1.49s[0m
[0m2021.03.08 11:33:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:33:31 INFO  time: compiled root in 1.33s[0m
[0m2021.03.08 11:33:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:33:33 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 11:35:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:35:26 INFO  time: compiled root in 1.44s[0m
[0m2021.03.08 11:35:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:35:39 INFO  time: compiled root in 1.79s[0m
[0m2021.03.08 11:36:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:36:28 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 11:36:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:36:31 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 11:38:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:38:52 INFO  time: compiled root in 1.57s[0m
[0m2021.03.08 11:38:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:38:54 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 11:39:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:39:56 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 11:39:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:39:58 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 11:40:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:07 INFO  time: compiled root in 1.33s[0m
[0m2021.03.08 11:40:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:19 INFO  time: compiled root in 1.51s[0m
[0m2021.03.08 11:40:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:22 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 11:40:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:23 INFO  time: compiled root in 0.33s[0m
[0m2021.03.08 11:40:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:27 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 11:40:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:34 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 11:40:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:41 INFO  time: compiled root in 1.27s[0m
[0m2021.03.08 11:40:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:48 INFO  time: compiled root in 1.28s[0m
[0m2021.03.08 11:40:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:51 INFO  time: compiled root in 0.35s[0m
[0m2021.03.08 11:40:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:56 INFO  time: compiled root in 1.43s[0m
[0m2021.03.08 11:40:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:40:57 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 11:43:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:20 INFO  time: compiled root in 0.35s[0m
[0m2021.03.08 11:43:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:27 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:43:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:29 INFO  time: compiled root in 0.31s[0m
[0m2021.03.08 11:43:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:30 INFO  time: compiled root in 0.34s[0m
[0m2021.03.08 11:43:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:35 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 11:43:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:42 INFO  time: compiled root in 0.28s[0m
[0m2021.03.08 11:43:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:46 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:43:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:43:52 INFO  time: compiled root in 1.58s[0m
[0m2021.03.08 11:47:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:47:41 INFO  time: compiled root in 0.3s[0m
[0m2021.03.08 11:47:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:47:44 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:48:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:03 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 11:48:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:07 INFO  time: compiled root in 0.28s[0m
[0m2021.03.08 11:48:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:21 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 11:48:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:31 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:48:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:32 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:48:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:38 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 11:48:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:46 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:48:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:47 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:48:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:52 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:48:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:48:55 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:49:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:49:00 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 11:49:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:49:03 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:49:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:49:13 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 11:49:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:49:20 INFO  time: compiled root in 0.28s[0m
Mar 08, 2021 11:49:51 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3571
[0m2021.03.08 11:50:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:50:42 INFO  time: compiled root in 1.53s[0m
[0m2021.03.08 11:52:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:52:34 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 11:52:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:52:35 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 11:54:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:54:56 INFO  time: compiled root in 1.41s[0m
[0m2021.03.08 11:55:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:00 INFO  time: compiled root in 0.14s[0m
Mar 08, 2021 11:55:02 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3675
[0m2021.03.08 11:55:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:03 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 11:55:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:05 INFO  time: compiled root in 0.28s[0m
[0m2021.03.08 11:55:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:11 INFO  time: compiled root in 1.28s[0m
[0m2021.03.08 11:55:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:15 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 11:55:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:56 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 11:55:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:55:59 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 11:56:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:02 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 11:56:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:04 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 11:56:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:11 INFO  time: compiled root in 1.35s[0m
[0m2021.03.08 11:56:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:16 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 11:56:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:22 INFO  time: compiled root in 1.38s[0m
[0m2021.03.08 11:56:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:24 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 11:56:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:56:28 INFO  time: compiled root in 1.51s[0m
[0m2021.03.08 11:57:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:57:16 INFO  time: compiled root in 1.39s[0m
[0m2021.03.08 11:57:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:57:20 INFO  time: compiled root in 2s[0m
[0m2021.03.08 11:57:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:57:25 INFO  time: compiled root in 1.26s[0m
Mar 08, 2021 11:58:42 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 08, 2021 11:58:42 AM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.08 11:58:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:58:43 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:58:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:58:46 INFO  time: compiled root in 0.3s[0m
Mar 08, 2021 11:58:48 AM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 3967
[0m2021.03.08 11:59:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:59:06 INFO  time: compiled root in 0.3s[0m
[0m2021.03.08 11:59:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:59:11 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 11:59:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 11:59:14 INFO  time: compiled root in 1.57s[0m
[0m2021.03.08 12:00:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:00:34 INFO  time: compiled root in 1.27s[0m
[0m2021.03.08 12:00:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:00:35 INFO  time: compiled root in 1.27s[0m
[0m2021.03.08 12:00:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:00:36 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 12:01:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:01:15 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 12:01:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:01:24 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 12:01:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:01:28 INFO  time: compiled root in 1.33s[0m
[0m2021.03.08 12:02:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:02:47 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 12:03:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:01 INFO  time: compiled root in 1.32s[0m
[0m2021.03.08 12:03:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:05 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 12:03:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:09 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 12:03:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:11 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 12:03:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:13 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 12:03:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:17 INFO  time: compiled root in 1.35s[0m
[0m2021.03.08 12:03:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:22 INFO  time: compiled root in 1.35s[0m
[0m2021.03.08 12:03:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:28 INFO  time: compiled root in 1.26s[0m
[0m2021.03.08 12:03:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:36 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 12:03:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:03:46 INFO  time: compiled root in 1.42s[0m
[0m2021.03.08 12:06:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:06:14 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 12:06:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:18 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:136:29: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Header", split($"value"))
                            ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:06:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:06:21 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 12:06:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:06:26 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.03.08 12:06:26 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 12:06:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:06:28 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 12:07:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:07:52 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 12:07:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:54 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:55 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:56 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:137:27: stale bloop error: not enough arguments for method split: (str: org.apache.spark.sql.Column, pattern: String)org.apache.spark.sql.Column.
Unspecified value parameter pattern.
      .withColumn("Text", split($"value"))
                          ^^^^^^^^^^^^^^^[0m
[0m2021.03.08 12:07:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:07:59 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 12:08:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:08:03 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 12:08:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:08:07 INFO  time: compiled root in 1.27s[0m
[0m2021.03.08 12:08:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:08:12 INFO  time: compiled root in 0.17s[0m
[0m2021.03.08 12:08:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:08:15 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 12:08:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:08:19 INFO  time: compiled root in 1.47s[0m
[0m2021.03.08 12:10:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:10:01 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 12:10:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:10:10 INFO  time: compiled root in 1.5s[0m
[0m2021.03.08 12:10:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:10:10 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 12:10:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:10:16 INFO  time: compiled root in 1.33s[0m
[0m2021.03.08 12:11:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:11:49 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 12:11:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:11:56 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 12:12:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:12:01 INFO  time: compiled root in 1.55s[0m
[0m2021.03.08 12:13:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:13:08 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 12:13:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:13:12 INFO  time: compiled root in 1.53s[0m
[0m2021.03.08 12:14:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:14:36 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 12:14:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:14:52 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 12:15:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:05 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 12:15:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:07 INFO  time: compiled root in 1.48s[0m
Mar 08, 2021 12:15:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 4867
[0m2021.03.08 12:15:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:13 INFO  time: compiled root in 1.42s[0m
[0m2021.03.08 12:15:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:15 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 12:15:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:18 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 12:15:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:22 INFO  time: compiled root in 1.74s[0m
[0m2021.03.08 12:15:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:40 INFO  time: compiled root in 2.87s[0m
[0m2021.03.08 12:15:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:45 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 12:15:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:47 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 12:15:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:15:53 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 12:16:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:16:15 INFO  time: compiled root in 2.06s[0m
[0m2021.03.08 12:16:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:16:19 INFO  time: compiled root in 1.43s[0m
[0m2021.03.08 12:19:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:19:13 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 12:19:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:19:15 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 12:19:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:19:19 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 12:19:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:19:23 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 12:22:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:22:11 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 12:22:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:22:17 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 12:22:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:22:19 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 12:22:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:22:23 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 12:23:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:23:57 INFO  time: compiled root in 1.45s[0m
[0m2021.03.08 12:26:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:26:59 INFO  time: compiled root in 0.18s[0m
[0m2021.03.08 12:27:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:03 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 12:27:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:16 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 12:27:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:27 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 12:27:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:32 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 12:27:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:34 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 12:27:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:46 INFO  time: compiled root in 0.29s[0m
[0m2021.03.08 12:27:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:27:54 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 12:29:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:29:46 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 12:33:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:33:59 INFO  time: compiled root in 1.33s[0m
[0m2021.03.08 12:34:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:34:02 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 12:35:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:35:24 INFO  time: compiled root in 1.37s[0m
[0m2021.03.08 12:35:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:35:27 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 12:35:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:35:32 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 12:35:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:35:35 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 12:36:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:36:57 INFO  time: compiled root in 1.26s[0m
[0m2021.03.08 12:36:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:36:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:99: stale bloop error: invalid escape character
    val jobsRegex = "/jobs|/job-listing|/job-posting|indeed + \bWARC-Identified-Content-Language:\ \beng\b"
                                                                                                  ^[0m
[0m2021.03.08 12:36:59 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 12:36:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:99: stale bloop error: invalid escape character
    val jobsRegex = "/jobs|/job-listing|/job-posting|indeed + \bWARC-Identified-Content-Language:\ \beng\b"
                                                                                                  ^[0m
[0m2021.03.08 12:36:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:85:99: stale bloop error: invalid escape character
    val jobsRegex = "/jobs|/job-listing|/job-posting|indeed + \bWARC-Identified-Content-Language:\ \beng\b"
                                                                                                  ^[0m
[0m2021.03.08 12:37:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:37:02 INFO  time: compiled root in 1.49s[0m
[0m2021.03.08 12:39:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:39:03 INFO  time: compiled root in 1.49s[0m
[0m2021.03.08 12:39:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:39:07 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 12:39:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:39:12 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 12:40:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:40:49 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 12:41:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:41:14 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 12:47:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:47:37 INFO  time: compiled root in 1.51s[0m
[0m2021.03.08 12:47:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:47:38 INFO  time: compiled root in 1.41s[0m
[0m2021.03.08 12:47:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:47:40 INFO  time: compiled root in 1.34s[0m
[0m2021.03.08 12:47:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:47:42 INFO  time: compiled root in 1.25s[0m
Mar 08, 2021 12:49:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5563
[0m2021.03.08 12:52:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:52:13 INFO  time: compiled root in 0.28s[0m
[0m2021.03.08 12:52:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:52:22 INFO  time: compiled root in 1.36s[0m
Mar 08, 2021 12:53:00 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5625
Mar 08, 2021 12:53:11 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5636
[0m2021.03.08 12:53:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:53:26 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 12:53:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:53:27 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 12:53:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:53:31 INFO  time: compiled root in 1.49s[0m
[0m2021.03.08 12:53:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:53:46 INFO  time: compiled root in 1.84s[0m
Mar 08, 2021 12:54:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5684
Mar 08, 2021 12:55:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5693
[0m2021.03.08 12:56:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:56:02 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 12:56:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:56:18 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 12:56:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:56:24 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 12:57:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:57:46 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 12:57:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:57:48 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 12:57:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:57:51 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 12:57:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:57:57 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 12:57:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:57:59 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 12:58:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:58:02 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 12:58:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:58:08 INFO  time: compiled root in 2.14s[0m
[0m2021.03.08 12:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 12:58:33 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 12:58:36 INFO  compiling root (1 scala source)[0m
Mar 08, 2021 12:58:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5842
[0m2021.03.08 12:58:37 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 13:00:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:00:22 INFO  time: compiled root in 0.98s[0m
[0m2021.03.08 13:00:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:00:36 INFO  time: compiled root in 1.01s[0m
Mar 08, 2021 1:00:46 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 5907
[0m2021.03.08 13:01:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:01:48 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:01:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:01:51 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:01:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:01:53 INFO  time: compiled root in 0.95s[0m
[0m2021.03.08 13:01:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:01:54 INFO  time: compiled root in 0.95s[0m
[0m2021.03.08 13:01:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:01:57 INFO  time: compiled root in 0.98s[0m
[0m2021.03.08 13:02:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:02:01 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 13:04:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:03 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:04:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:07 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 13:04:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:42 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 13:04:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:45 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 13:04:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:48 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:04:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:50 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:04:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:51 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:04:54 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 13:05:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:05:01 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:05:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:05:02 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:05:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:05:05 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:05:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:05:49 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:05:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:05:51 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:06:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:06:05 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:06:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:06:09 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:06:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:06:34 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:06:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:06:39 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:06:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:06:46 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 13:07:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:12 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:07:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:24 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:07:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:27 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:07:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:28 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:07:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:07:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:07:31 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:08:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:08:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:08:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:50 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:50 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:08:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:08:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:08:56 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 13:09:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:09:47 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 13:09:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:09:50 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 13:09:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:09:59 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:09:59 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:10:00 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:10:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: ')' expected but string literal found.
      .filter(lower($"Header") rlike jobsRegex and $"Header" not rlike ",")
                                                                       ^[0m
[0m2021.03.08 13:10:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:10:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:25 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:56: stale bloop error: invalid string interpolation: `$$', `$'ident or `$'BlockExpr expected
      .filter(lower($"Header") rlike jobsRegex and $"""Header" not rlike ",")
                                                       ^[0m
[0m2021.03.08 13:10:28 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:145:67: stale bloop error: ';' expected but ')' found.
      .filter($"Job Posting Websites" rlike techJobs.mkString("|"))
                                                                  ^[0m
[0m2021.03.08 13:10:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:10:29 INFO  time: compiled root in 0.98s[0m
[0m2021.03.08 13:10:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:10:35 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:10:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:10:46 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 13:12:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:36 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 13:12:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:76: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$""Header not rlike ",""""")
                                                                           ^[0m
[0m2021.03.08 13:12:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:76: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$""Header not rlike ",""""")
                                                                           ^[0m
[0m2021.03.08 13:12:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:38 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 13:12:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:76: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$""Header not rlike ",""""")
                                                                           ^[0m
[0m2021.03.08 13:12:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:76: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$""Header not rlike ",""""")
                                                                           ^[0m
[0m2021.03.08 13:12:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:76: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$""Header not rlike ",""""")
                                                                           ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:74: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header not rlike ",""""")
                                                                         ^[0m
[0m2021.03.08 13:12:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:70: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header rlike ",""""")
                                                                     ^[0m
[0m2021.03.08 13:12:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:70: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header rlike ",""""")
                                                                     ^[0m
[0m2021.03.08 13:12:41 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:41 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 13:12:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:70: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header rlike ",""""")
                                                                     ^[0m
[0m2021.03.08 13:12:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:70: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header rlike ",""""")
                                                                     ^[0m
[0m2021.03.08 13:12:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:70: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header rlike ",""""")
                                                                     ^[0m
[0m2021.03.08 13:12:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: illegal start of simple expression
  */
    ^[0m
[0m2021.03.08 13:12:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:44 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:140:72: stale bloop error: unclosed multi-line string literal
      .filter(lower($"Header") rlike jobsRegex and !$"Header" rlike ",""""")
                                                                       ^[0m
[0m2021.03.08 13:12:46 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:369:5: stale bloop error: ')' expected but eof found.
  */
    ^[0m
[0m2021.03.08 13:12:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:12:48 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:13:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:13:00 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:13:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:13:02 INFO  time: compiled root in 0.17s[0m
[0m2021.03.08 13:13:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:13:10 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:14:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:20 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 13:14:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:25 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:14:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:30 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 13:14:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:31 INFO  time: compiled root in 0.28s[0m
[0m2021.03.08 13:14:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:37 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:14:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:43 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 13:14:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:14:47 INFO  time: compiled root in 1.28s[0m
[0m2021.03.08 13:16:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:16:05 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:16:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:16:07 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 13:16:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:16:09 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:16:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:16:23 INFO  time: compiled root in 2.03s[0m
Mar 08, 2021 1:17:36 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6739
Mar 08, 2021 1:18:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 6745
[0m2021.03.08 13:18:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:18:34 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 13:18:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:18:46 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:18:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:18:49 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 13:18:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:18:53 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:18:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:18:54 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 13:19:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:19:03 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:20:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:20:52 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 13:21:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:00 INFO  time: compiled root in 0.21s[0m
[0m2021.03.08 13:21:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:03 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 13:21:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:23 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 13:21:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:30 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:21:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:34 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:21:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:37 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:21:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:56 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 13:21:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:21:59 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 13:22:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:22:20 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:22:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:22:23 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:22:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:22:49 INFO  time: compiled root in 0.21s[0m
[0m2021.03.08 13:22:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:22:53 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:22:56 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:22:56 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:23:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:23:00 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:23:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:23:07 INFO  time: compiled root in 0.2s[0m
[0m2021.03.08 13:23:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:23:16 INFO  time: compiled root in 1s[0m
[0m2021.03.08 13:23:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:23:24 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:23:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:23:28 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 13:25:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:25:35 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:25:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:25:45 INFO  time: compiled root in 1.04s[0m
[0m2021.03.08 13:25:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:25:52 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:26:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:26 INFO  time: compiled root in 1s[0m
[0m2021.03.08 13:26:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:34 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 13:26:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:36 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:26:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:44 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:26:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:46 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:26:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:26:57 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:27:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:17 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:22 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs|,
      ^[0m
[0m2021.03.08 13:27:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs|,
      ^[0m
[0m2021.03.08 13:27:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:26 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs|,
      ^[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs|,
      ^[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs|,
      ^[0m
[0m2021.03.08 13:27:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:30 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:30 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:27:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:31 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:32 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:27:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs
      ^[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs
      ^[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:34 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs
      ^[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs
      ^[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:35 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      "/jobs
      ^[0m
[0m2021.03.08 13:27:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:36 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:42 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:27:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:14: stale bloop error: ')' expected but string literal found.
      "/jobs"",job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
             ^[0m
[0m2021.03.08 13:27:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:14: stale bloop error: ')' expected but string literal found.
      "/jobs"",job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
             ^[0m
[0m2021.03.08 13:27:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:14: stale bloop error: ')' expected but string literal found.
      "/jobs"",job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
             ^[0m
[0m2021.03.08 13:27:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:14: stale bloop error: ')' expected but string literal found.
      "/jobs"",job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
             ^[0m
[0m2021.03.08 13:27:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:14: stale bloop error: ')' expected but string literal found.
      "/jobs"",job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
             ^[0m
[0m2021.03.08 13:27:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:45 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:45 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:27:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: illegal start of simple expression
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
      ^[0m
[0m2021.03.08 13:27:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: illegal start of simple expression
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
      ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: illegal start of simple expression
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
      ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:80: stale bloop error: unclosed string literal
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                               ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: illegal start of simple expression
      ,job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
      ^[0m
[0m2021.03.08 13:27:51 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:228:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:27:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:52 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:27:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:52 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:53 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:79: stale bloop error: unclosed string literal
      job-listing|/job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                              ^[0m
[0m2021.03.08 13:27:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:53 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:27:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:27:59 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:28:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:01 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:00 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:02 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:02 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:11 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:28:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:15 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:67: stale bloop error: unclosed string literal
      /job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                  ^[0m
[0m2021.03.08 13:28:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:66: stale bloop error: unclosed string literal
      job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                 ^[0m
[0m2021.03.08 13:28:16 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:66: stale bloop error: unclosed string literal
      job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                 ^[0m
[0m2021.03.08 13:28:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:66: stale bloop error: unclosed string literal
      job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                 ^[0m
[0m2021.03.08 13:28:16 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:66: stale bloop error: unclosed string literal
      job-posting|indeed.com/|careers|glassdoor.com/|/employment"
                                                                 ^[0m
[0m2021.03.08 13:28:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:17 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:65: stale bloop error: unclosed string literal
      job-postingindeed.com/|careers|glassdoor.com/|/employment"
                                                                ^[0m
[0m2021.03.08 13:28:17 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:28:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:65: stale bloop error: unclosed string literal
      job-postingindeed.com/|careers|glassdoor.com/|/employment"
                                                                ^[0m
[0m2021.03.08 13:28:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:65: stale bloop error: unclosed string literal
      job-postingindeed.com/|careers|glassdoor.com/|/employment"
                                                                ^[0m
[0m2021.03.08 13:28:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:65: stale bloop error: unclosed string literal
      job-postingindeed.com/|careers|glassdoor.com/|/employment"
                                                                ^[0m
[0m2021.03.08 13:28:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:64: stale bloop error: unclosed string literal
      job-postingindeed.com/careers|glassdoor.com/|/employment"
                                                               ^[0m
[0m2021.03.08 13:28:22 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:64: stale bloop error: unclosed string literal
      job-postingindeed.com/careers|glassdoor.com/|/employment"
                                                               ^[0m
[0m2021.03.08 13:28:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:64: stale bloop error: unclosed string literal
      job-postingindeed.com/careers|glassdoor.com/|/employment"
                                                               ^[0m
[0m2021.03.08 13:28:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:64: stale bloop error: unclosed string literal
      job-postingindeed.com/careers|glassdoor.com/|/employment"
                                                               ^[0m
[0m2021.03.08 13:28:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:23 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:63: stale bloop error: unclosed string literal
      job-postingindeed.com/careersglassdoor.com/|/employment"
                                                              ^[0m
[0m2021.03.08 13:28:23 INFO  time: compiled root in 0.15s[0m
[0m2021.03.08 13:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:63: stale bloop error: unclosed string literal
      job-postingindeed.com/careersglassdoor.com/|/employment"
                                                              ^[0m
[0m2021.03.08 13:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:63: stale bloop error: unclosed string literal
      job-postingindeed.com/careersglassdoor.com/|/employment"
                                                              ^[0m
[0m2021.03.08 13:28:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:63: stale bloop error: unclosed string literal
      job-postingindeed.com/careersglassdoor.com/|/employment"
                                                              ^[0m
[0m2021.03.08 13:28:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:25 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:31 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:28:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:34 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:35 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:28:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:5: stale bloop error: ')' expected but 'val' found.
    val techJobs = List(
    ^[0m
[0m2021.03.08 13:28:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: unclosed string literal
      /employment"
                  ^[0m
[0m2021.03.08 13:28:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: unclosed string literal
      /employment"
                  ^[0m
[0m2021.03.08 13:28:40 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:8: stale bloop error: ')' expected but string literal found.
      /employment"
       ^[0m
[0m2021.03.08 13:28:40 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 13:28:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: unclosed string literal
      /employment"
                  ^[0m
[0m2021.03.08 13:28:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:8: stale bloop error: ')' expected but string literal found.
      /employment"
       ^[0m
[0m2021.03.08 13:28:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: unclosed string literal
      /employment"
                  ^[0m
[0m2021.03.08 13:28:43 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:8: stale bloop error: ')' expected but string literal found.
      /employment"
       ^[0m
[0m2021.03.08 13:28:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:19: stale bloop error: unclosed string literal
      /employment"
                  ^[0m
[0m2021.03.08 13:28:44 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:8: stale bloop error: ')' expected but string literal found.
      /employment"
       ^[0m
[0m2021.03.08 13:28:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:44 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:28:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:53 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 13:28:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:28:57 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:29:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:01 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:29:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:05 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 13:29:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:13 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:29:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:16 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:29:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:24 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:26 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:25 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:149:38: stale bloop error: type mismatch;
 found   : List[String]
 required: String
      .filter(lower($"Header") rlike jobsRegex)
                                     ^^^^^^^^^[0m
[0m2021.03.08 13:29:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:28 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:29:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:29:34 INFO  time: compiled root in 1.48s[0m
[0m2021.03.08 13:30:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:01 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 13:30:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:07 INFO  time: compiled root in 1.51s[0m
[0m2021.03.08 13:30:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:15 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:30:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:19 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:30:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:27 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:30:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:31 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:30:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:33 INFO  time: compiled root in 0.95s[0m
[0m2021.03.08 13:30:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:37 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:30:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:30:52 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:31:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:31:14 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:31:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:31:15 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:31:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:31:17 INFO  time: compiled root in 1s[0m
[0m2021.03.08 13:31:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:31:20 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 13:31:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:31:21 INFO  time: compiled root in 1.01s[0m
Mar 08, 2021 1:32:47 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7986
Mar 08, 2021 1:32:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 7993
[0m2021.03.08 13:33:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:33:31 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:33:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:33:34 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 13:33:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:33:36 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:34:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:12 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 13:34:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:27 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:34:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:30 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:34:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:32 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:34:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:36 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:34:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:39 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:34:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:34:42 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 13:36:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:36:21 INFO  time: compiled root in 1.03s[0m
Mar 08, 2021 1:37:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8117
[0m2021.03.08 13:37:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:37:28 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 13:39:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:39:30 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 13:39:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:39:36 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 13:39:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:39:42 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:39:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:39:45 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:39:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:39:49 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 13:42:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:42:10 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 13:42:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:42:12 INFO  time: compiled root in 1s[0m
[0m2021.03.08 13:42:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:42:15 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 13:42:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:42:19 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:42:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:42:22 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 13:43:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:14 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 13:43:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:34 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:34 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.
      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:234:3: stale bloop error: ')' expected but '}' found.
  }
  ^[0m
[0m2021.03.08 13:43:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.*""
                                        ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.*""
                                        ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.*""
                                      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.*""
                                    ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.*""
                                   ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.*""
                                    ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:42: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.*""
                                         ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:38: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.*""
                                     ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.*""
                                      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:46: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.*""
                                             ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:50: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.*""
                                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.*""
                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.*""
                                       ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.*""
                                        ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.*""
                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.*""
                                      ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.*""
                                           ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:43: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.*""
                                          ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.*""
                                        ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.*""
                                           ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.*""
                                   ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.*""
                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.*""
                                    ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.*""
                                       ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.*""
                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.*""
                                 ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.*""
                                       ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.*""
                                  ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.*""
                                    ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.*""
                                   ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.*""
                                   ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:36 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 13:43:36 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:42: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.*""
                                         ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:38: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.*""
                                     ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:46: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.*""
                                             ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:50: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.*""
                                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.*""
                                           ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:43: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.*""
                                          ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.*""
                                           ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:42: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.*""
                                         ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:38: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.*""
                                     ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:46: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.*""
                                             ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:50: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.*""
                                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.*""
                                      ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.*""
                                           ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:43: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.*""
                                          ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.*""
                                        ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.*""
                                           ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.*""
                                 ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.*""
                                       ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.*""
                                  ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.*""
                                    ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.*""
                                   ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:38 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:95:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*technology.*""
                                        ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:96:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computer.*""
                                      ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:97:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*java.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:98:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*python.*""
                                    ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:99:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*scala.*""
                                   ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:100:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*code.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:101:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*coding.*""
                                    ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:102:42: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*programming.*""
                                         ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:103:38: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*backend.*""
                                     ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:104:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*frontend.*""
                                      ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:105:46: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*web-development.*""
                                             ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:106:50: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*website-development.*""
                                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:107:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*ruby.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:108:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sql.*""
                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:109:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*html.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:110:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*fullstack.*""
                                       ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:111:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*full-stack.*""
                                        ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:112:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*css.*""
                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:113:39: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*software.*""
                                      ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:114:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cybersecurity.*""
                                           ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:115:43: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*cryptography.*""
                                          ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:116:41: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-support.*""
                                        ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:117:44: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*it-specialist.*""
                                           ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:118:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*spark.*""
                                   ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:119:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hive.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:120:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hql.*""
                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:121:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hadoop.*""
                                    ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:122:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mapreduce.*""
                                       ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:123:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*hdfs.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:124:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:125:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*sdk.*""
                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:126:34: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*aws.*""
                                 ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:127:40: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*computing.*""
                                       ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:128:35: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*data.*""
                                  ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:129:37: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*apache.*""
                                    ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:130:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*kafka.*""
                                   ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:131:36: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*mongo.*""
                                   ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:132:33: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*c#.*""
                                ^[0m
[0m2021.03.08 13:43:39 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:143:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 13:43:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 13:43:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:51 INFO  time: compiled root in 1.32s[0m
[0m2021.03.08 13:43:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:43:56 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 13:44:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:44:58 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 13:45:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:03 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 13:45:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:04 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 13:45:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:10 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:45:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:17 INFO  time: compiled root in 1.04s[0m
[0m2021.03.08 13:45:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:22 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 13:45:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:26 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 13:45:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:30 INFO  time: compiled root in 1.88s[0m
[0m2021.03.08 13:45:30 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:45:33 INFO  time: compiled root in 2.59s[0m
[0m2021.03.08 13:46:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:46:20 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:46:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:46:47 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:46:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:46:52 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 13:47:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:47:11 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:47:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:47:18 INFO  time: compiled root in 1.16s[0m
Mar 08, 2021 1:48:31 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8641
[0m2021.03.08 13:49:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:50:00 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:50:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:50:04 INFO  time: compiled root in 1.04s[0m
[0m2021.03.08 13:50:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:50:14 INFO  time: compiled root in 0.97s[0m
[0m2021.03.08 13:50:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:50:20 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:50:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:50:30 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:52:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:52:16 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 13:52:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:52:19 INFO  time: compiled root in 0.99s[0m
[0m2021.03.08 13:52:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:52:38 INFO  time: compiled root in 1.04s[0m
[0m2021.03.08 13:53:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:53:02 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 13:53:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:53:08 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 13:53:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:53:19 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 13:53:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:53:27 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 13:55:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:55:38 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 13:56:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:56:51 INFO  time: compiled root in 1.35s[0m
[0m2021.03.08 13:56:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:56:53 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:56:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:56:59 INFO  time: compiled root in 1.05s[0m
Mar 08, 2021 1:56:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 8909
[0m2021.03.08 13:57:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:57:42 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 13:57:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:57:44 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 13:58:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:00 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 13:58:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:06 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 13:58:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:17 WARN  there was one deprecation warning; re-run with -deprecation for details[0m
[0m2021.03.08 13:58:17 INFO  time: compiled root in 1.45s[0m
[0m2021.03.08 13:58:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:25 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:58:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:27 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 13:58:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:34 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 13:58:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 13:58:44 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 14:00:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:00:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:21: stale bloop error: unclosed string literal
      glassdoor.com/",
                    ^[0m
[0m2021.03.08 14:00:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:21: stale bloop error: unclosed string literal
      glassdoor.com/",
                    ^[0m
[0m2021.03.08 14:00:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:16: stale bloop error: ')' expected but '.' found.
      glassdoor.com/",
               ^[0m
[0m2021.03.08 14:00:47 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:144:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 14:00:47 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:21: stale bloop error: unclosed string literal
      glassdoor.com/",
                    ^[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:16: stale bloop error: ')' expected but '.' found.
      glassdoor.com/",
               ^[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:144:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:21: stale bloop error: unclosed string literal
      glassdoor.com/",
                    ^[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:16: stale bloop error: ')' expected but '.' found.
      glassdoor.com/",
               ^[0m
[0m2021.03.08 14:00:48 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:144:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 14:00:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:21: stale bloop error: unclosed string literal
      glassdoor.com/",
                    ^[0m
[0m2021.03.08 14:00:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:16: stale bloop error: ')' expected but '.' found.
      glassdoor.com/",
               ^[0m
[0m2021.03.08 14:00:49 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:144:55: stale bloop error: ';' expected but ')' found.
      .map(str => str.substring(str.indexOf("\n") + 1))
                                                      ^[0m
[0m2021.03.08 14:00:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:00:49 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:00:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:00:55 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:00:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:01:01 INFO  time: compiled root in 1.27s[0m
[0m2021.03.08 14:03:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:01 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 14:03:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:04 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 14:03:06 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:06 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 14:03:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:20 INFO  time: compiled root in 1.7s[0m
[0m2021.03.08 14:03:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:23 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 14:03:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:03:26 INFO  time: compiled root in 1s[0m
[0m2021.03.08 14:04:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:04:43 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 14:04:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:04:48 INFO  time: compiled root in 2.26s[0m
[0m2021.03.08 14:04:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:04:55 INFO  time: compiled root in 1.01s[0m
Mar 08, 2021 2:04:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9272
Mar 08, 2021 2:04:55 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9273
[0m2021.03.08 14:04:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:04:59 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 14:05:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:05:55 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:05:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:05:58 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 14:06:33 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:06:33 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:06:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:06:36 INFO  time: compiled root in 0.11s[0m
Mar 08, 2021 2:06:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9345
[0m2021.03.08 14:06:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:06:39 INFO  time: compiled root in 1s[0m
[0m2021.03.08 14:08:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:08:13 INFO  time: compiled root in 1.01s[0m
[0m2021.03.08 14:08:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:08:16 INFO  time: compiled root in 1.03s[0m
[0m2021.03.08 14:10:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:10:43 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 14:13:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:13:01 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 14:13:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:13:41 INFO  time: compiled root in 1.16s[0m
Mar 08, 2021 2:13:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9446
[0m2021.03.08 14:13:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:13:54 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 14:16:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:04 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 14:16:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:11 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 14:16:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:13 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 14:16:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:16 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 14:16:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:47 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 14:16:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:16:58 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:17:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:02 INFO  time: compiled root in 0.18s[0m
[0m2021.03.08 14:17:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:04 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:17:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:07 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:17:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:11 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:17:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:14 INFO  time: compiled root in 0.1s[0m
[0m2021.03.08 14:17:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:15 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:17:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:17:18 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 14:19:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:19:45 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 14:19:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:19:45 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 14:19:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:19:50 INFO  time: compiled root in 1.33s[0m
Mar 08, 2021 2:19:50 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 9661
[0m2021.03.08 14:19:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:19:53 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 14:19:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:19:55 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 14:21:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:08 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:08 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:21:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:10 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:11 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:11 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:21:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:21 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*jobs.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*jobs.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-listing.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-posting.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*indeed.com/.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*careers.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*glassdoor.com/.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:22 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*jobs.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-listing.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-posting.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*indeed.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*careers.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*glassdoor.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*jobs.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-listing.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-posting.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*indeed.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*careers.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*glassdoor.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:86:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*jobs.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:87:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-listing.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:88:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*job-posting.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:89:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*indeed.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:90:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*careers.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:91:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*glassdoor.com/.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala:92:7: stale bloop error: unclosed string literal
      ".*WARC-Target-URI:.*employment.*
      ^[0m
[0m2021.03.08 14:21:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:29 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:21:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:36 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 14:21:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:39 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:21:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:51 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:21:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:21:53 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 14:22:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:09 INFO  time: compiled root in 0.15s[0m
[0m2021.03.08 14:22:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:15 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:22:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:18 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 14:22:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:22 INFO  time: compiled root in 0.12s[0m
[0m2021.03.08 14:22:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:25 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 14:22:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:28 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:22:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:32 INFO  time: compiled root in 0.13s[0m
[0m2021.03.08 14:22:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:35 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:22:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:22:43 INFO  time: compiled root in 1.26s[0m
[0m2021.03.08 14:23:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:23:07 INFO  time: compiled root in 1.97s[0m
[0m2021.03.08 14:23:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:23:23 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 14:25:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:25:55 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 14:25:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:25:59 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 14:26:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:04 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 14:26:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:05 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 14:26:05 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:07 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 14:26:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:14 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 14:26:16 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:17 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 14:26:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:19 INFO  time: compiled root in 1.02s[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import java.io.CharArrayWriter

import scala.collection.JavaConverters._
import scala.language.implicitConversions
import scala.reflect.runtime.universe.TypeTag
import scala.util.control.NonFatal

import org.apache.commons.lang3.StringUtils

import org.apache.spark.TaskContext
import org.apache.spark.annotation.{DeveloperApi, Experimental, InterfaceStability}
import org.apache.spark.api.java.JavaRDD
import org.apache.spark.api.java.function._
import org.apache.spark.api.python.{PythonRDD, SerDeUtil}
import org.apache.spark.broadcast.Broadcast
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.catalyst._
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.catalog.HiveTableRelation
import org.apache.spark.sql.catalyst.encoders._
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection
import org.apache.spark.sql.catalyst.json.{JacksonGenerator, JSONOptions}
import org.apache.spark.sql.catalyst.optimizer.CombineUnions
import org.apache.spark.sql.catalyst.parser.{ParseException, ParserUtils}
import org.apache.spark.sql.catalyst.plans._
import org.apache.spark.sql.catalyst.plans.logical._
import org.apache.spark.sql.catalyst.plans.physical.{Partitioning, PartitioningCollection}
import org.apache.spark.sql.execution._
import org.apache.spark.sql.execution.arrow.{ArrowBatchStreamWriter, ArrowConverters}
import org.apache.spark.sql.execution.command._
import org.apache.spark.sql.execution.datasources.LogicalRelation
import org.apache.spark.sql.execution.python.EvaluatePython
import org.apache.spark.sql.execution.stat.StatFunctions
import org.apache.spark.sql.streaming.DataStreamWriter
import org.apache.spark.sql.types._
import org.apache.spark.sql.util.SchemaUtils
import org.apache.spark.storage.StorageLevel
import org.apache.spark.unsafe.array.ByteArrayMethods
import org.apache.spark.unsafe.types.CalendarInterval
import org.apache.spark.util.Utils

private[sql] object Dataset {
  def apply[T: Encoder](sparkSession: SparkSession, logicalPlan: LogicalPlan): Dataset[T] = {
    val dataset = new Dataset(sparkSession, logicalPlan, implicitly[Encoder[T]])
    // Eagerly bind the encoder so we verify that the encoder matches the underlying
    // schema. The user will get an error if this is not the case.
    // optimization: it is guaranteed that [[InternalRow]] can be converted to [[Row]] so
    // do not do this check in that case. this check can be expensive since it requires running
    // the whole [[Analyzer]] to resolve the deserializer
    if (dataset.exprEnc.clsTag.runtimeClass != classOf[Row]) {
      dataset.deserializer
    }
    dataset
  }

  def ofRows(sparkSession: SparkSession, logicalPlan: LogicalPlan): DataFrame = {
    val qe = sparkSession.sessionState.executePlan(logicalPlan)
    qe.assertAnalyzed()
    new Dataset[Row](sparkSession, qe, RowEncoder(qe.analyzed.schema))
  }
}

/**
 * A Dataset is a strongly typed collection of domain-specific objects that can be transformed
 * in parallel using functional or relational operations. Each Dataset also has an untyped view
 * called a `DataFrame`, which is a Dataset of [[Row]].
 *
 * Operations available on Datasets are divided into transformations and actions. Transformations
 * are the ones that produce new Datasets, and actions are the ones that trigger computation and
 * return results. Example transformations include map, filter, select, and aggregate (`groupBy`).
 * Example actions count, show, or writing data out to file systems.
 *
 * Datasets are "lazy", i.e. computations are only triggered when an action is invoked. Internally,
 * a Dataset represents a logical plan that describes the computation required to produce the data.
 * When an action is invoked, Spark's query optimizer optimizes the logical plan and generates a
 * physical plan for efficient execution in a parallel and distributed manner. To explore the
 * logical plan as well as optimized physical plan, use the `explain` function.
 *
 * To efficiently support domain-specific objects, an [[Encoder]] is required. The encoder maps
 * the domain specific type `T` to Spark's internal type system. For example, given a class `Person`
 * with two fields, `name` (string) and `age` (int), an encoder is used to tell Spark to generate
 * code at runtime to serialize the `Person` object into a binary structure. This binary structure
 * often has much lower memory footprint as well as are optimized for efficiency in data processing
 * (e.g. in a columnar format). To understand the internal binary representation for data, use the
 * `schema` function.
 *
 * There are typically two ways to create a Dataset. The most common way is by pointing Spark
 * to some files on storage systems, using the `read` function available on a `SparkSession`.
 * {{{
 *   val people = spark.read.parquet("...").as[Person]  // Scala
 *   Dataset<Person> people = spark.read().parquet("...").as(Encoders.bean(Person.class)); // Java
 * }}}
 *
 * Datasets can also be created through transformations available on existing Datasets. For example,
 * the following creates a new Dataset by applying a filter on the existing one:
 * {{{
 *   val names = people.map(_.name)  // in Scala; names is a Dataset[String]
 *   Dataset<String> names = people.map((Person p) -> p.name, Encoders.STRING));
 * }}}
 *
 * Dataset operations can also be untyped, through various domain-specific-language (DSL)
 * functions defined in: Dataset (this class), [[Column]], and [[functions]]. These operations
 * are very similar to the operations available in the data frame abstraction in R or Python.
 *
 * To select a column from the Dataset, use `apply` method in Scala and `col` in Java.
 * {{{
 *   val ageCol = people("age")  // in Scala
 *   Column ageCol = people.col("age"); // in Java
 * }}}
 *
 * Note that the [[Column]] type can also be manipulated through its various functions.
 * {{{
 *   // The following creates a new column that increases everybody's age by 10.
 *   people("age") + 10  // in Scala
 *   people.col("age").plus(10);  // in Java
 * }}}
 *
 * A more concrete example in Scala:
 * {{{
 *   // To create Dataset[Row] using SparkSession
 *   val people = spark.read.parquet("...")
 *   val department = spark.read.parquet("...")
 *
 *   people.filter("age > 30")
 *     .join(department, people("deptId") === department("id"))
 *     .groupBy(department("name"), people("gender"))
 *     .agg(avg(people("salary")), max(people("age")))
 * }}}
 *
 * and in Java:
 * {{{
 *   // To create Dataset<Row> using SparkSession
 *   Dataset<Row> people = spark.read().parquet("...");
 *   Dataset<Row> department = spark.read().parquet("...");
 *
 *   people.filter(people.col("age").gt(30))
 *     .join(department, people.col("deptId").equalTo(department.col("id")))
 *     .groupBy(department.col("name"), people.col("gender"))
 *     .agg(avg(people.col("salary")), max(people.col("age")));
 * }}}
 *
 * @groupname basic Basic Dataset functions
 * @groupname action Actions
 * @groupname untypedrel Untyped transformations
 * @groupname typedrel Typed transformations
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class Dataset[T] private[sql](
    @transient val sparkSession: SparkSession,
    @DeveloperApi @InterfaceStability.Unstable @transient val queryExecution: QueryExecution,
    encoder: Encoder[T])
  extends Serializable {

  queryExecution.assertAnalyzed()

  // Note for Spark contributors: if adding or updating any action in `Dataset`, please make sure
  // you wrap it with `withNewExecutionId` if this actions doesn't call other action.

  def this(sparkSession: SparkSession, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sparkSession, sparkSession.sessionState.executePlan(logicalPlan), encoder)
  }

  def this(sqlContext: SQLContext, logicalPlan: LogicalPlan, encoder: Encoder[T]) = {
    this(sqlContext.sparkSession, logicalPlan, encoder)
  }

  @transient private[sql] val logicalPlan: LogicalPlan = {
    // For various commands (like DDL) and queries with side effects, we force query execution
    // to happen right away to let these side effects take place eagerly.
    queryExecution.analyzed match {
      case c: Command =>
        LocalRelation(c.output, withAction("command", queryExecution)(_.executeCollect()))
      case u @ Union(children) if children.forall(_.isInstanceOf[Command]) =>
        LocalRelation(u.output, withAction("command", queryExecution)(_.executeCollect()))
      case _ =>
        queryExecution.analyzed
    }
  }

  /**
   * Currently [[ExpressionEncoder]] is the only implementation of [[Encoder]], here we turn the
   * passed in encoder to [[ExpressionEncoder]] explicitly, and mark it implicit so that we can use
   * it when constructing new Dataset objects that have the same object type (that will be
   * possibly resolved to a different schema).
   */
  private[sql] implicit val exprEnc: ExpressionEncoder[T] = encoderFor(encoder)

  // The deserializer expression which can be used to build a projection and turn rows to objects
  // of type T, after collecting rows to the driver side.
  private lazy val deserializer =
    exprEnc.resolveAndBind(logicalPlan.output, sparkSession.sessionState.analyzer).deserializer

  private implicit def classTag = exprEnc.clsTag

  // sqlContext must be val because a stable identifier is expected when you import implicits
  @transient lazy val sqlContext: SQLContext = sparkSession.sqlContext

  private[sql] def resolve(colName: String): NamedExpression = {
    queryExecution.analyzed.resolveQuoted(colName, sparkSession.sessionState.analyzer.resolver)
      .getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
  }

  private[sql] def numericColumns: Seq[Expression] = {
    schema.fields.filter(_.dataType.isInstanceOf[NumericType]).map { n =>
      queryExecution.analyzed.resolveQuoted(n.name, sparkSession.sessionState.analyzer.resolver).get
    }
  }

  /**
   * Get rows represented in Sequence by specific truncate and vertical requirement.
   *
   * @param numRows Number of rows to return
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   */
  private[sql] def getRows(
      numRows: Int,
      truncate: Int): Seq[Seq[String]] = {
    val newDf = toDF()
    val castCols = newDf.logicalPlan.output.map { col =>
      // Since binary types in top-level schema fields have a specific format to print,
      // so we do not cast them to strings here.
      if (col.dataType == BinaryType) {
        Column(col)
      } else {
        Column(col).cast(StringType)
      }
    }
    val data = newDf.select(castCols: _*).take(numRows + 1)

    // For array values, replace Seq and Array with square brackets
    // For cells that are beyond `truncate` characters, replace it with the
    // first `truncate-3` and "..."
    schema.fieldNames.toSeq +: data.map { row =>
      row.toSeq.map { cell =>
        val str = cell match {
          case null => "null"
          case binary: Array[Byte] => binary.map("%02X".format(_)).mkString("[", " ", "]")
          case _ => cell.toString
        }
        if (truncate > 0 && str.length > truncate) {
          // do not show ellipses for strings shorter than 4 characters.
          if (truncate < 4) str.substring(0, truncate)
          else str.substring(0, truncate - 3) + "..."
        } else {
          str
        }
      }: Seq[String]
    }
  }

  /**
   * Compose the string representing rows for output
   *
   * @param _numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                   all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   */
  private[sql] def showString(
      _numRows: Int,
      truncate: Int = 20,
      vertical: Boolean = false): String = {
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    // Get rows represented by Seq[Seq[String]], we may get one more line if it has more data.
    val tmpRows = getRows(numRows, truncate)

    val hasMoreData = tmpRows.length - 1 > numRows
    val rows = tmpRows.take(numRows + 1)

    val sb = new StringBuilder
    val numCols = schema.fieldNames.length
    // We set a minimum column width at '3'
    val minimumColWidth = 3

    if (!vertical) {
      // Initialise the width of each column to a minimum value
      val colWidths = Array.fill(numCols)(minimumColWidth)

      // Compute the width of each column
      for (row <- rows) {
        for ((cell, i) <- row.zipWithIndex) {
          colWidths(i) = math.max(colWidths(i), Utils.stringHalfWidth(cell))
        }
      }

      val paddedRows = rows.map { row =>
        row.zipWithIndex.map { case (cell, i) =>
          if (truncate > 0) {
            StringUtils.leftPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          } else {
            StringUtils.rightPad(cell, colWidths(i) - Utils.stringHalfWidth(cell) + cell.length)
          }
        }
      }

      // Create SeparateLine
      val sep: String = colWidths.map("-" * _).addString(sb, "+", "+", "+\n").toString()

      // column names
      paddedRows.head.addString(sb, "|", "|", "|\n")
      sb.append(sep)

      // data
      paddedRows.tail.foreach(_.addString(sb, "|", "|", "|\n"))
      sb.append(sep)
    } else {
      // Extended display mode enabled
      val fieldNames = rows.head
      val dataRows = rows.tail

      // Compute the width of field name and data columns
      val fieldNameColWidth = fieldNames.foldLeft(minimumColWidth) { case (curMax, fieldName) =>
        math.max(curMax, Utils.stringHalfWidth(fieldName))
      }
      val dataColWidth = dataRows.foldLeft(minimumColWidth) { case (curMax, row) =>
        math.max(curMax, row.map(cell => Utils.stringHalfWidth(cell)).max)
      }

      dataRows.zipWithIndex.foreach { case (row, i) =>
        // "+ 5" in size means a character length except for padded names and data
        val rowHeader = StringUtils.rightPad(
          s"-RECORD $i", fieldNameColWidth + dataColWidth + 5, "-")
        sb.append(rowHeader).append("\n")
        row.zipWithIndex.map { case (cell, j) =>
          val fieldName = StringUtils.rightPad(fieldNames(j),
            fieldNameColWidth - Utils.stringHalfWidth(fieldNames(j)) + fieldNames(j).length)
          val data = StringUtils.rightPad(cell,
            dataColWidth - Utils.stringHalfWidth(cell) + cell.length)
          s" $fieldName | $data "
        }.addString(sb, "", "\n", "\n")
      }
    }

    // Print a footer
    if (vertical && rows.tail.isEmpty) {
      // In a vertical mode, print an empty row set explicitly
      sb.append("(0 rows)\n")
    } else if (hasMoreData) {
      // For Data that has more than "numRows" records
      val rowsString = if (numRows == 1) "row" else "rows"
      sb.append(s"only showing top $numRows $rowsString\n")
    }

    sb.toString()
  }

  override def toString: String = {
    try {
      val builder = new StringBuilder
      val fields = schema.take(2).map {
        case f => s"${f.name}: ${f.dataType.simpleString(2)}"
      }
      builder.append("[")
      builder.append(fields.mkString(", "))
      if (schema.length > 2) {
        if (schema.length - fields.size == 1) {
          builder.append(" ... 1 more field")
        } else {
          builder.append(" ... " + (schema.length - 2) + " more fields")
        }
      }
      builder.append("]").toString()
    } catch {
      case NonFatal(e) =>
        s"Invalid tree; ${e.getMessage}:\n$queryExecution"
    }
  }

  /**
   * Converts this strongly typed collection of data to generic Dataframe. In contrast to the
   * strongly typed objects that Dataset operations work on, a Dataframe returns generic [[Row]]
   * objects that allow fields to be accessed by ordinal or name.
   *
   * @group basic
   * @since 1.6.0
   */
  // This is declared with parentheses to prevent the Scala compiler from treating
  // `ds.toDF("1")` as invoking this toDF and then apply on the returned DataFrame.
  def toDF(): DataFrame = new Dataset[Row](sparkSession, queryExecution, RowEncoder(schema))

  /**
   * :: Experimental ::
   * Returns a new Dataset where each record has been mapped on to the specified type. The
   * method used to map columns depend on the type of `U`:
   *  - When `U` is a class, fields for the class will be mapped to columns of the same name
   *    (case sensitivity is determined by `spark.sql.caseSensitive`).
   *  - When `U` is a tuple, the columns will be mapped by ordinal (i.e. the first column will
   *    be assigned to `_1`).
   *  - When `U` is a primitive type (i.e. String, Int, etc), then the first column of the
   *    `DataFrame` will be used.
   *
   * If the schema of the Dataset does not match the desired `U` type, you can use `select`
   * along with `alias` or `as` to rearrange or rename as required.
   *
   * Note that `as[]` only changes the view of the data that is passed into typed operations,
   * such as `map()`, and does not eagerly project away any columns that are not present in
   * the specified class.
   *
   * @group basic
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def as[U : Encoder]: Dataset[U] = Dataset[U](sparkSession, logicalPlan)

  /**
   * Converts this strongly typed collection of data to generic `DataFrame` with columns renamed.
   * This can be quite convenient in conversion from an RDD of tuples into a `DataFrame` with
   * meaningful names. For example:
   * {{{
   *   val rdd: RDD[(Int, String)] = ...
   *   rdd.toDF()  // this implicit conversion creates a DataFrame with column name `_1` and `_2`
   *   rdd.toDF("id", "name")  // this creates a DataFrame with column name "id" and "name"
   * }}}
   *
   * @group basic
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def toDF(colNames: String*): DataFrame = {
    require(schema.size == colNames.size,
      "The number of columns doesn't match.\n" +
        s"Old column names (${schema.size}): " + schema.fields.map(_.name).mkString(", ") + "\n" +
        s"New column names (${colNames.size}): " + colNames.mkString(", "))

    val newCols = logicalPlan.output.zip(colNames).map { case (oldAttribute, newName) =>
      Column(oldAttribute).as(newName)
    }
    select(newCols : _*)
  }

  /**
   * Returns the schema of this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def schema: StructType = queryExecution.analyzed.schema

  /**
   * Prints the schema to the console in a nice tree format.
   *
   * @group basic
   * @since 1.6.0
   */
  // scalastyle:off println
  def printSchema(): Unit = println(schema.treeString)
  // scalastyle:on println

  /**
   * Prints the plans (logical and physical) to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(extended: Boolean): Unit = {
    val explain = ExplainCommand(queryExecution.logical, extended = extended)
    sparkSession.sessionState.executePlan(explain).executedPlan.executeCollect().foreach {
      // scalastyle:off println
      r => println(r.getString(0))
      // scalastyle:on println
    }
  }

  /**
   * Prints the physical plan to the console for debugging purposes.
   *
   * @group basic
   * @since 1.6.0
   */
  def explain(): Unit = explain(extended = false)

  /**
   * Returns all column names and their data types as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def dtypes: Array[(String, String)] = schema.fields.map { field =>
    (field.name, field.dataType.toString)
  }

  /**
   * Returns all column names as an array.
   *
   * @group basic
   * @since 1.6.0
   */
  def columns: Array[String] = schema.fields.map(_.name)

  /**
   * Returns true if the `collect` and `take` methods can be run locally
   * (without any Spark executors).
   *
   * @group basic
   * @since 1.6.0
   */
  def isLocal: Boolean = logicalPlan.isInstanceOf[LocalRelation]

  /**
   * Returns true if the `Dataset` is empty.
   *
   * @group basic
   * @since 2.4.0
   */
  def isEmpty: Boolean = withAction("isEmpty", limit(1).groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0) == 0
  }

  /**
   * Returns true if this Dataset contains one or more sources that continuously
   * return data as it arrives. A Dataset that reads data from a streaming source
   * must be executed as a `StreamingQuery` using the `start()` method in
   * `DataStreamWriter`. Methods that return a single answer, e.g. `count()` or
   * `collect()`, will throw an [[AnalysisException]] when there is a streaming
   * source present.
   *
   * @group streaming
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def isStreaming: Boolean = logicalPlan.isStreaming

  /**
   * Eagerly checkpoint a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = true)

  /**
   * Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the
   * logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. It will be saved to files inside the checkpoint
   * directory set with `SparkContext#setCheckpointDir`.
   *
   * @group basic
   * @since 2.1.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def checkpoint(eager: Boolean): Dataset[T] = checkpoint(eager = eager, reliableCheckpoint = true)

  /**
   * Eagerly locally checkpoints a Dataset and return the new Dataset. Checkpointing can be
   * used to truncate the logical plan of this Dataset, which is especially useful in iterative
   * algorithms where the plan may grow exponentially. Local checkpoints are written to executor
   * storage and despite potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(): Dataset[T] = checkpoint(eager = true, reliableCheckpoint = false)

  /**
   * Locally checkpoints a Dataset and return the new Dataset. Checkpointing can be used to truncate
   * the logical plan of this Dataset, which is especially useful in iterative algorithms where the
   * plan may grow exponentially. Local checkpoints are written to executor storage and despite
   * potentially faster they are unreliable and may compromise job completion.
   *
   * @group basic
   * @since 2.3.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def localCheckpoint(eager: Boolean): Dataset[T] = checkpoint(
    eager = eager,
    reliableCheckpoint = false
  )

  /**
   * Returns a checkpointed version of this Dataset.
   *
   * @param eager Whether to checkpoint this dataframe immediately
   * @param reliableCheckpoint Whether to create a reliable checkpoint saved to files inside the
   *                           checkpoint directory. If false creates a local checkpoint using
   *                           the caching subsystem
   */
  private def checkpoint(eager: Boolean, reliableCheckpoint: Boolean): Dataset[T] = {
    val internalRdd = queryExecution.toRdd.map(_.copy())
    if (reliableCheckpoint) {
      internalRdd.checkpoint()
    } else {
      internalRdd.localCheckpoint()
    }

    if (eager) {
      internalRdd.count()
    }

    val physicalPlan = queryExecution.executedPlan

    // Takes the first leaf partitioning whenever we see a `PartitioningCollection`. Otherwise the
    // size of `PartitioningCollection` may grow exponentially for queries involving deep inner
    // joins.
    def firstLeafPartitioning(partitioning: Partitioning): Partitioning = {
      partitioning match {
        case p: PartitioningCollection => firstLeafPartitioning(p.partitionings.head)
        case p => p
      }
    }

    val outputPartitioning = firstLeafPartitioning(physicalPlan.outputPartitioning)

    Dataset.ofRows(
      sparkSession,
      LogicalRDD(
        logicalPlan.output,
        internalRdd,
        outputPartitioning,
        physicalPlan.outputOrdering,
        isStreaming
      )(sparkSession)).as[T]
  }

  /**
   * Defines an event time watermark for this [[Dataset]]. A watermark tracks a point in time
   * before which we assume no more late data is going to arrive.
   *
   * Spark will use this watermark for several purposes:
   *  - To know when a given time window aggregation can be finalized and thus can be emitted when
   *    using output modes that do not allow updates.
   *  - To minimize the amount of state that we need to keep for on-going aggregations,
   *    `mapGroupsWithState` and `dropDuplicates` operators.
   *
   *  The current watermark is computed by looking at the `MAX(eventTime)` seen across
   *  all of the partitions in the query minus a user specified `delayThreshold`.  Due to the cost
   *  of coordinating this value across partitions, the actual watermark used is only guaranteed
   *  to be at least `delayThreshold` behind the actual event time.  In some cases we may still
   *  process records that arrive more than `delayThreshold` late.
   *
   * @param eventTime the name of the column that contains the event time of the row.
   * @param delayThreshold the minimum delay to wait to data to arrive late, relative to the latest
   *                       record that has been processed in the form of an interval
   *                       (e.g. "1 minute" or "5 hours"). NOTE: This should not be negative.
   *
   * @group streaming
   * @since 2.1.0
   */
  @InterfaceStability.Evolving
  // We only accept an existing column name, not a derived column here as a watermark that is
  // defined on a derived column cannot referenced elsewhere in the plan.
  def withWatermark(eventTime: String, delayThreshold: String): Dataset[T] = withTypedPlan {
    val parsedDelay =
      try {
        CalendarInterval.fromCaseInsensitiveString(delayThreshold)
      } catch {
        case e: IllegalArgumentException =>
          throw new AnalysisException(
            s"Unable to parse time delay '$delayThreshold'",
            cause = Some(e))
      }
    require(parsedDelay.milliseconds >= 0 && parsedDelay.months >= 0,
      s"delay threshold ($delayThreshold) should not be negative.")
    EliminateEventTimeWatermark(
      EventTimeWatermark(UnresolvedAttribute(eventTime), parsedDelay, logicalPlan))
  }

  /**
   * Displays the Dataset in a tabular form. Strings more than 20 characters will be truncated,
   * and all cells will be aligned right. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   *
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int): Unit = show(numRows, truncate = true)

  /**
   * Displays the top 20 rows of Dataset in a tabular form. Strings more than 20 characters
   * will be truncated, and all cells will be aligned right.
   *
   * @group action
   * @since 1.6.0
   */
  def show(): Unit = show(20)

  /**
   * Displays the top 20 rows of Dataset in a tabular form.
   *
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *                 be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  def show(truncate: Boolean): Unit = show(20, truncate)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   * @param numRows Number of rows to show
   * @param truncate Whether truncate long strings. If true, strings more than 20 characters will
   *              be truncated and all cells will be aligned right
   *
   * @group action
   * @since 1.6.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Boolean): Unit = if (truncate) {
    println(showString(numRows, truncate = 20))
  } else {
    println(showString(numRows, truncate = 0))
  }

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @group action
   * @since 1.6.0
   */
  def show(numRows: Int, truncate: Int): Unit = show(numRows, truncate, vertical = false)

  /**
   * Displays the Dataset in a tabular form. For example:
   * {{{
   *   year  month AVG('Adj Close) MAX('Adj Close)
   *   1980  12    0.503218        0.595103
   *   1981  01    0.523289        0.570307
   *   1982  02    0.436504        0.475256
   *   1983  03    0.410516        0.442194
   *   1984  04    0.450090        0.483521
   * }}}
   *
   * If `vertical` enabled, this command prints output rows vertically (one line per column value)?
   *
   * {{{
   * -RECORD 0-------------------
   *  year            | 1980
   *  month           | 12
   *  AVG('Adj Close) | 0.503218
   *  AVG('Adj Close) | 0.595103
   * -RECORD 1-------------------
   *  year            | 1981
   *  month           | 01
   *  AVG('Adj Close) | 0.523289
   *  AVG('Adj Close) | 0.570307
   * -RECORD 2-------------------
   *  year            | 1982
   *  month           | 02
   *  AVG('Adj Close) | 0.436504
   *  AVG('Adj Close) | 0.475256
   * -RECORD 3-------------------
   *  year            | 1983
   *  month           | 03
   *  AVG('Adj Close) | 0.410516
   *  AVG('Adj Close) | 0.442194
   * -RECORD 4-------------------
   *  year            | 1984
   *  month           | 04
   *  AVG('Adj Close) | 0.450090
   *  AVG('Adj Close) | 0.483521
   * }}}
   *
   * @param numRows Number of rows to show
   * @param truncate If set to more than 0, truncates strings to `truncate` characters and
   *                    all cells will be aligned right.
   * @param vertical If set to true, prints output rows vertically (one line per column value).
   * @group action
   * @since 2.3.0
   */
  // scalastyle:off println
  def show(numRows: Int, truncate: Int, vertical: Boolean): Unit =
    println(showString(numRows, truncate, vertical))
  // scalastyle:on println

  /**
   * Returns a [[DataFrameNaFunctions]] for working with missing data.
   * {{{
   *   // Dropping rows containing any null values.
   *   ds.na.drop()
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def na: DataFrameNaFunctions = new DataFrameNaFunctions(toDF())

  /**
   * Returns a [[DataFrameStatFunctions]] for working statistic functions support.
   * {{{
   *   // Finding frequent items in column with name 'a'.
   *   ds.stat.freqItems(Seq("a"))
   * }}}
   *
   * @group untypedrel
   * @since 1.6.0
   */
  def stat: DataFrameStatFunctions = new DataFrameStatFunctions(toDF())

  /**
   * Join with another `DataFrame`.
   *
   * Behaves as an INNER JOIN and requires a subsequent join predicate.
   *
   * @param right Right side of the join operation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Inner, None)
  }

  /**
   * Inner equi-join with another `DataFrame` using the given column.
   *
   * Different from other join functions, the join column will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the column "user_id"
   *   df1.join(df2, "user_id")
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumn Name of the column to join on. This column must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumn: String): DataFrame = {
    join(right, Seq(usingColumn))
  }

  /**
   * Inner equi-join with another `DataFrame` using the given columns.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * {{{
   *   // Joining df1 and df2 using the columns "user_id" and "user_name"
   *   df1.join(df2, Seq("user_id", "user_name"))
   * }}}
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String]): DataFrame = {
    join(right, usingColumns, "inner")
  }

  /**
   * Equi-join with another `DataFrame` using the given columns. A cross join with a predicate
   * is specified as an inner join. If you would explicitly like to perform a cross join use the
   * `crossJoin` method.
   *
   * Different from other join functions, the join columns will only appear once in the output,
   * i.e. similar to SQL's `JOIN USING` syntax.
   *
   * @param right Right side of the join operation.
   * @param usingColumns Names of the columns to join on. This columns must exist on both sides.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @note If you perform a self-join using this function without aliasing the input
   * `DataFrame`s, you will NOT be able to reference any columns after the join, since
   * there is no way to disambiguate which side of the join you would like to reference.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], usingColumns: Seq[String], joinType: String): DataFrame = {
    // Analyze the self join. The assumption is that the analyzer will disambiguate left vs right
    // by creating a new instance for one of the branch.
    val joined = sparkSession.sessionState.executePlan(
      Join(logicalPlan, right.logicalPlan, joinType = JoinType(joinType), None))
      .analyzed.asInstanceOf[Join]

    withPlan {
      Join(
        joined.left,
        joined.right,
        UsingJoin(JoinType(joinType), usingColumns),
        None)
    }
  }

  /**
   * Inner join with another `DataFrame`, using the given join expression.
   *
   * {{{
   *   // The following two are equivalent:
   *   df1.join(df2, $"df1Key" === $"df2Key")
   *   df1.join(df2).where($"df1Key" === $"df2Key")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column): DataFrame = join(right, joinExprs, "inner")

  /**
   * Join with another `DataFrame`, using the given join expression. The following performs
   * a full outer join between `df1` and `df2`.
   *
   * {{{
   *   // Scala:
   *   import org.apache.spark.sql.functions._
   *   df1.join(df2, $"df1Key" === $"df2Key", "outer")
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df1.join(df2, col("df1Key").equalTo(col("df2Key")), "outer");
   * }}}
   *
   * @param right Right side of the join.
   * @param joinExprs Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`, `left_semi`, `left_anti`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def join(right: Dataset[_], joinExprs: Column, joinType: String): DataFrame = {
    // Note that in this function, we introduce a hack in the case of self-join to automatically
    // resolve ambiguous join conditions into ones that might make sense [SPARK-6231].
    // Consider this case: df.join(df, df("key") === df("key"))
    // Since df("key") === df("key") is a trivially true condition, this actually becomes a
    // cartesian join. However, most likely users expect to perform a self join using "key".
    // With that assumption, this hack turns the trivially true condition into equality on join
    // keys that are resolved to both sides.

    // Trigger analysis so in the case of self-join, the analyzer will clone the plan.
    // After the cloning, left and right side will have distinct expression ids.
    val plan = withPlan(
      Join(logicalPlan, right.logicalPlan, JoinType(joinType), Some(joinExprs.expr)))
      .queryExecution.analyzed.asInstanceOf[Join]

    // If auto self join alias is disabled, return the plan.
    if (!sparkSession.sessionState.conf.dataFrameSelfJoinAutoResolveAmbiguity) {
      return withPlan(plan)
    }

    // If left/right have no output set intersection, return the plan.
    val lanalyzed = withPlan(this.logicalPlan).queryExecution.analyzed
    val ranalyzed = withPlan(right.logicalPlan).queryExecution.analyzed
    if (lanalyzed.outputSet.intersect(ranalyzed.outputSet).isEmpty) {
      return withPlan(plan)
    }

    // Otherwise, find the trivially true predicates and automatically resolves them to both sides.
    // By the time we get here, since we have already run analysis, all attributes should've been
    // resolved and become AttributeReference.
    val cond = plan.condition.map { _.transform {
      case catalyst.expressions.EqualTo(a: AttributeReference, b: AttributeReference)
          if a.sameRef(b) =>
        catalyst.expressions.EqualTo(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
      case catalyst.expressions.EqualNullSafe(a: AttributeReference, b: AttributeReference)
        if a.sameRef(b) =>
        catalyst.expressions.EqualNullSafe(
          withPlan(plan.left).resolve(a.name),
          withPlan(plan.right).resolve(b.name))
    }}

    withPlan {
      plan.copy(condition = cond)
    }
  }

  /**
   * Explicit cartesian join with another `DataFrame`.
   *
   * @param right Right side of the join operation.
   *
   * @note Cartesian joins are very expensive without an extra filter that can be pushed down.
   *
   * @group untypedrel
   * @since 2.1.0
   */
  def crossJoin(right: Dataset[_]): DataFrame = withPlan {
    Join(logicalPlan, right.logicalPlan, joinType = Cross, None)
  }

  /**
   * :: Experimental ::
   * Joins this Dataset returning a `Tuple2` for each pair where `condition` evaluates to
   * true.
   *
   * This is similar to the relation `join` function with one important difference in the
   * result schema. Since `joinWith` preserves objects present on either side of the join, the
   * result schema is similarly nested into a tuple under the column names `_1` and `_2`.
   *
   * This type of join can be useful both for preserving type-safety with the original object
   * types as well as working with relational data where either side of the join has column
   * names in common.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   * @param joinType Type of join to perform. Default `inner`. Must be one of:
   *                 `inner`, `cross`, `outer`, `full`, `full_outer`, `left`, `left_outer`,
   *                 `right`, `right_outer`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column, joinType: String): Dataset[(T, U)] = {
    // Creates a Join node and resolve it first, to get join condition resolved, self-join resolved,
    // etc.
    val joined = sparkSession.sessionState.executePlan(
      Join(
        this.logicalPlan,
        other.logicalPlan,
        JoinType(joinType),
        Some(condition.expr))).analyzed.asInstanceOf[Join]

    if (joined.joinType == LeftSemi || joined.joinType == LeftAnti) {
      throw new AnalysisException("Invalid join type in joinWith: " + joined.joinType.sql)
    }

    // For both join side, combine all outputs into a single column and alias it with "_1" or "_2",
    // to match the schema for the encoder of the join result.
    // Note that we do this before joining them, to enable the join operator to return null for one
    // side, in cases like outer-join.
    val left = {
      val combined = if (this.exprEnc.flat) {
        assert(joined.left.output.length == 1)
        Alias(joined.left.output.head, "_1")()
      } else {
        Alias(CreateStruct(joined.left.output), "_1")()
      }
      Project(combined :: Nil, joined.left)
    }

    val right = {
      val combined = if (other.exprEnc.flat) {
        assert(joined.right.output.length == 1)
        Alias(joined.right.output.head, "_2")()
      } else {
        Alias(CreateStruct(joined.right.output), "_2")()
      }
      Project(combined :: Nil, joined.right)
    }

    // Rewrites the join condition to make the attribute point to correct column/field, after we
    // combine the outputs of each join side.
    val conditionExpr = joined.condition.get transformUp {
      case a: Attribute if joined.left.outputSet.contains(a) =>
        if (this.exprEnc.flat) {
          left.output.head
        } else {
          val index = joined.left.output.indexWhere(_.exprId == a.exprId)
          GetStructField(left.output.head, index)
        }
      case a: Attribute if joined.right.outputSet.contains(a) =>
        if (other.exprEnc.flat) {
          right.output.head
        } else {
          val index = joined.right.output.indexWhere(_.exprId == a.exprId)
          GetStructField(right.output.head, index)
        }
    }

    implicit val tuple2Encoder: Encoder[(T, U)] =
      ExpressionEncoder.tuple(this.exprEnc, other.exprEnc)

    withTypedPlan(Join(left, right, joined.joinType, Some(conditionExpr)))
  }

  /**
   * :: Experimental ::
   * Using inner equi-join to join this Dataset returning a `Tuple2` for each pair
   * where `condition` evaluates to true.
   *
   * @param other Right side of the join.
   * @param condition Join expression.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def joinWith[U](other: Dataset[U], condition: Column): Dataset[(T, U)] = {
    joinWith(other, condition, "inner")
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortCol: String, sortCols: String*): Dataset[T] = {
    sortWithinPartitions((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset with each partition sorted by the given expressions.
   *
   * This is the same operation as "SORT BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sortWithinPartitions(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = false, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the specified column, all in ascending order.
   * {{{
   *   // The following 3 are equivalent
   *   ds.sort("sortcol")
   *   ds.sort($"sortcol")
   *   ds.sort($"sortcol".asc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortCol: String, sortCols: String*): Dataset[T] = {
    sort((sortCol +: sortCols).map(Column(_)) : _*)
  }

  /**
   * Returns a new Dataset sorted by the given expressions. For example:
   * {{{
   *   ds.sort($"col1", $"col2".desc)
   * }}}
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def sort(sortExprs: Column*): Dataset[T] = {
    sortInternal(global = true, sortExprs)
  }

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortCol: String, sortCols: String*): Dataset[T] = sort(sortCol, sortCols : _*)

  /**
   * Returns a new Dataset sorted by the given expressions.
   * This is an alias of the `sort` function.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def orderBy(sortExprs: Column*): Dataset[T] = sort(sortExprs : _*)

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def apply(colName: String): Column = col(colName)

  /**
   * Specifies some hint on the current Dataset. As an example, the following code specifies
   * that one of the plan can be broadcasted:
   *
   * {{{
   *   df1.join(df2.hint("broadcast"))
   * }}}
   *
   * @group basic
   * @since 2.2.0
   */
  @scala.annotation.varargs
  def hint(name: String, parameters: Any*): Dataset[T] = withTypedPlan {
    UnresolvedHint(name, parameters, logicalPlan)
  }

  /**
   * Selects column based on the column name and returns it as a [[Column]].
   *
   * @note The column name can also reference to a nested column like `a.b`.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def col(colName: String): Column = colName match {
    case "*" =>
      Column(ResolvedStar(queryExecution.analyzed.output))
    case _ =>
      if (sqlContext.conf.supportQuotedRegexColumnName) {
        colRegex(colName)
      } else {
        val expr = resolve(colName)
        Column(expr)
      }
  }

  /**
   * Selects column based on the column name specified as a regex and returns it as [[Column]].
   * @group untypedrel
   * @since 2.3.0
   */
  def colRegex(colName: String): Column = {
    val caseSensitive = sparkSession.sessionState.conf.caseSensitiveAnalysis
    colName match {
      case ParserUtils.escapedIdentifier(columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, None, caseSensitive))
      case ParserUtils.qualifiedEscapedIdentifier(nameParts, columnNameRegex) =>
        Column(UnresolvedRegex(columnNameRegex, Some(nameParts), caseSensitive))
      case _ =>
        Column(resolve(colName))
    }
  }

  /**
   * Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def as(alias: String): Dataset[T] = withTypedPlan {
    SubqueryAlias(alias, logicalPlan)
  }

  /**
   * (Scala-specific) Returns a new Dataset with an alias set.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def as(alias: Symbol): Dataset[T] = as(alias.name)

  /**
   * Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: String): Dataset[T] = as(alias)

  /**
   * (Scala-specific) Returns a new Dataset with an alias set. Same as `as`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def alias(alias: Symbol): Dataset[T] = as(alias)

  /**
   * Selects a set of column based expressions.
   * {{{
   *   ds.select($"colA", $"colB" + 1)
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(cols: Column*): DataFrame = withPlan {
    Project(cols.map(_.named), logicalPlan)
  }

  /**
   * Selects a set of columns. This is a variant of `select` that can only select
   * existing columns using column names (i.e. cannot construct expressions).
   *
   * {{{
   *   // The following two are equivalent:
   *   ds.select("colA", "colB")
   *   ds.select($"colA", $"colB")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def select(col: String, cols: String*): DataFrame = select((col +: cols).map(Column(_)) : _*)

  /**
   * Selects a set of SQL expressions. This is a variant of `select` that accepts
   * SQL expressions.
   *
   * {{{
   *   // The following are equivalent:
   *   ds.selectExpr("colA", "colB as newName", "abs(colC)")
   *   ds.select(expr("colA"), expr("colB as newName"), expr("abs(colC)"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def selectExpr(exprs: String*): DataFrame = {
    select(exprs.map { expr =>
      Column(sparkSession.sessionState.sqlParser.parseExpression(expr))
    }: _*)
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expression for each element.
   *
   * {{{
   *   val ds = Seq(1, 2, 3).toDS()
   *   val newDS = ds.select(expr("value + 1").as[Int])
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1](c1: TypedColumn[T, U1]): Dataset[U1] = {
    implicit val encoder = c1.encoder
    val project = Project(c1.withInputType(exprEnc, logicalPlan.output).named :: Nil, logicalPlan)

    if (encoder.flat) {
      new Dataset[U1](sparkSession, project, encoder)
    } else {
      // Flattens inner fields of U1
      new Dataset[Tuple1[U1]](sparkSession, project, ExpressionEncoder.tuple(encoder)).map(_._1)
    }
  }

  /**
   * Internal helper function for building typed selects that return tuples. For simplicity and
   * code reuse, we do this without the help of the type system and then use helper functions
   * that cast appropriately for the user facing interface.
   */
  protected def selectUntyped(columns: TypedColumn[_, _]*): Dataset[_] = {
    val encoders = columns.map(_.encoder)
    val namedColumns =
      columns.map(_.withInputType(exprEnc, logicalPlan.output).named)
    val execution = new QueryExecution(sparkSession, Project(namedColumns, logicalPlan))
    new Dataset(sparkSession, execution, ExpressionEncoder.tuple(encoders))
  }

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2](c1: TypedColumn[T, U1], c2: TypedColumn[T, U2]): Dataset[(U1, U2)] =
    selectUntyped(c1, c2).asInstanceOf[Dataset[(U1, U2)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3]): Dataset[(U1, U2, U3)] =
    selectUntyped(c1, c2, c3).asInstanceOf[Dataset[(U1, U2, U3)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4]): Dataset[(U1, U2, U3, U4)] =
    selectUntyped(c1, c2, c3, c4).asInstanceOf[Dataset[(U1, U2, U3, U4)]]

  /**
   * :: Experimental ::
   * Returns a new Dataset by computing the given [[Column]] expressions for each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def select[U1, U2, U3, U4, U5](
      c1: TypedColumn[T, U1],
      c2: TypedColumn[T, U2],
      c3: TypedColumn[T, U3],
      c4: TypedColumn[T, U4],
      c5: TypedColumn[T, U5]): Dataset[(U1, U2, U3, U4, U5)] =
    selectUntyped(c1, c2, c3, c4, c5).asInstanceOf[Dataset[(U1, U2, U3, U4, U5)]]

  /**
   * Filters rows using the given condition.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(condition: Column): Dataset[T] = withTypedPlan {
    Filter(condition.expr, logicalPlan)
  }

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.filter("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def filter(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Filters rows using the given condition. This is an alias for `filter`.
   * {{{
   *   // The following are equivalent:
   *   peopleDs.filter($"age" > 15)
   *   peopleDs.where($"age" > 15)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(condition: Column): Dataset[T] = filter(condition)

  /**
   * Filters rows using the given SQL expression.
   * {{{
   *   peopleDs.where("age > 15")
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def where(conditionExpr: String): Dataset[T] = {
    filter(Column(sparkSession.sessionState.sqlParser.parseExpression(conditionExpr)))
  }

  /**
   * Groups the Dataset using the specified columns, so we can run aggregation on them. See
   * [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy($"department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.GroupByType)
  }

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube($"department", $"group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(cols: Column*): RelationalGroupedDataset = {
    RelationalGroupedDataset(toDF(), cols.map(_.expr), RelationalGroupedDataset.CubeType)
  }

  /**
   * Groups the Dataset using the specified columns, so that we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of groupBy that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns grouped by department.
   *   ds.groupBy("department").avg()
   *
   *   // Compute the max age and average salary, grouped by department and gender.
   *   ds.groupBy($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def groupBy(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.GroupByType)
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: (T, T) => T): T = withNewRDDExecutionId {
    rdd.reduce(func)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Reduces the elements of this Dataset using the specified binary function. The given `func`
   * must be commutative and associative or the result may be non-deterministic.
   *
   * @group action
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def reduce(func: ReduceFunction[T]): T = reduce(func.call(_, _))

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K: Encoder](func: T => K): KeyValueGroupedDataset[K, T] = {
    val withGroupingKey = AppendColumns(func, logicalPlan)
    val executed = sparkSession.sessionState.executePlan(withGroupingKey)

    new KeyValueGroupedDataset(
      encoderFor[K],
      encoderFor[T],
      executed,
      logicalPlan.output,
      withGroupingKey.newColumns)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a [[KeyValueGroupedDataset]] where the data is grouped by the given key `func`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def groupByKey[K](func: MapFunction[T, K], encoder: Encoder[K]): KeyValueGroupedDataset[K, T] =
    groupByKey(func.call(_))(encoder)

  /**
   * Create a multi-dimensional rollup for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of rollup that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns rolluped by department and group.
   *   ds.rollup("department", "group").avg()
   *
   *   // Compute the max age and average salary, rolluped by department and gender.
   *   ds.rollup($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def rollup(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.RollupType)
  }

  /**
   * Create a multi-dimensional cube for the current Dataset using the specified columns,
   * so we can run aggregation on them.
   * See [[RelationalGroupedDataset]] for all the available aggregate functions.
   *
   * This is a variant of cube that can only group by existing columns using column names
   * (i.e. cannot construct expressions).
   *
   * {{{
   *   // Compute the average for all numeric columns cubed by department and group.
   *   ds.cube("department", "group").avg()
   *
   *   // Compute the max age and average salary, cubed by department and gender.
   *   ds.cube($"department", $"gender").agg(Map(
   *     "salary" -> "avg",
   *     "age" -> "max"
   *   ))
   * }}}
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def cube(col1: String, cols: String*): RelationalGroupedDataset = {
    val colNames: Seq[String] = col1 +: cols
    RelationalGroupedDataset(
      toDF(), colNames.map(colName => resolve(colName)), RelationalGroupedDataset.CubeType)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg("age" -> "max", "salary" -> "avg")
   *   ds.groupBy().agg("age" -> "max", "salary" -> "avg")
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(aggExpr: (String, String), aggExprs: (String, String)*): DataFrame = {
    groupBy().agg(aggExpr, aggExprs : _*)
  }

  /**
   * (Scala-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * (Java-specific) Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(Map("age" -> "max", "salary" -> "avg"))
   *   ds.groupBy().agg(Map("age" -> "max", "salary" -> "avg"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def agg(exprs: java.util.Map[String, String]): DataFrame = groupBy().agg(exprs)

  /**
   * Aggregates on the entire Dataset without groups.
   * {{{
   *   // ds.agg(...) is a shorthand for ds.groupBy().agg(...)
   *   ds.agg(max($"age"), avg($"salary"))
   *   ds.groupBy().agg(max($"age"), avg($"salary"))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def agg(expr: Column, exprs: Column*): DataFrame = groupBy().agg(expr, exprs : _*)

  /**
   * Returns a new Dataset by taking the first `n` rows. The difference between this function
   * and `head` is that `head` is an action and returns an array (by triggering query execution)
   * while `limit` returns a new Dataset.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def limit(n: Int): Dataset[T] = withTypedPlan {
    Limit(Literal(n), logicalPlan)
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @deprecated("use union()", "2.0.0")
  def unionAll(other: Dataset[T]): Dataset[T] = union(other)

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is equivalent to `UNION ALL` in SQL. To do a SQL-style set union (that does
   * deduplication of elements), use this function followed by a [[distinct]].
   *
   * Also as standard in SQL, this function resolves columns by position (not by name):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.union(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   4|   5|   6|
   *   // +----+----+----+
   * }}}
   *
   * Notice that the column positions in the schema aren't necessarily matched with the
   * fields in the strongly typed objects in a Dataset. This function resolves columns
   * by their positions in the schema, not the fields in the strongly typed objects. Use
   * [[unionByName]] to resolve columns by field name in the typed objects.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def union(other: Dataset[T]): Dataset[T] = withSetOperator {
    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, other.logicalPlan))
  }

  /**
   * Returns a new Dataset containing union of rows in this Dataset and another Dataset.
   *
   * This is different from both `UNION ALL` and `UNION DISTINCT` in SQL. To do a SQL-style set
   * union (that does deduplication of elements), use this function followed by a [[distinct]].
   *
   * The difference between this function and [[union]] is that this function
   * resolves columns by name (not by position):
   *
   * {{{
   *   val df1 = Seq((1, 2, 3)).toDF("col0", "col1", "col2")
   *   val df2 = Seq((4, 5, 6)).toDF("col1", "col2", "col0")
   *   df1.unionByName(df2).show
   *
   *   // output:
   *   // +----+----+----+
   *   // |col0|col1|col2|
   *   // +----+----+----+
   *   // |   1|   2|   3|
   *   // |   6|   4|   5|
   *   // +----+----+----+
   * }}}
   *
   * @group typedrel
   * @since 2.3.0
   */
  def unionByName(other: Dataset[T]): Dataset[T] = withSetOperator {
    // Check column name duplication
    val resolver = sparkSession.sessionState.analyzer.resolver
    val leftOutputAttrs = logicalPlan.output
    val rightOutputAttrs = other.logicalPlan.output

    SchemaUtils.checkColumnNameDuplication(
      leftOutputAttrs.map(_.name),
      "in the left attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)
    SchemaUtils.checkColumnNameDuplication(
      rightOutputAttrs.map(_.name),
      "in the right attributes",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    // Builds a project list for `other` based on `logicalPlan` output names
    val rightProjectList = leftOutputAttrs.map { lattr =>
      rightOutputAttrs.find { rattr => resolver(lattr.name, rattr.name) }.getOrElse {
        throw new AnalysisException(
          s"""Cannot resolve column name "${lattr.name}" among """ +
            s"""(${rightOutputAttrs.map(_.name).mkString(", ")})""")
      }
    }

    // Delegates failure checks to `CheckAnalysis`
    val notFoundAttrs = rightOutputAttrs.diff(rightProjectList)
    val rightChild = Project(rightProjectList ++ notFoundAttrs, other.logicalPlan)

    // This breaks caching, but it's usually ok because it addresses a very specific use case:
    // using union to union many files or partitions.
    CombineUnions(Union(logicalPlan, rightChild))
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset.
   * This is equivalent to `INTERSECT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def intersect(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows only in both this Dataset and another Dataset while
   * preserving the duplicates.
   * This is equivalent to `INTERSECT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard
   * in SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def intersectAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Intersect(logicalPlan, other.logicalPlan, isAll = true)
  }


  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset.
   * This is equivalent to `EXCEPT DISTINCT` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def except(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = false)
  }

  /**
   * Returns a new Dataset containing rows in this Dataset but not in another Dataset while
   * preserving the duplicates.
   * This is equivalent to `EXCEPT ALL` in SQL.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`. Also as standard in
   * SQL, this function resolves columns by position (not by name).
   *
   * @group typedrel
   * @since 2.4.0
   */
  def exceptAll(other: Dataset[T]): Dataset[T] = withSetOperator {
    Except(logicalPlan, other.logicalPlan, isAll = true)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a user-supplied seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double, seed: Long): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction, seed = seed)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows (without replacement),
   * using a random seed.
   *
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 2.3.0
   */
  def sample(fraction: Double): Dataset[T] = {
    sample(withReplacement = false, fraction = fraction)
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a user-supplied seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   * @param seed Seed for sampling.
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double, seed: Long): Dataset[T] = {
    withTypedPlan {
      Sample(0.0, fraction, withReplacement, seed, logicalPlan)
    }
  }

  /**
   * Returns a new [[Dataset]] by sampling a fraction of rows, using a random seed.
   *
   * @param withReplacement Sample with replacement or not.
   * @param fraction Fraction of rows to generate, range [0.0, 1.0].
   *
   * @note This is NOT guaranteed to provide exactly the fraction of the total count
   * of the given [[Dataset]].
   *
   * @group typedrel
   * @since 1.6.0
   */
  def sample(withReplacement: Boolean, fraction: Double): Dataset[T] = {
    sample(withReplacement, fraction, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * For Java API, use [[randomSplitAsList]].
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double], seed: Long): Array[Dataset[T]] = {
    require(weights.forall(_ >= 0),
      s"Weights must be nonnegative, but got ${weights.mkString("[", ",", "]")}")
    require(weights.sum > 0,
      s"Sum of weights must be positive, but got ${weights.mkString("[", ",", "]")}")

    // It is possible that the underlying dataframe doesn't guarantee the ordering of rows in its
    // constituent partitions each time a split is materialized which could result in
    // overlapping splits. To prevent this, we explicitly sort each input partition to make the
    // ordering deterministic. Note that MapTypes cannot be sorted and are explicitly pruned out
    // from the sort order.
    val sortOrder = logicalPlan.output
      .filter(attr => RowOrdering.isOrderable(attr.dataType))
      .map(SortOrder(_, Ascending))
    val plan = if (sortOrder.nonEmpty) {
      Sort(sortOrder, global = false, logicalPlan)
    } else {
      // SPARK-12662: If sort order is empty, we materialize the dataset to guarantee determinism
      cache()
      logicalPlan
    }
    val sum = weights.sum
    val normalizedCumWeights = weights.map(_ / sum).scanLeft(0.0d)(_ + _)
    normalizedCumWeights.sliding(2).map { x =>
      new Dataset[T](
        sparkSession, Sample(x(0), x(1), withReplacement = false, seed, plan), encoder)
    }.toArray
  }

  /**
   * Returns a Java list that contains randomly split Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplitAsList(weights: Array[Double], seed: Long): java.util.List[Dataset[T]] = {
    val values = randomSplit(weights, seed)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Randomly splits this Dataset with the provided weights.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @group typedrel
   * @since 2.0.0
   */
  def randomSplit(weights: Array[Double]): Array[Dataset[T]] = {
    randomSplit(weights, Utils.random.nextLong)
  }

  /**
   * Randomly splits this Dataset with the provided weights. Provided for the Python Api.
   *
   * @param weights weights for splits, will be normalized if they don't sum to 1.
   * @param seed Seed for sampling.
   */
  private[spark] def randomSplit(weights: List[Double], seed: Long): Array[Dataset[T]] = {
    randomSplit(weights.toArray, seed)
  }

  /**
   * (Scala-specific) Returns a new Dataset where each row has been expanded to zero or more
   * rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. The columns of
   * the input row are implicitly joined with each row that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()` or `flatMap()`. The following example uses these alternatives to count
   * the number of books that contain a given word:
   *
   * {{{
   *   case class Book(title: String, words: String)
   *   val ds: Dataset[Book]
   *
   *   val allWords = ds.select('title, explode(split('words, " ")).as("word"))
   *
   *   val bookCountPerWord = allWords.groupBy("word").agg(countDistinct("title"))
   * }}}
   *
   * Using `flatMap()` this can similarly be exploded as:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A <: Product : TypeTag](input: Column*)(f: Row => TraversableOnce[A]): DataFrame = {
    val elementSchema = ScalaReflection.schemaFor[A].dataType.asInstanceOf[StructType]

    val convert = CatalystTypeConverters.createToCatalystConverter(elementSchema)

    val rowFunction =
      f.andThen(_.map(convert(_).asInstanceOf[InternalRow]))
    val generator = UserDefinedGenerator(elementSchema, rowFunction, input.map(_.expr))

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * (Scala-specific) Returns a new Dataset where a single column has been expanded to zero
   * or more rows by the provided function. This is similar to a `LATERAL VIEW` in HiveQL. All
   * columns of the input row are implicitly joined with each value that is output by the function.
   *
   * Given that this is deprecated, as an alternative, you can explode columns either using
   * `functions.explode()`:
   *
   * {{{
   *   ds.select(explode(split('words, " ")).as("word"))
   * }}}
   *
   * or `flatMap()`:
   *
   * {{{
   *   ds.flatMap(_.words.split(" "))
   * }}}
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @deprecated("use flatMap() or select() with functions.explode() instead", "2.0.0")
  def explode[A, B : TypeTag](inputColumn: String, outputColumn: String)(f: A => TraversableOnce[B])
    : DataFrame = {
    val dataType = ScalaReflection.schemaFor[B].dataType
    val attributes = AttributeReference(outputColumn, dataType)() :: Nil
    // TODO handle the metadata?
    val elementSchema = attributes.toStructType

    def rowFunction(row: Row): TraversableOnce[InternalRow] = {
      val convert = CatalystTypeConverters.createToCatalystConverter(dataType)
      f(row(0).asInstanceOf[A]).map(o => InternalRow(convert(o)))
    }
    val generator = UserDefinedGenerator(elementSchema, rowFunction, apply(inputColumn).expr :: Nil)

    withPlan {
      Generate(generator, unrequiredChildIndex = Nil, outer = false,
        qualifier = None, generatorOutput = Nil, logicalPlan)
    }
  }

  /**
   * Returns a new Dataset by adding a column or replacing the existing column that has
   * the same name.
   *
   * `column`'s expression must only refer to attributes supplied by this Dataset. It is an
   * error to add a column that refers to some other Dataset.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumn(colName: String, col: Column): DataFrame = withColumns(Seq(colName), Seq(col))

  /**
   * Returns a new Dataset by adding columns or replacing the existing columns that has
   * the same names.
   */
  private[spark] def withColumns(colNames: Seq[String], cols: Seq[Column]): DataFrame = {
    require(colNames.size == cols.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of columns: ${cols.size}")
    SchemaUtils.checkColumnNameDuplication(
      colNames,
      "in given column names",
      sparkSession.sessionState.conf.caseSensitiveAnalysis)

    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output

    val columnMap = colNames.zip(cols).toMap

    val replacedAndExistingColumns = output.map { field =>
      columnMap.find { case (colName, _) =>
        resolver(field.name, colName)
      } match {
        case Some((colName: String, col: Column)) => col.as(colName)
        case _ => Column(field)
      }
    }

    val newColumns = columnMap.filter { case (colName, col) =>
      !output.exists(f => resolver(f.name, colName))
    }.map { case (colName, col) => col.as(colName) }

    select(replacedAndExistingColumns ++ newColumns : _*)
  }

  /**
   * Returns a new Dataset by adding columns with metadata.
   */
  private[spark] def withColumns(
      colNames: Seq[String],
      cols: Seq[Column],
      metadata: Seq[Metadata]): DataFrame = {
    require(colNames.size == metadata.size,
      s"The size of column names: ${colNames.size} isn't equal to " +
        s"the size of metadata elements: ${metadata.size}")
    val newCols = colNames.zip(cols).zip(metadata).map { case ((colName, col), metadata) =>
      col.as(colName, metadata)
    }
    withColumns(colNames, newCols)
  }

  /**
   * Returns a new Dataset by adding a column with metadata.
   */
  private[spark] def withColumn(colName: String, col: Column, metadata: Metadata): DataFrame =
    withColumns(Seq(colName), Seq(col), Seq(metadata))

  /**
   * Returns a new Dataset with a column renamed.
   * This is a no-op if schema doesn't contain existingName.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def withColumnRenamed(existingName: String, newName: String): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val output = queryExecution.analyzed.output
    val shouldRename = output.exists(f => resolver(f.name, existingName))
    if (shouldRename) {
      val columns = output.map { col =>
        if (resolver(col.name, existingName)) {
          Column(col).as(newName)
        } else {
          Column(col)
        }
      }
      select(columns : _*)
    } else {
      toDF()
    }
  }

  /**
   * Returns a new Dataset with a column dropped. This is a no-op if schema doesn't contain
   * column name.
   *
   * This method can only be used to drop top level columns. the colName string is treated
   * literally without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(colName: String): DataFrame = {
    drop(Seq(colName) : _*)
  }

  /**
   * Returns a new Dataset with columns dropped.
   * This is a no-op if schema doesn't contain column name(s).
   *
   * This method can only be used to drop top level columns. the colName string is treated literally
   * without further interpretation.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def drop(colNames: String*): DataFrame = {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val remainingCols = allColumns.filter { attribute =>
      colNames.forall(n => !resolver(attribute.name, n))
    }.map(attribute => Column(attribute))
    if (remainingCols.size == allColumns.size) {
      toDF()
    } else {
      this.select(remainingCols: _*)
    }
  }

  /**
   * Returns a new Dataset with a column dropped.
   * This version of drop accepts a [[Column]] rather than a name.
   * This is a no-op if the Dataset doesn't have a column
   * with an equivalent expression.
   *
   * @group untypedrel
   * @since 2.0.0
   */
  def drop(col: Column): DataFrame = {
    val expression = col match {
      case Column(u: UnresolvedAttribute) =>
        queryExecution.analyzed.resolveQuoted(
          u.name, sparkSession.sessionState.analyzer.resolver).getOrElse(u)
      case Column(expr: Expression) => expr
    }
    val attrs = this.logicalPlan.output
    val colsAfterDrop = attrs.filter { attr =>
      attr != expression
    }.map(attr => Column(attr))
    select(colsAfterDrop : _*)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `distinct`.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(): Dataset[T] = dropDuplicates(this.columns)

  /**
   * (Scala-specific) Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Seq[String]): Dataset[T] = withTypedPlan {
    val resolver = sparkSession.sessionState.analyzer.resolver
    val allColumns = queryExecution.analyzed.output
    val groupCols = colNames.toSet.toSeq.flatMap { (colName: String) =>
      // It is possibly there are more than one columns with the same name,
      // so we call filter instead of find.
      val cols = allColumns.filter(col => resolver(col.name, colName))
      if (cols.isEmpty) {
        throw new AnalysisException(
          s"""Cannot resolve column name "$colName" among (${schema.fieldNames.mkString(", ")})""")
      }
      cols
    }
    Deduplicate(groupCols, logicalPlan)
  }

  /**
   * Returns a new Dataset with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def dropDuplicates(colNames: Array[String]): Dataset[T] = dropDuplicates(colNames.toSeq)

  /**
   * Returns a new [[Dataset]] with duplicate rows removed, considering only
   * the subset of columns.
   *
   * For a static batch [[Dataset]], it just drops duplicate rows. For a streaming [[Dataset]], it
   * will keep all data across triggers as intermediate state to drop duplicates rows. You can use
   * [[withWatermark]] to limit how late the duplicate data can be and system will accordingly limit
   * the state. In addition, too late data older than watermark will be dropped to avoid any
   * possibility of duplicates.
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def dropDuplicates(col1: String, cols: String*): Dataset[T] = {
    val colNames: Seq[String] = col1 +: cols
    dropDuplicates(colNames)
  }

  /**
   * Computes basic statistics for numeric and string columns, including count, mean, stddev, min,
   * and max. If no columns are given, this function computes statistics for all numerical or
   * string columns.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.describe("age", "height").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // max     92.0  192.0
   * }}}
   *
   * Use [[summary]] for expanded statistics and control over which statistics to compute.
   *
   * @param cols Columns to compute statistics on.
   *
   * @group action
   * @since 1.6.0
   */
  @scala.annotation.varargs
  def describe(cols: String*): DataFrame = {
    val selected = if (cols.isEmpty) this else select(cols.head, cols.tail: _*)
    selected.summary("count", "mean", "stddev", "min", "max")
  }

  /**
   * Computes specified statistics for numeric and string columns. Available statistics are:
   *
   * - count
   * - mean
   * - stddev
   * - min
   * - max
   * - arbitrary approximate percentiles specified as a percentage (eg, 75%)
   *
   * If no statistics are given, this function computes count, mean, stddev, min,
   * approximate quartiles (percentiles at 25%, 50%, and 75%), and max.
   *
   * This function is meant for exploratory data analysis, as we make no guarantee about the
   * backward compatibility of the schema of the resulting Dataset. If you want to
   * programmatically compute summary statistics, use the `agg` function instead.
   *
   * {{{
   *   ds.summary().show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // mean    53.3  178.05
   *   // stddev  11.6  15.7
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 50%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * {{{
   *   ds.summary("count", "min", "25%", "75%", "max").show()
   *
   *   // output:
   *   // summary age   height
   *   // count   10.0  10.0
   *   // min     18.0  163.0
   *   // 25%     24.0  176.0
   *   // 75%     32.0  180.0
   *   // max     92.0  192.0
   * }}}
   *
   * To do a summary for specific columns first select them:
   *
   * {{{
   *   ds.select("age", "height").summary().show()
   * }}}
   *
   * See also [[describe]] for basic statistics.
   *
   * @param statistics Statistics from above list to be computed.
   *
   * @group action
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def summary(statistics: String*): DataFrame = StatFunctions.summary(this, statistics.toSeq)

  /**
   * Returns the first `n` rows.
   *
   * @note this method should only be used if the resulting array is expected to be small, as
   * all the data is loaded into the driver's memory.
   *
   * @group action
   * @since 1.6.0
   */
  def head(n: Int): Array[T] = withAction("head", limit(n).queryExecution)(collectFromPlan)

  /**
   * Returns the first row.
   * @group action
   * @since 1.6.0
   */
  def head(): T = head(1).head

  /**
   * Returns the first row. Alias for head().
   * @group action
   * @since 1.6.0
   */
  def first(): T = head()

  /**
   * Concise syntax for chaining custom transformations.
   * {{{
   *   def featurize(ds: Dataset[T]): Dataset[U] = ...
   *
   *   ds
   *     .transform(featurize)
   *     .transform(...)
   * }}}
   *
   * @group typedrel
   * @since 1.6.0
   */
  def transform[U](t: Dataset[T] => Dataset[U]): Dataset[U] = t(this)

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: T => Boolean): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that only contains elements where `func` returns `true`.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def filter(func: FilterFunction[T]): Dataset[T] = {
    withTypedPlan(TypedFilter(func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U : Encoder](func: T => U): Dataset[U] = withTypedPlan {
    MapElements[T, U](func, logicalPlan)
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `func` to each element.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def map[U](func: MapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    implicit val uEnc = encoder
    withTypedPlan(MapElements[T, U](func, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset that contains the result of applying `func` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U : Encoder](func: Iterator[T] => Iterator[U]): Dataset[U] = {
    new Dataset[U](
      sparkSession,
      MapPartitions[T, U](func, logicalPlan),
      implicitly[Encoder[U]])
  }

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset that contains the result of applying `f` to each partition.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def mapPartitions[U](f: MapPartitionsFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (Iterator[T]) => Iterator[U] = x => f.call(x.asJava).asScala
    mapPartitions(func)(encoder)
  }

  /**
   * Returns a new `DataFrame` that contains the result of applying a serialized R function
   * `func` to each partition.
   */
  private[sql] def mapPartitionsInR(
      func: Array[Byte],
      packageNames: Array[Byte],
      broadcastVars: Array[Broadcast[Object]],
      schema: StructType): DataFrame = {
    val rowEncoder = encoder.asInstanceOf[ExpressionEncoder[Row]]
    Dataset.ofRows(
      sparkSession,
      MapPartitionsInR(func, packageNames, broadcastVars, schema, rowEncoder, logicalPlan))
  }

  /**
   * :: Experimental ::
   * (Scala-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U : Encoder](func: T => TraversableOnce[U]): Dataset[U] =
    mapPartitions(_.flatMap(func))

  /**
   * :: Experimental ::
   * (Java-specific)
   * Returns a new Dataset by first applying a function to all elements of this Dataset,
   * and then flattening the results.
   *
   * @group typedrel
   * @since 1.6.0
   */
  @Experimental
  @InterfaceStability.Evolving
  def flatMap[U](f: FlatMapFunction[T, U], encoder: Encoder[U]): Dataset[U] = {
    val func: (T) => Iterator[U] = x => f.call(x).asScala
    flatMap(func)(encoder)
  }

  /**
   * Applies a function `f` to all rows.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(f: T => Unit): Unit = withNewRDDExecutionId {
    rdd.foreach(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each element of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreach(func: ForeachFunction[T]): Unit = foreach(func.call(_))

  /**
   * Applies a function `f` to each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(f: Iterator[T] => Unit): Unit = withNewRDDExecutionId {
    rdd.foreachPartition(f)
  }

  /**
   * (Java-specific)
   * Runs `func` on each partition of this Dataset.
   *
   * @group action
   * @since 1.6.0
   */
  def foreachPartition(func: ForeachPartitionFunction[T]): Unit = {
    foreachPartition((it: Iterator[T]) => func.call(it.asJava))
  }

  /**
   * Returns the first `n` rows in the Dataset.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def take(n: Int): Array[T] = head(n)

  /**
   * Returns the first `n` rows in the Dataset as a list.
   *
   * Running take requires moving data into the application's driver process, and doing so with
   * a very large `n` can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def takeAsList(n: Int): java.util.List[T] = java.util.Arrays.asList(take(n) : _*)

  /**
   * Returns an array that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * For Java API, use [[collectAsList]].
   *
   * @group action
   * @since 1.6.0
   */
  def collect(): Array[T] = withAction("collect", queryExecution)(collectFromPlan)

  /**
   * Returns a Java list that contains all rows in this Dataset.
   *
   * Running collect requires moving all the data into the application's driver process, and
   * doing so on a very large dataset can crash the driver process with OutOfMemoryError.
   *
   * @group action
   * @since 1.6.0
   */
  def collectAsList(): java.util.List[T] = withAction("collectAsList", queryExecution) { plan =>
    val values = collectFromPlan(plan)
    java.util.Arrays.asList(values : _*)
  }

  /**
   * Returns an iterator that contains all rows in this Dataset.
   *
   * The iterator will consume as much memory as the largest partition in this Dataset.
   *
   * @note this results in multiple Spark jobs, and if the input Dataset is the result
   * of a wide transformation (e.g. join with different partitioners), to avoid
   * recomputing the input Dataset should be cached first.
   *
   * @group action
   * @since 2.0.0
   */
  def toLocalIterator(): java.util.Iterator[T] = {
    withAction("toLocalIterator", queryExecution) { plan =>
      // This projection writes output to a `InternalRow`, which means applying this projection is
      // not thread-safe. Here we create the projection inside this method to make `Dataset`
      // thread-safe.
      val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
      plan.executeToIterator().map { row =>
        // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
        // parameter of its `get` method, so it's safe to use null here.
        objProj(row).get(0, null).asInstanceOf[T]
      }.asJava
    }
  }

  /**
   * Returns the number of rows in the Dataset.
   * @group action
   * @since 1.6.0
   */
  def count(): Long = withAction("count", groupBy().count().queryExecution) { plan =>
    plan.executeCollect().head.getLong(0)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions.
   *
   * @group typedrel
   * @since 1.6.0
   */
  def repartition(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = true, logicalPlan)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    // The underlying `LogicalPlan` operator special-cases all-`SortOrder` arguments.
    // However, we don't want to complicate the semantics of this API method.
    // Instead, let's give users a friendly error message, pointing them to the new method.
    val sortOrders = partitionExprs.filter(_.expr.isInstanceOf[SortOrder])
    if (sortOrders.nonEmpty) throw new IllegalArgumentException(
      s"""Invalid partitionExprs specified: $sortOrders
         |For range partitioning use repartitionByRange(...) instead.
       """.stripMargin)
    withTypedPlan {
      RepartitionByExpression(partitionExprs.map(_.expr), logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is hash partitioned.
   *
   * This is the same operation as "DISTRIBUTE BY" in SQL (Hive QL).
   *
   * @group typedrel
   * @since 2.0.0
   */
  @scala.annotation.varargs
  def repartition(partitionExprs: Column*): Dataset[T] = {
    repartition(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions into
   * `numPartitions`. The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(numPartitions: Int, partitionExprs: Column*): Dataset[T] = {
    require(partitionExprs.nonEmpty, "At least one partition-by expression must be specified.")
    val sortOrder: Seq[SortOrder] = partitionExprs.map(_.expr match {
      case expr: SortOrder => expr
      case expr: Expression => SortOrder(expr, Ascending)
    })
    withTypedPlan {
      RepartitionByExpression(sortOrder, logicalPlan, numPartitions)
    }
  }

  /**
   * Returns a new Dataset partitioned by the given partitioning expressions, using
   * `spark.sql.shuffle.partitions` as number of partitions.
   * The resulting Dataset is range partitioned.
   *
   * At least one partition-by expression must be specified.
   * When no explicit sort order is specified, "ascending nulls first" is assumed.
   * Note, the rows are not sorted in each partition of the resulting Dataset.
   *
   * @group typedrel
   * @since 2.3.0
   */
  @scala.annotation.varargs
  def repartitionByRange(partitionExprs: Column*): Dataset[T] = {
    repartitionByRange(sparkSession.sessionState.conf.numShufflePartitions, partitionExprs: _*)
  }

  /**
   * Returns a new Dataset that has exactly `numPartitions` partitions, when the fewer partitions
   * are requested. If a larger number of partitions is requested, it will stay at the current
   * number of partitions. Similar to coalesce defined on an `RDD`, this operation results in
   * a narrow dependency, e.g. if you go from 1000 partitions to 100 partitions, there will not
   * be a shuffle, instead each of the 100 new partitions will claim 10 of the current partitions.
   *
   * However, if you're doing a drastic coalesce, e.g. to numPartitions = 1,
   * this may result in your computation taking place on fewer nodes than
   * you like (e.g. one node in the case of numPartitions = 1). To avoid this,
   * you can call repartition. This will add a shuffle step, but means the
   * current upstream partitions will be executed in parallel (per whatever
   * the current partitioning is).
   *
   * @group typedrel
   * @since 1.6.0
   */
  def coalesce(numPartitions: Int): Dataset[T] = withTypedPlan {
    Repartition(numPartitions, shuffle = false, logicalPlan)
  }

  /**
   * Returns a new Dataset that contains only the unique rows from this Dataset.
   * This is an alias for `dropDuplicates`.
   *
   * @note Equality checking is performed directly on the encoded representation of the data
   * and thus is not affected by a custom `equals` function defined on `T`.
   *
   * @group typedrel
   * @since 2.0.0
   */
  def distinct(): Dataset[T] = dropDuplicates()

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this)
    this
  }

  /**
   * Persist this Dataset with the default storage level (`MEMORY_AND_DISK`).
   *
   * @group basic
   * @since 1.6.0
   */
  def cache(): this.type = persist()

  /**
   * Persist this Dataset with the given storage level.
   * @param newLevel One of: `MEMORY_ONLY`, `MEMORY_AND_DISK`, `MEMORY_ONLY_SER`,
   *                 `MEMORY_AND_DISK_SER`, `DISK_ONLY`, `MEMORY_ONLY_2`,
   *                 `MEMORY_AND_DISK_2`, etc.
   *
   * @group basic
   * @since 1.6.0
   */
  def persist(newLevel: StorageLevel): this.type = {
    sparkSession.sharedState.cacheManager.cacheQuery(this, None, newLevel)
    this
  }

  /**
   * Get the Dataset's current storage level, or StorageLevel.NONE if not persisted.
   *
   * @group basic
   * @since 2.1.0
   */
  def storageLevel: StorageLevel = {
    sparkSession.sharedState.cacheManager.lookupCachedData(this).map { cachedData =>
      cachedData.cachedRepresentation.cacheBuilder.storageLevel
    }.getOrElse(StorageLevel.NONE)
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @param blocking Whether to block until all blocks are deleted.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(blocking: Boolean): this.type = {
    sparkSession.sharedState.cacheManager.uncacheQuery(this, cascade = false, blocking)
    this
  }

  /**
   * Mark the Dataset as non-persistent, and remove all blocks for it from memory and disk.
   * This will not un-persist any cached data that is built upon this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  def unpersist(): this.type = unpersist(blocking = false)

  // Represents the `QueryExecution` used to produce the content of the Dataset as an `RDD`.
  @transient private lazy val rddQueryExecution: QueryExecution = {
    val deserialized = CatalystSerde.deserialize[T](logicalPlan)
    sparkSession.sessionState.executePlan(deserialized)
  }

  /**
   * Represents the content of the Dataset as an `RDD` of `T`.
   *
   * @group basic
   * @since 1.6.0
   */
  lazy val rdd: RDD[T] = {
    val objectType = exprEnc.deserializer.dataType
    rddQueryExecution.toRdd.mapPartitions { rows =>
      rows.map(_.get(0, objectType).asInstanceOf[T])
    }
  }

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def toJavaRDD: JavaRDD[T] = rdd.toJavaRDD()

  /**
   * Returns the content of the Dataset as a `JavaRDD` of `T`s.
   * @group basic
   * @since 1.6.0
   */
  def javaRDD: JavaRDD[T] = toJavaRDD

  /**
   * Registers this Dataset as a temporary table using the given name. The lifetime of this
   * temporary table is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 1.6.0
   */
  @deprecated("Use createOrReplaceTempView(viewName) instead.", "2.0.0")
  def registerTempTable(tableName: String): Unit = {
    createOrReplaceTempView(tableName)
  }

  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * Local temporary view is session-scoped. Its lifetime is the lifetime of the session that
   * created it, i.e. it will be automatically dropped when the session terminates. It's not
   * tied to any databases, i.e. we can't use `db1.view1` to reference a local temporary view.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.0.0
   */
  @throws[AnalysisException]
  def createTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = false)
  }



  /**
   * Creates a local temporary view using the given name. The lifetime of this
   * temporary view is tied to the [[SparkSession]] that was used to create this Dataset.
   *
   * @group basic
   * @since 2.0.0
   */
  def createOrReplaceTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = false)
  }

  /**
   * Creates a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @throws AnalysisException if the view name is invalid or already exists
   *
   * @group basic
   * @since 2.1.0
   */
  @throws[AnalysisException]
  def createGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = false, global = true)
  }

  /**
   * Creates or replaces a global temporary view using the given name. The lifetime of this
   * temporary view is tied to this Spark application.
   *
   * Global temporary view is cross-session. Its lifetime is the lifetime of the Spark application,
   * i.e. it will be automatically dropped when the application terminates. It's tied to a system
   * preserved database `global_temp`, and we must use the qualified name to refer a global temp
   * view, e.g. `SELECT * FROM global_temp.view1`.
   *
   * @group basic
   * @since 2.2.0
   */
  def createOrReplaceGlobalTempView(viewName: String): Unit = withPlan {
    createTempViewCommand(viewName, replace = true, global = true)
  }

  private def createTempViewCommand(
      viewName: String,
      replace: Boolean,
      global: Boolean): CreateViewCommand = {
    val viewType = if (global) GlobalTempView else LocalTempView

    val tableIdentifier = try {
      sparkSession.sessionState.sqlParser.parseTableIdentifier(viewName)
    } catch {
      case _: ParseException => throw new AnalysisException(s"Invalid view name: $viewName")
    }
    CreateViewCommand(
      name = tableIdentifier,
      userSpecifiedColumns = Nil,
      comment = None,
      properties = Map.empty,
      originalText = None,
      child = logicalPlan,
      allowExisting = false,
      replace = replace,
      viewType = viewType)
  }

  /**
   * Interface for saving the content of the non-streaming Dataset out into external storage.
   *
   * @group basic
   * @since 1.6.0
   */
  def write: DataFrameWriter[T] = {
    if (isStreaming) {
      logicalPlan.failAnalysis(
        "'write' can not be called on streaming Dataset/DataFrame")
    }
    new DataFrameWriter[T](this)
  }

  /**
   * Interface for saving the content of the streaming Dataset out into external storage.
   *
   * @group basic
   * @since 2.0.0
   */
  @InterfaceStability.Evolving
  def writeStream: DataStreamWriter[T] = {
    if (!isStreaming) {
      logicalPlan.failAnalysis(
        "'writeStream' can be called only on streaming Dataset/DataFrame")
    }
    new DataStreamWriter[T](this)
  }


  /**
   * Returns the content of the Dataset as a Dataset of JSON strings.
   * @since 2.0.0
   */
  def toJSON: Dataset[String] = {
    val rowSchema = this.schema
    val sessionLocalTimeZone = sparkSession.sessionState.conf.sessionLocalTimeZone
    mapPartitions { iter =>
      val writer = new CharArrayWriter()
      // create the Generator without separator inserted between 2 records
      val gen = new JacksonGenerator(rowSchema, writer,
        new JSONOptions(Map.empty[String, String], sessionLocalTimeZone))

      new Iterator[String] {
        override def hasNext: Boolean = iter.hasNext
        override def next(): String = {
          gen.write(exprEnc.toRow(iter.next()))
          gen.flush()

          val json = writer.toString
          if (hasNext) {
            writer.reset()
          } else {
            gen.close()
          }

          json
        }
      }
    } (Encoders.STRING)
  }

  /**
   * Returns a best-effort snapshot of the files that compose this Dataset. This method simply
   * asks each constituent BaseRelation for its respective files and takes the union of all results.
   * Depending on the source relations, this may not find all input files. Duplicates are removed.
   *
   * @group basic
   * @since 2.0.0
   */
  def inputFiles: Array[String] = {
    val files: Seq[String] = queryExecution.optimizedPlan.collect {
      case LogicalRelation(fsBasedRelation: FileRelation, _, _, _) =>
        fsBasedRelation.inputFiles
      case fr: FileRelation =>
        fr.inputFiles
      case r: HiveTableRelation =>
        r.tableMeta.storage.locationUri.map(_.toString).toArray
    }.flatten
    files.toSet.toArray
  }

  ////////////////////////////////////////////////////////////////////////////
  // For Python API
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Converts a JavaRDD to a PythonRDD.
   */
  private[sql] def javaToPython: JavaRDD[Array[Byte]] = {
    val structType = schema  // capture it for closure
    val rdd = queryExecution.toRdd.map(EvaluatePython.toJava(_, structType))
    EvaluatePython.javaToPython(rdd)
  }

  private[sql] def collectToPython(): Array[Any] = {
    EvaluatePython.registerPicklers()
    withAction("collectToPython", queryExecution) { plan =>
      val toJava: (Any) => Any = EvaluatePython.toJava(_, schema)
      val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
        plan.executeCollect().iterator.map(toJava))
      PythonRDD.serveIterator(iter, "serve-DataFrame")
    }
  }

  private[sql] def getRowsToPython(
      _numRows: Int,
      truncate: Int): Array[Any] = {
    EvaluatePython.registerPicklers()
    val numRows = _numRows.max(0).min(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH - 1)
    val rows = getRows(numRows, truncate).map(_.toArray).toArray
    val toJava: (Any) => Any = EvaluatePython.toJava(_, ArrayType(ArrayType(StringType)))
    val iter: Iterator[Array[Byte]] = new SerDeUtil.AutoBatchedPickler(
      rows.iterator.map(toJava))
    PythonRDD.serveIterator(iter, "serve-GetRows")
  }

  /**
   * Collect a Dataset as Arrow batches and serve stream to PySpark.
   */
  private[sql] def collectAsArrowToPython(): Array[Any] = {
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone

    PythonRDD.serveToStreamWithSync("serve-Arrow") { out =>
      withAction("collectAsArrowToPython", queryExecution) { plan =>
        val batchWriter = new ArrowBatchStreamWriter(schema, out, timeZoneId)
        val arrowBatchRdd = toArrowBatchRdd(plan)
        val numPartitions = arrowBatchRdd.partitions.length

        // Store collection results for worst case of 1 to N-1 partitions
        val results = new Array[Array[Array[Byte]]](Math.max(0, numPartitions - 1))
        var lastIndex = -1  // index of last partition written

        // Handler to eagerly write partitions to Python in order
        def handlePartitionBatches(index: Int, arrowBatches: Array[Array[Byte]]): Unit = {
          // If result is from next partition in order
          if (index - 1 == lastIndex) {
            batchWriter.writeBatches(arrowBatches.iterator)
            lastIndex += 1
            // Write stored partitions that come next in order
            while (lastIndex < results.length && results(lastIndex) != null) {
              batchWriter.writeBatches(results(lastIndex).iterator)
              results(lastIndex) = null
              lastIndex += 1
            }
            // After last batch, end the stream
            if (lastIndex == results.length) {
              batchWriter.end()
            }
          } else {
            // Store partitions received out of order
            results(index - 1) = arrowBatches
          }
        }

        sparkSession.sparkContext.runJob(
          arrowBatchRdd,
          (ctx: TaskContext, it: Iterator[Array[Byte]]) => it.toArray,
          0 until numPartitions,
          handlePartitionBatches)
      }
    }
  }

  private[sql] def toPythonIterator(): Array[Any] = {
    withNewExecutionId {
      PythonRDD.toLocalIteratorAndServe(javaToPython.rdd)
    }
  }

  ////////////////////////////////////////////////////////////////////////////
  // Private Helpers
  ////////////////////////////////////////////////////////////////////////////

  /**
   * Wrap a Dataset action to track all Spark jobs in the body so that we can connect them with
   * an execution.
   */
  private def withNewExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, queryExecution)(body)
  }

  /**
   * Wrap an action of the Dataset's RDD to track all Spark jobs in the body so that we can connect
   * them with an execution. Before performing the action, the metrics of the executed plan will be
   * reset.
   */
  private def withNewRDDExecutionId[U](body: => U): U = {
    SQLExecution.withNewExecutionId(sparkSession, rddQueryExecution) {
      rddQueryExecution.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      body
    }
  }

  /**
   * Wrap a Dataset action to track the QueryExecution and time cost, then report to the
   * user-registered callback functions.
   */
  private def withAction[U](name: String, qe: QueryExecution)(action: SparkPlan => U) = {
    try {
      qe.executedPlan.foreach { plan =>
        plan.resetMetrics()
      }
      val start = System.nanoTime()
      val result = SQLExecution.withNewExecutionId(sparkSession, qe) {
        action(qe.executedPlan)
      }
      val end = System.nanoTime()
      sparkSession.listenerManager.onSuccess(name, qe, end - start)
      result
    } catch {
      case e: Throwable =>
        sparkSession.listenerManager.onFailure(name, qe, e)
        throw e
    }
  }

  /**
   * Collect all elements from a spark plan.
   */
  private def collectFromPlan(plan: SparkPlan): Array[T] = {
    // This projection writes output to a `InternalRow`, which means applying this projection is not
    // thread-safe. Here we create the projection inside this method to make `Dataset` thread-safe.
    val objProj = GenerateSafeProjection.generate(deserializer :: Nil)
    plan.executeCollect().map { row =>
      // The row returned by SafeProjection is `SpecificInternalRow`, which ignore the data type
      // parameter of its `get` method, so it's safe to use null here.
      objProj(row).get(0, null).asInstanceOf[T]
    }
  }

  private def sortInternal(global: Boolean, sortExprs: Seq[Column]): Dataset[T] = {
    val sortOrder: Seq[SortOrder] = sortExprs.map { col =>
      col.expr match {
        case expr: SortOrder =>
          expr
        case expr: Expression =>
          SortOrder(expr, Ascending)
      }
    }
    withTypedPlan {
      Sort(sortOrder, global = global, logicalPlan)
    }
  }

  /** A convenient function to wrap a logical plan and produce a DataFrame. */
  @inline private def withPlan(logicalPlan: LogicalPlan): DataFrame = {
    Dataset.ofRows(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a logical plan and produce a Dataset. */
  @inline private def withTypedPlan[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    Dataset(sparkSession, logicalPlan)
  }

  /** A convenient function to wrap a set based logical plan and produce a Dataset. */
  @inline private def withSetOperator[U : Encoder](logicalPlan: LogicalPlan): Dataset[U] = {
    if (classTag.runtimeClass.isAssignableFrom(classOf[Row])) {
      // Set operators widen types (change the schema), so we cannot reuse the row encoder.
      Dataset.ofRows(sparkSession, logicalPlan).asInstanceOf[Dataset[U]]
    } else {
      Dataset(sparkSession, logicalPlan)
    }
  }

  /** Convert to an RDD of serialized ArrowRecordBatches. */
  private[sql] def toArrowBatchRdd(plan: SparkPlan): RDD[Array[Byte]] = {
    val schemaCaptured = this.schema
    val maxRecordsPerBatch = sparkSession.sessionState.conf.arrowMaxRecordsPerBatch
    val timeZoneId = sparkSession.sessionState.conf.sessionLocalTimeZone
    plan.execute().mapPartitionsInternal { iter =>
      val context = TaskContext.get()
      ArrowConverters.toBatchIterator(
        iter, schemaCaptured, maxRecordsPerBatch, timeZoneId, context)
    }
  }

  // This is only used in tests, for now.
  private[sql] def toArrowBatchRdd: RDD[Array[Byte]] = {
    toArrowBatchRdd(queryExecution.executedPlan)
  }
}

[0m2021.03.08 14:26:26 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:27 INFO  time: compiled root in 1.87s[0m
[0m2021.03.08 14:26:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:29 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 14:26:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:38 INFO  time: compiled root in 1.43s[0m
[0m2021.03.08 14:26:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:42 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 14:26:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:26:45 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 14:27:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:27:30 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 14:27:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:27:35 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 14:28:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:28:35 INFO  time: compiled root in 0.21s[0m
[0m2021.03.08 14:28:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:28:38 INFO  time: compiled root in 0.27s[0m
[0m2021.03.08 14:28:50 INFO  compiling root (1 scala source)[0m
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements.  See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the "License"); you may not use this file except in compliance with
 * the License.  You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

package org.apache.spark.sql

import scala.collection.JavaConverters._
import scala.language.implicitConversions

import org.apache.spark.annotation.InterfaceStability
import org.apache.spark.internal.Logging
import org.apache.spark.sql.catalyst.analysis._
import org.apache.spark.sql.catalyst.encoders.{encoderFor, ExpressionEncoder}
import org.apache.spark.sql.catalyst.expressions._
import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression
import org.apache.spark.sql.catalyst.parser.CatalystSqlParser
import org.apache.spark.sql.catalyst.util.toPrettySQL
import org.apache.spark.sql.execution.aggregate.TypedAggregateExpression
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.lit
import org.apache.spark.sql.types._

private[sql] object Column {

  def apply(colName: String): Column = new Column(colName)

  def apply(expr: Expression): Column = new Column(expr)

  def unapply(col: Column): Option[Expression] = Some(col.expr)

  private[sql] def generateAlias(e: Expression): String = {
    e match {
      case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
        a.aggregateFunction.toString
      case expr => toPrettySQL(expr)
    }
  }
}

/**
 * A [[Column]] where an [[Encoder]] has been given for the expected input and return type.
 * To create a [[TypedColumn]], use the `as` function on a [[Column]].
 *
 * @tparam T The input type expected for this expression.  Can be `Any` if the expression is type
 *           checked by the analyzer instead of the compiler (i.e. `expr("sum(...)")`).
 * @tparam U The output type of this column.
 *
 * @since 1.6.0
 */
@InterfaceStability.Stable
class TypedColumn[-T, U](
    expr: Expression,
    private[sql] val encoder: ExpressionEncoder[U])
  extends Column(expr) {

  /**
   * Inserts the specific input type and schema into any expressions that are expected to operate
   * on a decoded object.
   */
  private[sql] def withInputType(
      inputEncoder: ExpressionEncoder[_],
      inputAttributes: Seq[Attribute]): TypedColumn[T, U] = {
    val unresolvedDeserializer = UnresolvedDeserializer(inputEncoder.deserializer, inputAttributes)
    val newExpr = expr transform {
      case ta: TypedAggregateExpression if ta.inputDeserializer.isEmpty =>
        ta.withInputInfo(
          deser = unresolvedDeserializer,
          cls = inputEncoder.clsTag.runtimeClass,
          schema = inputEncoder.schema)
    }
    new TypedColumn[T, U](newExpr, encoder)
  }

  /**
   * Gives the [[TypedColumn]] a name (alias).
   * If the current `TypedColumn` has metadata associated with it, this metadata will be propagated
   * to the new column.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  override def name(alias: String): TypedColumn[T, U] =
    new TypedColumn[T, U](super.name(alias).expr, encoder)

}

/**
 * A column that will be computed based on the data in a `DataFrame`.
 *
 * A new column can be constructed based on the input columns present in a DataFrame:
 *
 * {{{
 *   df("columnName")            // On a specific `df` DataFrame.
 *   col("columnName")           // A generic column not yet associated with a DataFrame.
 *   col("columnName.field")     // Extracting a struct field
 *   col("`a.column.with.dots`") // Escape `.` in column names.
 *   $"columnName"               // Scala short hand for a named column.
 * }}}
 *
 * [[Column]] objects can be composed to form complex expressions:
 *
 * {{{
 *   $"a" + 1
 *   $"a" === $"b"
 * }}}
 *
 * @note The internal Catalyst expression can be accessed via [[expr]], but this method is for
 * debugging purposes only and can change in any future Spark releases.
 *
 * @groupname java_expr_ops Java-specific expression operators
 * @groupname expr_ops Expression operators
 * @groupname df_ops DataFrame functions
 * @groupname Ungrouped Support functions for DataFrames
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class Column(val expr: Expression) extends Logging {

  def this(name: String) = this(name match {
    case "*" => UnresolvedStar(None)
    case _ if name.endsWith(".*") =>
      val parts = UnresolvedAttribute.parseAttributeName(name.substring(0, name.length - 2))
      UnresolvedStar(Some(parts))
    case _ => UnresolvedAttribute.quotedString(name)
  })

  override def toString: String = toPrettySQL(expr)

  override def equals(that: Any): Boolean = that match {
    case that: Column => that.expr.equals(this.expr)
    case _ => false
  }

  override def hashCode: Int = this.expr.hashCode()

  /** Creates a column based on the given expression. */
  private def withExpr(newExpr: Expression): Column = new Column(newExpr)

  /**
   * Returns the expression for this column either with an existing or auto assigned name.
   */
  private[sql] def named: NamedExpression = expr match {
    // Wrap UnresolvedAttribute with UnresolvedAlias, as when we resolve UnresolvedAttribute, we
    // will remove intermediate Alias for ExtractValue chain, and we need to alias it again to
    // make it a NamedExpression.
    case u: UnresolvedAttribute => UnresolvedAlias(u)

    case u: UnresolvedExtractValue => UnresolvedAlias(u)

    case expr: NamedExpression => expr

    // Leave an unaliased generator with an empty list of names since the analyzer will generate
    // the correct defaults after the nested expression's type has been resolved.
    case g: Generator => MultiAlias(g, Nil)

    case func: UnresolvedFunction => UnresolvedAlias(func, Some(Column.generateAlias))

    // If we have a top level Cast, there is a chance to give it a better alias, if there is a
    // NamedExpression under this Cast.
    case c: Cast =>
      c.transformUp {
        case c @ Cast(_: NamedExpression, _, _) => UnresolvedAlias(c)
      } match {
        case ne: NamedExpression => ne
        case _ => Alias(expr, toPrettySQL(expr))()
      }

    case a: AggregateExpression if a.aggregateFunction.isInstanceOf[TypedAggregateExpression] =>
      UnresolvedAlias(a, Some(Column.generateAlias))

    // Wait until the struct is resolved. This will generate a nicer looking alias.
    case struct: CreateNamedStructLike => UnresolvedAlias(struct)

    case expr: Expression => Alias(expr, toPrettySQL(expr))()
  }

  /**
   * Provides a type hint about the expected return value of this column.  This information can
   * be used by operations such as `select` on a [[Dataset]] to automatically convert the
   * results into the correct JVM types.
   * @since 1.6.0
   */
  def as[U : Encoder]: TypedColumn[Any, U] = new TypedColumn[Any, U](expr, encoderFor[U])

  /**
   * Extracts a value or values from a complex type.
   * The following types of extraction are supported:
   * <ul>
   * <li>Given an Array, an integer ordinal can be used to retrieve a single value.</li>
   * <li>Given a Map, a key of the correct type can be used to retrieve an individual value.</li>
   * <li>Given a Struct, a string fieldName can be used to extract that field.</li>
   * <li>Given an Array of Structs, a string fieldName can be used to extract filed
   *    of every struct in that array, and return an Array of fields.</li>
   * </ul>
   * @group expr_ops
   * @since 1.4.0
   */
  def apply(extraction: Any): Column = withExpr {
    UnresolvedExtractValue(expr, lit(extraction).expr)
  }

  /**
   * Unary minus, i.e. negate the expression.
   * {{{
   *   // Scala: select the amount column and negates all values.
   *   df.select( -df("amount") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.select( negate(col("amount") );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_- : Column = withExpr { UnaryMinus(expr) }

  /**
   * Inversion of boolean expression, i.e. NOT.
   * {{{
   *   // Scala: select rows that are not active (isActive === false)
   *   df.filter( !df("isActive") )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( not(df.col("isActive")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def unary_! : Column = withExpr { Not(expr) }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def === (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} = $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualTo(expr, right)
  }

  /**
   * Equality test.
   * {{{
   *   // Scala:
   *   df.filter( df("colA") === df("colB") )
   *
   *   // Java
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").equalTo(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def equalTo(other: Any): Column = this === other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") =!= df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
    */
  def =!= (other: Any): Column = withExpr{ Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
    */
  @deprecated("!== does not have the same precedence as ===, use =!= instead", "2.0.0")
  def !== (other: Any): Column = this =!= other

  /**
   * Inequality test.
   * {{{
   *   // Scala:
   *   df.select( df("colA") !== df("colB") )
   *   df.select( !(df("colA") === df("colB")) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   df.filter( col("colA").notEqual(col("colB")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def notEqual(other: Any): Column = withExpr { Not(EqualTo(expr, lit(other).expr)) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > 21 )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def > (other: Any): Column = withExpr { GreaterThan(expr, lit(other).expr) }

  /**
   * Greater than.
   * {{{
   *   // Scala: The following selects people older than 21.
   *   people.select( people("age") > lit(21) )
   *
   *   // Java:
   *   import static org.apache.spark.sql.functions.*;
   *   people.select( people.col("age").gt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def gt(other: Any): Column = this > other

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def < (other: Any): Column = withExpr { LessThan(expr, lit(other).expr) }

  /**
   * Less than.
   * {{{
   *   // Scala: The following selects people younger than 21.
   *   people.select( people("age") < 21 )
   *
   *   // Java:
   *   people.select( people.col("age").lt(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def lt(other: Any): Column = this < other

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <= (other: Any): Column = withExpr { LessThanOrEqual(expr, lit(other).expr) }

  /**
   * Less than or equal to.
   * {{{
   *   // Scala: The following selects people age 21 or younger than 21.
   *   people.select( people("age") <= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").leq(21) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def leq(other: Any): Column = this <= other

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def >= (other: Any): Column = withExpr { GreaterThanOrEqual(expr, lit(other).expr) }

  /**
   * Greater than or equal to an expression.
   * {{{
   *   // Scala: The following selects people age 21 or older than 21.
   *   people.select( people("age") >= 21 )
   *
   *   // Java:
   *   people.select( people.col("age").geq(21) )
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def geq(other: Any): Column = this >= other

  /**
   * Equality test that is safe for null values.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def <=> (other: Any): Column = withExpr {
    val right = lit(other).expr
    if (this.expr == right) {
      logWarning(
        s"Constructing trivially true equals predicate, '${this.expr} <=> $right'. " +
          "Perhaps you need to use aliases.")
    }
    EqualNullSafe(expr, right)
  }

  /**
   * Equality test that is safe for null values.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def eqNullSafe(other: Any): Column = this <=> other

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def when(condition: Column, value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches :+ ((condition.expr, lit(value).expr))) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "when() cannot be applied once otherwise() is applied")
    case _ =>
      throw new IllegalArgumentException(
        "when() can only be applied on a Column previously generated by when() function")
  }

  /**
   * Evaluates a list of conditions and returns one of multiple possible result expressions.
   * If otherwise is not defined at the end, null is returned for unmatched conditions.
   *
   * {{{
   *   // Example: encoding gender string column into integer.
   *
   *   // Scala:
   *   people.select(when(people("gender") === "male", 0)
   *     .when(people("gender") === "female", 1)
   *     .otherwise(2))
   *
   *   // Java:
   *   people.select(when(col("gender").equalTo("male"), 0)
   *     .when(col("gender").equalTo("female"), 1)
   *     .otherwise(2))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def otherwise(value: Any): Column = this.expr match {
    case CaseWhen(branches, None) =>
      withExpr { CaseWhen(branches, Option(lit(value).expr)) }
    case CaseWhen(branches, Some(_)) =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied once on a Column previously generated by when()")
    case _ =>
      throw new IllegalArgumentException(
        "otherwise() can only be applied on a Column previously generated by when()")
  }

  /**
   * True if the current column is between the lower bound and upper bound, inclusive.
   *
   * @group java_expr_ops
   * @since 1.4.0
   */
  def between(lowerBound: Any, upperBound: Any): Column = {
    (this >= lowerBound) && (this <= upperBound)
  }

  /**
   * True if the current expression is NaN.
   *
   * @group expr_ops
   * @since 1.5.0
   */
  def isNaN: Column = withExpr { IsNaN(expr) }

  /**
   * True if the current expression is null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNull: Column = withExpr { IsNull(expr) }

  /**
   * True if the current expression is NOT null.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def isNotNull: Column = withExpr { IsNotNull(expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def || (other: Any): Column = withExpr { Or(expr, lit(other).expr) }

  /**
   * Boolean OR.
   * {{{
   *   // Scala: The following selects people that are in school or employed.
   *   people.filter( people("inSchool") || people("isEmployed") )
   *
   *   // Java:
   *   people.filter( people.col("inSchool").or(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def or(other: Column): Column = this || other

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def && (other: Any): Column = withExpr { And(expr, lit(other).expr) }

  /**
   * Boolean AND.
   * {{{
   *   // Scala: The following selects people that are in school and employed at the same time.
   *   people.select( people("inSchool") && people("isEmployed") )
   *
   *   // Java:
   *   people.select( people.col("inSchool").and(people.col("isEmployed")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def and(other: Column): Column = this && other

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def + (other: Any): Column = withExpr { Add(expr, lit(other).expr) }

  /**
   * Sum of this expression and another expression.
   * {{{
   *   // Scala: The following selects the sum of a person's height and weight.
   *   people.select( people("height") + people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").plus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def plus(other: Any): Column = this + other

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def - (other: Any): Column = withExpr { Subtract(expr, lit(other).expr) }

  /**
   * Subtraction. Subtract the other expression from this expression.
   * {{{
   *   // Scala: The following selects the difference between people's height and their weight.
   *   people.select( people("height") - people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").minus(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def minus(other: Any): Column = this - other

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def * (other: Any): Column = withExpr { Multiply(expr, lit(other).expr) }

  /**
   * Multiplication of this expression and another expression.
   * {{{
   *   // Scala: The following multiplies a person's height by their weight.
   *   people.select( people("height") * people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").multiply(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def multiply(other: Any): Column = this * other

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def / (other: Any): Column = withExpr { Divide(expr, lit(other).expr) }

  /**
   * Division this expression by another expression.
   * {{{
   *   // Scala: The following divides a person's height by their weight.
   *   people.select( people("height") / people("weight") )
   *
   *   // Java:
   *   people.select( people.col("height").divide(people.col("weight")) );
   * }}}
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def divide(other: Any): Column = this / other

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def % (other: Any): Column = withExpr { Remainder(expr, lit(other).expr) }

  /**
   * Modulo (a.k.a. remainder) expression.
   *
   * @group java_expr_ops
   * @since 1.3.0
   */
  def mod(other: Any): Column = this % other

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the evaluated values of the arguments.
   *
   * Note: Since the type of the elements in the list are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 1.5.0
   */
  @scala.annotation.varargs
  def isin(list: Any*): Column = withExpr { In(expr, list.map(lit(_).expr)) }

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: scala.collection.Iterable[_]): Column = isin(values.toSeq: _*)

  /**
   * A boolean expression that is evaluated to true if the value of this expression is contained
   * by the provided collection.
   *
   * Note: Since the type of the elements in the collection are inferred only during the run time,
   * the elements will be "up-casted" to the most common type for comparison.
   * For eg:
   *   1) In the case of "Int vs String", the "Int" will be up-casted to "String" and the
   * comparison will look like "String vs String".
   *   2) In the case of "Float vs Double", the "Float" will be up-casted to "Double" and the
   * comparison will look like "Double vs Double"
   *
   * @group java_expr_ops
   * @since 2.4.0
   */
  def isInCollection(values: java.lang.Iterable[_]): Column = isInCollection(values.asScala)

  /**
   * SQL like expression. Returns a boolean column based on a SQL LIKE match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def like(literal: String): Column = withExpr { Like(expr, lit(literal).expr) }

  /**
   * SQL RLIKE expression (LIKE with Regex). Returns a boolean column based on a regex
   * match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def rlike(literal: String): Column = withExpr { RLike(expr, lit(literal).expr) }

  /**
   * An expression that gets an item at position `ordinal` out of an array,
   * or gets a value by key `key` in a `MapType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getItem(key: Any): Column = withExpr { UnresolvedExtractValue(expr, Literal(key)) }

  /**
   * An expression that gets a field by name in a `StructType`.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def getField(fieldName: String): Column = withExpr {
    UnresolvedExtractValue(expr, Literal(fieldName))
  }

  /**
   * An expression that returns a substring.
   * @param startPos expression for the starting position.
   * @param len expression for the length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Column, len: Column): Column = withExpr {
    Substring(expr, startPos.expr, len.expr)
  }

  /**
   * An expression that returns a substring.
   * @param startPos starting position.
   * @param len length of the substring.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def substr(startPos: Int, len: Int): Column = withExpr {
    Substring(expr, lit(startPos).expr, lit(len).expr)
  }

  /**
   * Contains the other element. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def contains(other: Any): Column = withExpr { Contains(expr, lit(other).expr) }

  /**
   * String starts with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(other: Column): Column = withExpr { StartsWith(expr, lit(other).expr) }

  /**
   * String starts with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def startsWith(literal: String): Column = this.startsWith(lit(literal))

  /**
   * String ends with. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(other: Column): Column = withExpr { EndsWith(expr, lit(other).expr) }

  /**
   * String ends with another string literal. Returns a boolean column based on a string match.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def endsWith(literal: String): Column = this.endsWith(lit(literal))

  /**
   * Gives the column an alias. Same as `as`.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".alias("colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def alias(alias: String): Column = name(alias)

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String): Column = name(alias)

  /**
   * (Scala-specific) Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Seq[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Assigns the given aliases to the results of a table generating function.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select(explode($"myMap").as("key" :: "value" :: Nil))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def as(aliases: Array[String]): Column = withExpr { MultiAlias(expr, aliases) }

  /**
   * Gives the column an alias.
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".as('colB))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: Symbol): Column = name(alias.name)

  /**
   * Gives the column an alias with metadata.
   * {{{
   *   val metadata: Metadata = ...
   *   df.select($"colA".as("colB", metadata))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def as(alias: String, metadata: Metadata): Column = withExpr {
    Alias(expr, alias)(explicitMetadata = Some(metadata))
  }

  /**
   * Gives the column a name (alias).
   * {{{
   *   // Renames colA to colB in select output.
   *   df.select($"colA".name("colB"))
   * }}}
   *
   * If the current column has metadata associated with it, this metadata will be propagated
   * to the new column.  If this not desired, use `as` with explicitly empty metadata.
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def name(alias: String): Column = withExpr {
    expr match {
      case ne: NamedExpression => Alias(expr, alias)(explicitMetadata = Some(ne.metadata))
      case other => Alias(other, alias)()
    }
  }

  /**
   * Casts the column to a different data type.
   * {{{
   *   // Casts colA to IntegerType.
   *   import org.apache.spark.sql.types.IntegerType
   *   df.select(df("colA").cast(IntegerType))
   *
   *   // equivalent to
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: DataType): Column = withExpr { Cast(expr, to) }

  /**
   * Casts the column to a different data type, using the canonical string representation
   * of the type. The supported types are: `string`, `boolean`, `byte`, `short`, `int`, `long`,
   * `float`, `double`, `decimal`, `date`, `timestamp`.
   * {{{
   *   // Casts colA to integer.
   *   df.select(df("colA").cast("int"))
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def cast(to: String): Column = cast(CatalystSqlParser.parseDataType(to))

  /**
   * Returns a sort expression based on the descending order of the column.
   * {{{
   *   // Scala
   *   df.sort(df("age").desc)
   *
   *   // Java
   *   df.sort(df.col("age").desc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def desc: Column = withExpr { SortOrder(expr, Descending) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing first.
   *   df.sort(df("age").desc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_first: Column = withExpr { SortOrder(expr, Descending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on the descending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in descending order and null values appearing last.
   *   df.sort(df("age").desc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").desc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def desc_nulls_last: Column = withExpr { SortOrder(expr, Descending, NullsLast, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order.
   *   df.sort(df("age").asc)
   *
   *   // Java
   *   df.sort(df.col("age").asc());
   * }}}
   *
   * @group expr_ops
   * @since 1.3.0
   */
  def asc: Column = withExpr { SortOrder(expr, Ascending) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values return before non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing first.
   *   df.sort(df("age").asc_nulls_first)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_first());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_first: Column = withExpr { SortOrder(expr, Ascending, NullsFirst, Set.empty) }

  /**
   * Returns a sort expression based on ascending order of the column,
   * and null values appear after non-null values.
   * {{{
   *   // Scala: sort a DataFrame by age column in ascending order and null values appearing last.
   *   df.sort(df("age").asc_nulls_last)
   *
   *   // Java
   *   df.sort(df.col("age").asc_nulls_last());
   * }}}
   *
   * @group expr_ops
   * @since 2.1.0
   */
  def asc_nulls_last: Column = withExpr { SortOrder(expr, Ascending, NullsLast, Set.empty) }

  /**
   * Prints the expression to the console for debugging purposes.
   *
   * @group df_ops
   * @since 1.3.0
   */
  def explain(extended: Boolean): Unit = {
    // scalastyle:off println
    if (extended) {
      println(expr)
    } else {
      println(expr.sql)
    }
    // scalastyle:on println
  }

  /**
   * Compute bitwise OR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseOR(other: Any): Column = withExpr { BitwiseOr(expr, lit(other).expr) }

  /**
   * Compute bitwise AND of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseAND($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseAND(other: Any): Column = withExpr { BitwiseAnd(expr, lit(other).expr) }

  /**
   * Compute bitwise XOR of this expression with another expression.
   * {{{
   *   df.select($"colA".bitwiseXOR($"colB"))
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def bitwiseXOR(other: Any): Column = withExpr { BitwiseXor(expr, lit(other).expr) }

  /**
   * Defines a windowing column.
   *
   * {{{
   *   val w = Window.partitionBy("name").orderBy("id")
   *   df.select(
   *     sum("price").over(w.rangeBetween(Window.unboundedPreceding, 2)),
   *     avg("price").over(w.rowsBetween(Window.currentRow, 4))
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 1.4.0
   */
  def over(window: expressions.WindowSpec): Column = window.withAggregate(this)

  /**
   * Defines an empty analytic clause. In this case the analytic function is applied
   * and presented for all rows in the result set.
   *
   * {{{
   *   df.select(
   *     sum("price").over(),
   *     avg("price").over()
   *   )
   * }}}
   *
   * @group expr_ops
   * @since 2.0.0
   */
  def over(): Column = over(Window.spec)

}


/**
 * A convenient class used for constructing schema.
 *
 * @since 1.3.0
 */
@InterfaceStability.Stable
class ColumnName(name: String) extends Column(name) {

  /**
   * Creates a new `StructField` of type boolean.
   * @since 1.3.0
   */
  def boolean: StructField = StructField(name, BooleanType)

  /**
   * Creates a new `StructField` of type byte.
   * @since 1.3.0
   */
  def byte: StructField = StructField(name, ByteType)

  /**
   * Creates a new `StructField` of type short.
   * @since 1.3.0
   */
  def short: StructField = StructField(name, ShortType)

  /**
   * Creates a new `StructField` of type int.
   * @since 1.3.0
   */
  def int: StructField = StructField(name, IntegerType)

  /**
   * Creates a new `StructField` of type long.
   * @since 1.3.0
   */
  def long: StructField = StructField(name, LongType)

  /**
   * Creates a new `StructField` of type float.
   * @since 1.3.0
   */
  def float: StructField = StructField(name, FloatType)

  /**
   * Creates a new `StructField` of type double.
   * @since 1.3.0
   */
  def double: StructField = StructField(name, DoubleType)

  /**
   * Creates a new `StructField` of type string.
   * @since 1.3.0
   */
  def string: StructField = StructField(name, StringType)

  /**
   * Creates a new `StructField` of type date.
   * @since 1.3.0
   */
  def date: StructField = StructField(name, DateType)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal: StructField = StructField(name, DecimalType.USER_DEFAULT)

  /**
   * Creates a new `StructField` of type decimal.
   * @since 1.3.0
   */
  def decimal(precision: Int, scale: Int): StructField =
    StructField(name, DecimalType(precision, scale))

  /**
   * Creates a new `StructField` of type timestamp.
   * @since 1.3.0
   */
  def timestamp: StructField = StructField(name, TimestampType)

  /**
   * Creates a new `StructField` of type binary.
   * @since 1.3.0
   */
  def binary: StructField = StructField(name, BinaryType)

  /**
   * Creates a new `StructField` of type array.
   * @since 1.3.0
   */
  def array(dataType: DataType): StructField = StructField(name, ArrayType(dataType))

  /**
   * Creates a new `StructField` of type map.
   * @since 1.3.0
   */
  def map(keyType: DataType, valueType: DataType): StructField =
    map(MapType(keyType, valueType))

  def map(mapType: MapType): StructField = StructField(name, mapType)

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(fields: StructField*): StructField = struct(StructType(fields))

  /**
   * Creates a new `StructField` of type struct.
   * @since 1.3.0
   */
  def struct(structType: StructType): StructField = StructField(name, structType)
}

[0m2021.03.08 14:28:52 INFO  time: compiled root in 1.64s[0m
[0m2021.03.08 14:29:00 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:01 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 14:29:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:03 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 14:29:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:12 INFO  time: compiled root in 1.54s[0m
[0m2021.03.08 14:29:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:13 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 14:29:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:48 INFO  time: compiled root in 0.11s[0m
[0m2021.03.08 14:29:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:29:54 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 14:30:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:15 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 14:30:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:22 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 14:30:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:28 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 14:30:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:32 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 14:30:35 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:36 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 14:30:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:42 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 14:30:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:30:46 INFO  time: compiled root in 1.4s[0m
[0m2021.03.08 14:31:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:31:55 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 14:31:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:00 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 14:32:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:27 INFO  time: compiled root in 1.52s[0m
[0m2021.03.08 14:32:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:28 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 14:32:34 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:34 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 14:32:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:36 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 14:32:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:47 INFO  time: compiled root in 0.24s[0m
[0m2021.03.08 14:32:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:32:54 INFO  time: compiled root in 0.26s[0m
[0m2021.03.08 14:33:01 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:33:01 INFO  time: compiled root in 0.23s[0m
[0m2021.03.08 14:33:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:33:15 INFO  time: compiled root in 0.22s[0m
[0m2021.03.08 14:33:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:33:25 INFO  time: compiled root in 0.25s[0m
[0m2021.03.08 14:33:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:33:30 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 14:33:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:33:32 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 14:34:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:34:38 INFO  time: compiled root in 0.48s[0m
[0m2021.03.08 14:34:41 INFO  compiling root (1 scala source)[0m
Mar 08, 2021 2:34:42 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
[0m2021.03.08 14:34:42 INFO  time: compiled root in 1.45s[0m
[0m2021.03.08 14:34:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:34:46 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 14:36:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:36:46 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 14:36:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:36:52 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 14:37:07 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:37:08 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 14:37:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:37:30 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 14:37:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:37:38 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 14:37:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:37:42 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 14:37:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 14:37:45 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 15:57:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:13 INFO  time: compiled root in 3.88s[0m
[0m2021.03.08 15:57:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:29 INFO  time: compiled root in 4.61s[0m
[0m2021.03.08 15:57:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:41 INFO  time: compiled root in 1.51s[0m
[0m2021.03.08 15:57:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:45 INFO  time: compiled root in 2.77s[0m
[0m2021.03.08 15:57:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:55 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 15:57:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:57 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 15:57:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:57:58 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 15:58:20 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:21 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 15:58:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:25 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 15:58:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:33 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 15:58:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:37 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 15:58:40 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:41 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 15:58:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:58:54 INFO  time: compiled root in 1.07s[0m
[0m2021.03.08 15:58:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:00 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 15:59:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:04 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 15:59:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:12 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 15:59:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:14 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 15:59:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:16 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 15:59:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:19 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 15:59:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:24 INFO  time: compiled root in 1.09s[0m
[0m2021.03.08 15:59:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:28 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 15:59:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:33 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 15:59:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:45 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 15:59:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:50 INFO  time: compiled root in 1.14s[0m
[0m2021.03.08 15:59:52 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 15:59:53 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:00:02 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:03 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 16:00:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:05 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 16:00:10 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:11 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 16:00:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:15 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 16:00:17 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:18 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 16:00:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:22 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 16:00:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:25 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 16:00:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:27 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 16:00:28 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:29 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 16:00:29 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:31 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 16:00:38 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:39 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 16:00:45 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:46 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:00:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:48 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 16:00:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:55 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 16:00:58 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:00:59 INFO  time: compiled root in 1.17s[0m
[0m2021.03.08 16:01:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:14 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:01:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:26 INFO  time: compiled root in 1.18s[0m
[0m2021.03.08 16:01:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:28 INFO  time: compiled root in 1.28s[0m
[0m2021.03.08 16:01:32 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:33 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 16:01:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:38 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:01:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:44 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 16:01:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:52 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 16:01:55 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:01:57 INFO  time: compiled root in 1.8s[0m
[0m2021.03.08 16:02:03 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:07 INFO  time: compiled root in 3.06s[0m
[0m2021.03.08 16:02:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:09 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:02:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:16 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 16:02:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:20 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 16:02:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:23 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 16:02:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:25 INFO  time: compiled root in 1.2s[0m
[0m2021.03.08 16:02:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:38 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 16:02:42 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:43 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 16:02:47 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:48 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 16:02:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:51 INFO  time: compiled root in 1.16s[0m
[0m2021.03.08 16:02:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:02:55 INFO  time: compiled root in 1.02s[0m
[0m2021.03.08 16:03:12 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:13 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 16:03:13 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:14 INFO  time: compiled root in 1.34s[0m
[0m2021.03.08 16:03:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:19 INFO  time: compiled root in 1.06s[0m
[0m2021.03.08 16:03:21 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:22 INFO  time: compiled root in 1.22s[0m
[0m2021.03.08 16:03:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:24 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 16:03:27 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:28 INFO  time: compiled root in 1.05s[0m
[0m2021.03.08 16:03:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:33 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 16:03:36 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:37 INFO  time: compiled root in 1.08s[0m
[0m2021.03.08 16:03:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:40 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 16:03:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:49 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 16:03:57 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:03:58 INFO  time: compiled root in 1.13s[0m
[0m2021.03.08 16:04:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:09 INFO  time: compiled root in 1.23s[0m
[0m2021.03.08 16:04:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:10 INFO  time: compiled root in 1.1s[0m
[0m2021.03.08 16:04:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:15 INFO  time: compiled root in 1.28s[0m
[0m2021.03.08 16:04:31 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:32 INFO  time: compiled root in 1.11s[0m
[0m2021.03.08 16:04:39 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:40 INFO  time: compiled root in 1.19s[0m
[0m2021.03.08 16:04:48 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:04:49 INFO  time: compiled root in 1.12s[0m
[0m2021.03.08 16:06:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:06:55 INFO  time: compiled root in 1.32s[0m
[0m2021.03.08 16:07:04 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:05 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 16:07:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:09 INFO  time: compiled root in 1.21s[0m
[0m2021.03.08 16:07:09 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:11 INFO  time: compiled root in 1.31s[0m
[0m2021.03.08 16:07:11 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:13 INFO  time: compiled root in 1.25s[0m
[0m2021.03.08 16:07:15 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:17 INFO  time: compiled root in 1.24s[0m
[0m2021.03.08 16:07:19 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:20 INFO  time: compiled root in 1.3s[0m
[0m2021.03.08 16:07:24 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:25 INFO  time: compiled root in 1.15s[0m
[0m2021.03.08 16:07:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:42 INFO  time: compiled root in 1.26s[0m
[0m2021.03.08 16:07:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:47 INFO  time: compiled root in 1.35s[0m
[0m2021.03.08 16:07:51 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:07:52 INFO  time: compiled root in 1.54s[0m
Mar 08, 2021 4:08:12 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 11907
[0m2021.03.08 16:11:53 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:11:54 INFO  time: compiled root in 1.55s[0m
[0m2021.03.08 16:11:54 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:11:56 INFO  time: compiled root in 1.36s[0m
[0m2021.03.08 16:13:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:13:18 INFO  time: compiled root in 0.15s[0m
[0m2021.03.08 16:13:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:13:23 INFO  time: compiled root in 1.61s[0m
[0m2021.03.08 16:13:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:13:38 INFO  time: compiled root in 1.55s[0m
[0m2021.03.08 16:13:41 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:13:41 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 16:13:44 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:13:45 INFO  time: compiled root in 1.38s[0m
[0m2021.03.08 16:14:08 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:14:08 INFO  time: compiled root in 0.15s[0m
[0m2021.03.08 16:14:23 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:14:25 INFO  time: compiled root in 2.59s[0m
[0m2021.03.08 16:14:37 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:14:40 INFO  time: compiled root in 2.9s[0m
[0m2021.03.08 16:14:43 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:14:43 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 16:14:49 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:14:53 INFO  time: compiled root in 4.27s[0m
[0m2021.03.08 16:15:14 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:15:14 INFO  time: compiled root in 0.14s[0m
[0m2021.03.08 16:15:18 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:15:22 INFO  time: compiled root in 4s[0m
Mar 08, 2021 4:15:22 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12145
[0m2021.03.08 16:15:22 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:15:22 INFO  time: compiled root in 0.16s[0m
[0m2021.03.08 16:15:25 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:15:27 INFO  time: compiled root in 1.64s[0m
[0m2021.03.08 16:16:30 INFO  time: code lens generation in 3.43s[0m
Mar 08, 2021 4:16:30 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12173
Mar 08, 2021 4:16:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12176
Mar 08, 2021 4:16:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12177
Mar 08, 2021 4:16:33 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12179
Mar 08, 2021 4:16:35 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12174
Mar 08, 2021 4:16:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12182
[0m2021.03.08 16:16:38 INFO  time: code lens generation in 2.41s[0m
Mar 08, 2021 4:16:38 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12185
Mar 08, 2021 4:16:42 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12181
Mar 08, 2021 4:16:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12183
[0m2021.03.08 16:16:46 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:16:48 INFO  time: compiled root in 2.42s[0m
[0m2021.03.08 16:16:48 INFO  time: code lens generation in 6.17s[0m
[0m2021.03.08 16:16:54 INFO  time: code lens generation in 5.2s[0m
Mar 08, 2021 4:16:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12189
[0m2021.03.08 16:16:59 INFO  compiling root (1 scala source)[0m
[0m2021.03.08 16:16:59 INFO  time: compiled root in 1ms[0m
Mar 08, 2021 4:17:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12192
Mar 08, 2021 4:17:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12190
Mar 08, 2021 4:17:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12197
Mar 08, 2021 4:17:04 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12191
Mar 08, 2021 4:17:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12193
Mar 08, 2021 4:17:05 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12200
[0m2021.03.08 16:17:08 INFO  time: code lens generation in 4.8s[0m
Mar 08, 2021 4:17:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12201
Mar 08, 2021 4:17:24 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12204
Exception in thread "pool-1-thread-679" java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.runtime.IntRef.create(IntRef.java:23)
	at scala.meta.internal.tokenizers.ScalametaTokenizer.loop$1(ScalametaTokenizer.scala:185)
	at scala.meta.internal.tokenizers.ScalametaTokenizer.uncachedTokenize(ScalametaTokenizer.scala:318)
	at scala.meta.internal.tokenizers.ScalametaTokenizer.$anonfun$tokenize$1(ScalametaTokenizer.scala:17)
	at scala.meta.internal.tokenizers.ScalametaTokenizer$$Lambda$470/957455889.apply(Unknown Source)
	at scala.collection.concurrent.TrieMap.getOrElseUpdate(TrieMap.scala:895)
	at scala.meta.internal.tokenizers.ScalametaTokenizer.tokenize(ScalametaTokenizer.scala:17)
	at scala.meta.internal.tokenizers.ScalametaTokenizer$$anon$2.apply(ScalametaTokenizer.scala:336)
	at scala.meta.tokenizers.Api$XtensionTokenizeDialectInput.tokenize(Api.scala:25)
	at scala.meta.tokenizers.Api$XtensionTokenizeInputLike.tokenize(Api.scala:14)
	at scala.meta.internal.parsers.ScalametaParser.scannerTokens$lzycompute(ScalametaParser.scala:239)
	at scala.meta.internal.parsers.ScalametaParser.scannerTokens(ScalametaParser.scala:239)
	at scala.meta.internal.parsers.ScalametaParser.<init>(ScalametaParser.scala:154)
	at scala.meta.internal.parsers.ScalametaParser$$anon$257.apply(ScalametaParser.scala:5039)
	at scala.meta.parsers.Api$XtensionParseDialectInput.parse(Api.scala:25)
	at scala.meta.internal.parsing.Trees.$anonfun$parse$2(Trees.scala:80)
	at scala.meta.internal.parsing.Trees$$Lambda$479/847930947.apply(Unknown Source)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.parsing.Trees.parse(Trees.scala:77)
	at scala.meta.internal.parsing.Trees.didChange(Trees.scala:49)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$parseTreesAndPublishDiags$2(MetalsLanguageServer.scala:271)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$parseTreesAndPublishDiags$2$adapted(MetalsLanguageServer.scala:270)
	at scala.meta.internal.metals.MetalsLanguageServer$$Lambda$3951/1477516830.apply(Unknown Source)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.Future$$Lambda$116/1096568925.apply(Unknown Source)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.Promise$$Lambda$117/1266200593.apply(Unknown Source)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
[0m2021.03.08 16:17:40 INFO  time: code lens generation in 31s[0m
Mar 08, 2021 4:17:40 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleNotification
WARNING: Notification threw an exception: {
  "jsonrpc": "2.0",
  "method": "textDocument/didChange",
  "params": {
    "textDocument": {
      "version": 3565,
      "uri": "file:///home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala"
    },
    "contentChanges": [
      {
        "text": "package `com.revature.scala`\n\nimport org.apache.log4j._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.SaveMode\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types.{\n  StructType,\n  StructField,\n  BooleanType,\n  StringType\n}\nimport org.apache.spark.sql.Dataset\n\nobject GetS3Data {\n\n  def main(args: Array[String]): Unit \u003d {\n\n    val spark \u003d SparkSession\n      .builder()\n      .appName(\"Get S3 Data\")\n      //.config(\"spark.master\", \"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"src/main/recources/warehouse\")\n      .getOrCreate()\n\n    Logger.getLogger(\"org\").setLevel(Level.WARN)\n\n    val key \u003d System.getenv((\"AWS_ACCESS_KEY\"))\n    val secret \u003d System.getenv((\"AWS_SECRET_KEY\"))\n\n    spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.access.key\", key)\n    spark.sparkContext.hadoopConfiguration.set(\"fs.s3a.secret.key\", secret)\n\n    //rddParser(spark)\n    dfParser(spark)\n    //urlIndex(spark)\n    //jobExample(spark)\n\n    spark.close()\n  }\n\n  def rddParser(spark: SparkSession): Unit \u003d {\n\n    val rdd \u003d spark.sparkContext.textFile(\n      \"s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz\"\n    )\n\n    //rdd.take(2000).foreach(println)\n\n    val flatRDD \u003d\n      rdd.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_ + _).filter(_._2 \u003e 5)\n\n    flatRDD.take(2000).foreach(println)\n\n    // rdd\n    // .filter(line \u003d\u003e line.contains(\"a href\") \u0026\u0026 (line.contains(\"/job\") || line.contains(\"/jobs\") || line.contains(\"/job-listing\")))\n    // .take(200)\n    // .distinct\n    // .foreach(println)\n\n    // val parsedRDD \u003d rdd\n    //   .flatMap(line \u003d\u003e\n    //     line.split(\"\"\"\\s+\"\"\") match {\n    //       case Array(href, _) \u003d\u003e Some(href)\n    //     }\n    //   )\n  }\n\n  def dfParser(spark: SparkSession): Unit \u003d {\n\n    import spark.implicits._\n    // val df \u003d spark.read\n    //   .format(\"json\")\n    //   .options(\n    //     Map(\n    //       \"compression\" -\u003e \"gzip\",\n    //       \"inferSchema\" -\u003e \"true\",\n    //       \"mode\" -\u003e \"dropMalformed\",\n    //       \"lineSep\" -\u003e \"\"\"\\r\\n\\r\\n\"\"\",\n    //       \"path\" -\u003e \"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz\"\n    //     )\n    //   )\n    //   .load()\n\n    val jobsRegex \u003d List(\n      \"jobs\",\n      \"job-listing\",\n      \"job-posting\",\n      \"indeed.com/\",\n      \"careers\",\n      \"glassdoor.com/\",\n      \"/employment\"\n    )\n\n    val techJobs \u003d List(\n      \"technology\",\n      \"comput\",\n      \"java\",\n      \"python\",\n      \"scala\",\n      \"code\",\n      \"coding\",\n      \"programming\",\n      \"backend\",\n      \"frontend\",\n      \"web-development\",\n      \"website-development\",\n      \"ruby\",\n      \"sql\",\n      \"html\",\n      \"fullstack\",\n      \"full-stack\",\n      \"css\",\n      \"software\",\n      \"cybersecurity\",\n      \"cryptography\",\n      \"it-support\",\n      \"it-specialist\",\n      \"spark\",\n      \"hive\",\n      \"hql\",\n      \"hadoop\",\n      \"mapreduce\",\n      \"hdfs\",\n      \"c#\",\n      \"sdk\",\n      \"aws\",\n      \"data\",\n      \"apache\",\n      \"kafka\",\n      \"mongo\",\n      \"c#\",\n      \"programmer\",\n      \"analytics\"\n    )\n\n    val questionFilter \u003d \"low cod|no cod|low-cod\"\n\n    val commonCrawl \u003d spark.read\n      .option(\"lineSep\", \"WARC/1.0\")\n      .textFile(\n        \"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00121.warc.wet.gz\"\n      )\n      .map(str \u003d\u003e str.substring(str.indexOf(\"\\n\") + 1))\n      .withColumn(\"Header\", split($\"value\", \"\\r\\n\\r\\n\").getItem(0))\n      .withColumn(\"Content\", split($\"value\", \"\\r\\n\\r\\n\").getItem(1))\n      .drop(\"value\")\n\n    val englishJobSites \u003d commonCrawl\n      .filter(\n        lower($\"Header\") rlike \".*WARC-Target-URI:.*careers.*\"\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*job-listing.*\"))\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*jobs.*\"))\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*employment.*\"))\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*indeed/.*\"))\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*job-posting.*\"))\n          or (lower($\"Header\" rlike \".*WARC-Target-URI:.*glassdoor/.*\"))\n          and (lower($\"Header\" contains \"WARC-Identified-Content-Language: eng\" and !($\"Header\" contains \",\")))\n      )\n\n    val techJobSites \u003d englishJobSites\n    .filter(\n      lower($\"Header\") rlike \".*WARC-Target-URI:.*jdk.*\"\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*technology.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*comput.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*java.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*python.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*scala.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*code.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*coding.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*programming.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*backend.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*frontend.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*webdevelopment.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*web-development.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*websitedevelopment.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*website-development.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*ruby.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*sql.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*html.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*fullstack.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*full-stack.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*css.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*software.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*cyber.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*crypto.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*itsupport.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*it-support.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*itspecialist.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*it-specialist.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*spark.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*hive.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*hql.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*hadoop.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*apache.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*c#*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*mapreduce.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*hdfs.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*kafka.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*cassandra.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*mongo.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*programmer.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*programming.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*aws.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*athena.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*emr.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*s3.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*cloud.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*analytics.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*sdk.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*jvm.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*jre.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*byte.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*visual-studio/.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*eclipse.*\"))\n        or (lower($\"Header\" rlike \".*WARC-Target-URI:.*intellij.*\"))\n    )\n\n    techJobSites.show(5, false)\n\n    val lowCodeJobs \u003d techJobSites\n      .filter($\"Content\" rlike questionFilter)\n\n    //val techJobCount \u003d println(s\"The total number of computer tech jobs in the January 2021 Common Crawl is: $commonCrawlTechJobs\")\n\n    val warcSchema \u003d StructType(\n      Array(\n        StructField(\n          \"Container\",\n          StructType(\n            Array(\n              StructField(\"Compressed\", BooleanType, nullable \u003d true),\n              StructField(\"Filename\", StringType, nullable \u003d true),\n              StructField(\n                \"Gzip-Metadata\",\n                StructType(\n                  Array(\n                    StructField(\"Deflate-Length\", StringType, nullable \u003d true),\n                    StructField(\"Footer-Length\", StringType, nullable \u003d true),\n                    StructField(\"Header-Length\", StringType, nullable \u003d true),\n                    StructField(\"Inflated-CRC\", StringType, nullable \u003d true),\n                    StructField(\"Inflated-Length\", StringType, nullable \u003d true)\n                  )\n                )\n              )\n            )\n          )\n        )\n      )\n    )\n\n    val testSchema \u003d StructType(\n      Array(StructField(\"WARC-Target-URI\", StringType, nullable \u003d true))\n    )\n\n    case class WARC(\n        Compressed: Boolean\n    )\n\n    // val df \u003d spark.read\n    //   .format(\"text\")\n    //   .options(\n    //     Map(\n    //       \"compression\" -\u003e \"gzip\",\n    //       \"mode\" -\u003e \"dropMalformed\",\n    //       \"multiline\" -\u003e \"true\",\n    //       \"encoding\" -\u003e \"UTF-16LE\",\n    //       \"path\" -\u003e \"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz\",\n    //       \"inferSchema\" -\u003e \"true\"\n    //     )\n    //   )\n    //   .load()\n\n    // val splitDF \u003d\n    //   df.select(split($\"value\", \"\u003c/html\u003e\").as(\"Websites\")).drop(\"value\")\n\n    // val dropDFs \u003d df\n    //   .drop(\n    //     ($\"Container\") and ($\"Envelope.Format\") and ($\"Envelope.Payload-Metadata.Actual-Content-Length\")\n    //     and ($\"Envelope.Payload-Metadata.Actual-Content-Type\") and ($\"Envelope.Payload-Metadata.Block-Digest\")\n    //     and ($\"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest\") and ($\"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length\")\n    //     and ($\"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length\")\n    //   )\n    //   .filter(\n    //     $\"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language\" contains \"en-US\"\n    //   )\n\n    // val parsedDF \u003d df\n    //   .select(\"Envelope.WARC-Header-Metadata\")\n    //   .filter(($\"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language\" contains \"en-US\")\n    //   and ($\"Envelope.WARC-Header-Metadata.WARC-Target-URI\" contains \"/job-listing\"))\n\n    // parsedDF.show(200, false)\n\n  }\n\n  def urlIndex(spark: SparkSession): Unit \u003d {\n\n    import spark.implicits._\n    val df \u003d spark.read\n      .format(\"parquet\")\n      .options(\n        Map(\n          \"compression\" -\u003e \"gzip\",\n          \"mode\" -\u003e \"dropMalformed\",\n          \"inferSchema\" -\u003e \"true\",\n          \"path\" -\u003e \"s3a://commoncrawl/cc-index/table/cc-main/warc/crawl\u003dCC-MAIN-2021-04/subset\u003dwarc/\"\n        )\n      )\n      .load()\n\n    val jobsRegex \u003d \"/jobs|/job-listing|/job-posting\"\n\n    val techJobs \u003d List(\n      \"/technology\",\n      \"/computer\",\n      \"/java\",\n      \"/python\",\n      \"/scala\",\n      \"/code\",\n      \"/coding\",\n      \"/programming\",\n      \"/backend\",\n      \"/frontend\",\n      \"/web-development\",\n      \"/website-development\",\n      \"/ruby\",\n      \"/sql\",\n      \"/html\",\n      \"/fullstack\",\n      \"/full-stack\",\n      \"/css\",\n      \"/software\",\n      \"/cybersecurity\",\n      \"/cryptography\",\n      \"/it-support\",\n      \"/it-specialist\",\n      \"/spark\",\n      \"/hive\",\n      \"/hql\",\n      \"/hadoop\",\n      \"/mapreduce\",\n      \"/hdfs\",\n      \"/c#\",\n      \"/sdk\",\n      \"/aws\",\n      \"/computing\",\n      \"/data\",\n      \"/apache\",\n      \"/kafka\",\n      \"/mongo\"\n    )\n\n    val jobSiteIndex \u003d df\n      .filter(\n        ($\"content_languages\" \u003d\u003d\u003d \"eng\") and ($\"content_charset\" \u003d\u003d\u003d \"UTF-8\") and (($\"url_path\"\n          .rlike(jobsRegex)) and ($\"url_path\".rlike(techJobs.mkString(\"|\"))))\n      )\n      .select(\n        $\"url\",\n        $\"warc_filename\",\n        $\"warc_record_offset\",\n        $\"warc_record_length\"\n      )\n\n    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)\n\n    jobSiteIndex\n      .coalesce(1)\n      .write\n      .format(\"csv\")\n      .option(\"header\", \"true\")\n      .mode(SaveMode.Append)\n      .save(\"TechJobsiteIndex\")\n\n    // val df \u003d spark.read\n    //   .format(\"json\")\n    //   .options(\n    //     Map(\n    //       \"header\" -\u003e \"false\",\n    //       \"mode\" -\u003e \"dropMalformed\",\n    //       \"inferSchema\" -\u003e \"true\",\n    //       \"path\" -\u003e \"s3a://commoncrawl/cc-index/table/cc-main/warc/\"\n    //     )\n    //   )\n    //   .load()\n\n    // df.select()\n\n    //   val techJobsDF \u003d df\n    //     .filter(\n    //       $\"_c0\" contains \"/jobs\" and $\"_c0\" contains\n    //       \"tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix\"\n    //     )\n    //     .withColumnRenamed(\"_c0\", \"URI\")\n    //     .withColumnRenamed(\"_c1\", \"Path\")\n  }\n\n  def jobExample(spark: SparkSession): Unit \u003d {\n\n    import spark.implicits._\n    // val df \u003d spark.read\n    //   .format(\"parquet\")\n    //   .options(\n    //     Map(\n    //       \"mode\" -\u003e \"dropMalformed\",\n    //       \"inferSchema\" -\u003e \"true\",\n    //       \"compression\" -\u003e \"gzip\",\n    //       \"path\" -\u003e \"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz\"\n    //     )\n    //   )\n    //   .load()\n\n    // val exampleFormat \u003d df\n    //   .filter(\n    //     ($\"url_path\" contains \"jobs\") and ($\"content_languages\" \u003d\u003d\u003d \"eng\")\n    //   )\n    //   .select($\"url_host_name\", $\"url_path\" as \"sample_path\")\n    //   .groupBy(\"url_host_name\", \"sample_path\")\n    //   .count()\n    //   .orderBy($\"count\" desc)\n    //   .as(\"n\")\n\n    // exampleFormat.show(200, false)\n\n    val rdd \u003d spark.sparkContext\n      .textFile(\"s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz\")\n      .coalesce(1, true)\n      .saveAsTextFile(\"WETfiles\")\n\n  }\n}\n\n/** val regexSting \u003d \"volkswagen|vw\"\n  * val vwDF \u003d carsDF.select(\n  *   col(\"Name\"),\n  *   regexp_extract(col(\"Name\"), regexString, 0).as(\"regex_extract\")\n  * ).where(col(\"regex_extract\") \u003d!\u003d \"\").drop(\"regex_extract\")\n  *\n  * vwDF.select(\n  *   col(\"Name\"),\n  *   regexp_replace(col(\"Name\"), regexString, \"People\u0027s Car\").as(\"regex_replace\")\n  * .show())\n  */\n"
      }
    ]
  }
}
java.lang.RuntimeException: java.lang.reflect.InvocationTargetException
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:67)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.notify(GenericEndpoint.java:152)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.handleNotification(RemoteEndpoint.java:220)
	at org.eclipse.lsp4j.jsonrpc.RemoteEndpoint.consume(RemoteEndpoint.java:187)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.handleMessage(StreamMessageProducer.java:194)
	at org.eclipse.lsp4j.jsonrpc.json.StreamMessageProducer.listen(StreamMessageProducer.java:94)
	at org.eclipse.lsp4j.jsonrpc.json.ConcurrentMessageProcessor.run(ConcurrentMessageProcessor.java:113)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	... 11 more
Caused by: java.lang.OutOfMemoryError: GC overhead limit exceeded
	at scala.meta.tokens.Token$LF.pos(Token.scala:138)
	at scala.meta.internal.parsing.TokenEditDistance$TokenEqualizer$.equals(TokenEditDistance.scala:358)
	at scala.meta.internal.parsing.TokenEditDistance$.loop$1(TokenEditDistance.scala:293)
	at scala.meta.internal.parsing.TokenEditDistance$.fromTokens(TokenEditDistance.scala:319)
	at scala.meta.internal.parsing.TokenEditDistance$.$anonfun$apply$2(TokenEditDistance.scala:346)
	at scala.meta.internal.parsing.TokenEditDistance$$$Lambda$3957/1398028445.apply(Unknown Source)
	at scala.Option.map(Option.scala:230)
	at scala.meta.internal.parsing.TokenEditDistance$.$anonfun$apply$1(TokenEditDistance.scala:340)
	at scala.meta.internal.parsing.TokenEditDistance$$$Lambda$3956/1729142312.apply(Unknown Source)
	at scala.Option.flatMap(Option.scala:271)
	at scala.meta.internal.parsing.TokenEditDistance$.apply(TokenEditDistance.scala:339)
	at scala.meta.internal.metals.Diagnostics.publishDiagnostics(Diagnostics.scala:180)
	at scala.meta.internal.metals.Diagnostics.publishDiagnostics(Diagnostics.scala:163)
	at scala.meta.internal.metals.Diagnostics.didChange(Diagnostics.scala:111)
	at scala.meta.internal.metals.MetalsLanguageServer.didChange(MetalsLanguageServer.scala:1047)
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint.lambda$null$0(GenericEndpoint.java:65)
	at org.eclipse.lsp4j.jsonrpc.services.GenericEndpoint$$Lambda$102/594427726.apply(Unknown Source)
	... 11 more

Mar 08, 2021 4:17:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12203
Mar 08, 2021 4:17:43 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12205
Mar 08, 2021 4:17:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12209
Mar 08, 2021 4:17:49 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12212
[0m2021.03.08 16:17:57 INFO  time: code lens generation in 14s[0m
Mar 08, 2021 4:17:58 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12213
Mar 08, 2021 4:17:59 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12216
Exception in thread "pool-1-thread-676" java.lang.OutOfMemoryError: GC overhead limit exceeded
[0mMar 08, 2021 4:18:10 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12217
2021.03.08 16:18:10 INFO  time: code lens generation in 13s[0m
Exception in thread "pool-5-thread-1" java.lang.OutOfMemoryError: GC overhead limit exceeded
Mar 08, 2021 4:18:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12221
[0m2021.03.08 16:18:14 INFO  time: code lens generation in 8.01s[0m
Mar 08, 2021 4:18:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12225
Mar 08, 2021 4:18:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12224
Mar 08, 2021 4:18:14 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12233
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12236
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12237
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12234
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12248
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12250
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12252
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12253
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12254
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12256
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12258
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12260
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12262
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12264
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12265
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12266
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12268
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12269
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12270
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12272
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12278
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12279
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12273
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12274
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12283
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12288
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12282
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12292
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12284
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12296
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12300
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12295
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12297
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12303
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12307
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12302
Mar 08, 2021 4:18:15 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12304
Mar 08, 2021 4:26:13 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12636
Mar 08, 2021 4:26:52 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12656
Mar 08, 2021 4:30:03 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12787
Mar 08, 2021 4:30:18 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12794
Mar 08, 2021 4:30:44 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12800
Mar 08, 2021 4:31:09 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 12818
Mar 08, 2021 4:37:20 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 13006
Mar 08, 2021 6:02:30 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$retryWithCleanCompiler
INFO: compiler crashed due to an error in the Scala compiler, retrying with new compiler instance.
Mar 08, 2021 6:02:30 PM scala.meta.internal.pc.CompilerAccess scala$meta$internal$pc$CompilerAccess$$handleError
SEVERE: assertion failed: bad position: [8590:8575]
java.lang.AssertionError: assertion failed: bad position: [8590:8575]
	at scala.reflect.internal.util.Position$.validate(Position.scala:34)
	at scala.reflect.internal.util.Position$.range(Position.scala:51)
	at scala.reflect.internal.util.InternalPositionImpl$class.copyRange(Position.scala:205)
	at scala.reflect.internal.util.InternalPositionImpl$class.withStart(Position.scala:124)
	at scala.reflect.internal.util.Position.withStart(Position.scala:12)
	at scala.meta.internal.pc.CompletionProvider.editRange$lzycompute$1(CompletionProvider.scala:366)
	at scala.meta.internal.pc.CompletionProvider.editRange$2(CompletionProvider.scala:365)
	at scala.meta.internal.pc.CompletionProvider.safeCompletionsAt(CompletionProvider.scala:444)
	at scala.meta.internal.pc.CompletionProvider.completions(CompletionProvider.scala:57)

Mar 08, 2021 8:12:08 PM org.eclipse.lsp4j.jsonrpc.RemoteEndpoint handleCancellation
WARNING: Unmatched cancel notification for request id 15107
Exception in thread "pool-5-thread-2" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
Exception in thread "pool-5-thread-3" java.lang.Error: java.lang.InterruptedException
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1155)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.lang.InterruptedException
	at scala.meta.internal.metals.FutureCancelToken.checkCanceled(FutureCancelToken.scala:29)
	at scala.meta.internal.pc.CompilerAccess$$anonfun$onCompilerJobQueue$1.apply$mcV$sp(CompilerAccess.scala:195)
	at scala.meta.internal.pc.CompilerJobQueue$Job.run(CompilerJobQueue.scala:103)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	... 2 more
[0m2021.03.09 11:32:41 INFO  Started: Metals version 0.10.0 in workspace '/home/skyler/project3/s3data/s3dataget' for client vscode 1.54.1.[0m
[0m2021.03.09 11:32:41 INFO  time: initialize in 0.45s[0m
[0m2021.03.09 11:32:41 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher3931507395622351277/bsp.socket'...
Waiting for the bsp connection to come up...
[0m2021.03.09 11:32:41 WARN  no build target for: /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala/GetS3Data.scala[0m
[0m2021.03.09 11:32:42 INFO  skipping build import with status 'Installed'[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher3931507395622351277/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher3931507395622351277/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.09 11:32:42 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.09 11:32:42 INFO  Attempting to connect to the build server...[0m
[0m2021.03.09 11:32:42 INFO  Attempting to connect to the build server...[0m
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8406586681737574719/bsp.socket'...
Waiting for the bsp connection to come up...
Starting the bsp launcher for bloop...
Opening a bsp server connection with 'bsp --protocol local --socket /tmp/bsp-launcher8204300737486617273/bsp.socket'...
Waiting for the bsp connection to come up...
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8406586681737574719/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8406586681737574719/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.09 11:32:42 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m[32m[D][0m Loading workspace settings from bloop.settings.json
[0m[32m[D][0m Waiting for a connection at local:///tmp/bsp-launcher8204300737486617273/bsp.socket...
The server is listening for incoming connections at local:///tmp/bsp-launcher8204300737486617273/bsp.socket...
Starting thread that pumps stdin and redirects it to the bsp server...
Starting thread that pumps server stdout and redirects it to the client stdout...
[0m2021.03.09 11:32:42 INFO  tracing is disabled for protocol BSP, to enable tracing of incoming and outgoing JSON messages create an empty file at /home/skyler/.cache/metals/bsp.trace.json[0m
[0m2021.03.09 11:32:42 INFO  time: Connected to build server in 0.82s[0m
[0m2021.03.09 11:32:42 INFO  Connected to Build server: Bloop v1.4.8[0m
[0m2021.03.09 11:32:43 INFO  no build target: using presentation compiler with only scala-library: 2.12.13[0m
[0m2021.03.09 11:32:44 INFO  time: Imported build in 0.15s[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}
import org.apache.spark.sql.Dataset

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = List(
      "jobs",
      "job-listing",
      "job-posting",
      "indeed.com/",
      "careers",
      "glassdoor.com/",
      "/employment"
    )

    val techJobs = List(
      "technology",
      "comput",
      "java",
      "python",
      "scala",
      "code",
      "coding",
      "programming",
      "backend",
      "frontend",
      "web-development",
      "website-development",
      "ruby",
      "sql",
      "html",
      "fullstack",
      "full-stack",
      "css",
      "software",
      "cybersecurity",
      "cryptography",
      "it-support",
      "it-specialist",
      "spark",
      "hive",
      "hql",
      "hadoop",
      "mapreduce",
      "hdfs",
      "c#",
      "sdk",
      "aws",
      "data",
      "apache",
      "kafka",
      "mongo",
      "c#",
      "programmer",
      "analytics"
    )

    val questionFilter = "low cod|no cod|low-cod|no-cod"

    lazy val commonCrawl = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00182.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .withColumn("Header", split($"value", "\r\n\r\n").getItem(0))
      .withColumn("Content", split($"value", "\r\n\r\n").getItem(1))
      .drop("value")
      .repartition(20)

    lazy val englishJobSites = commonCrawl
      .filter(
        $"Header" rlike ".*WARC-Target-URI:.*careers.*"
          or ($"Header" rlike ".*WARC-Target-URI:.*job-listing.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*jobs.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*employment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*indeed/.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*job-posting.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*glassdoor/.*")
          and ($"Header" contains "WARC-Identified-Content-Language: eng" and !($"Header" contains ","))
      )
      .repartition(6)
      .cache()

    lazy val techJobSites = englishJobSites
      .filter(
        $"Header" rlike ".*WARC-Target-URI:.*/jdk.*"
          or ($"Header" rlike ".*WARC-Target-URI:.*/technology.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/comput.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/java.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/python.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/scala.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/code.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/coding.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programming.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/backend.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/frontend.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/webdevelopment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/web-development.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/websitedevelopment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/website-development.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/ruby.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/sql.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/html.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/fullstack.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/full-stack.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/css.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/software.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cyber.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/crypto.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/itsupport.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/it-support.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/itspecialist.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/it-specialist.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/spark.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hive.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hql.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hadoop.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/apache.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/mapreduce.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hdfs.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/kafka.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cassandra.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/mongo.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programmer.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programming.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/aws.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/athena.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/emr.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/s3.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cloud.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/analytics.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/sdk.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/jvm.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/jre.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/byte.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/visual-studio.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/eclipse.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/intellij.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/visualstudio.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/vsc.*")
      )
      .repartition(2)
      .cache()

    techJobSites.show(5, false)

    lazy val lowCodeJobs = techJobSites
      .filter(lower($"Content") rlike questionFilter)
      .repartition(2)
      .cache()

    lazy val jobCount = englishJobSites.count.toDouble

    println(s"The total number of job related websites in the Common Crawl database is: $jobCount")

    lazy val techCount = techJobSites.count.toDouble

    println(s"The total number of tech related websites in the Common Crawl database is: $techCount")

    lazy val lowCodeCount = lowCodeJobs.count.toDouble

    println(f"The total number of low code websites in the Common Crawl database is: $lowCodeCount")

    lazy val techJobPercent = techCount / jobCount * 100

    println(f"The percentage of tech jobs to total jobs in the Common Crawl database is: $techJobPercent%.4f%%")

    lazy val lowCodePercent = lowCodeCount / techCount * 100

    println(f"The percentage of low code jobs to tech jobs is: $lowCodePercent%.4f%%")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext
      .textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz")
      .coalesce(1, true)
      .saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.09 11:32:47 INFO  no build target: using presentation compiler with only scala-library: 2.11.12[0m
[0m2021.03.09 11:32:49 ERROR /home/skyler/project3/s3data/s3dataget/src/main/scala/com.revature.scala
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.mtags.CommonMtagsEnrichments$XtensionAbsolutePath.toInput(CommonMtagsEnrichments.scala:411)
	at scala.meta.internal.metals.MetalsLanguageServer.indexSourceFile(MetalsLanguageServer.scala:2090)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4(MetalsLanguageServer.scala:2057)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$4$adapted(MetalsLanguageServer.scala:2051)
	at geny.Generator.$anonfun$foreach$1(Generator.scala:50)
	at geny.Generator$Filtered.$anonfun$generate$3(Generator.scala:276)
	at geny.Generator$Mapped.$anonfun$generate$4(Generator.scala:283)
	at geny.Generator$SelfClosing.generate(Generator.scala:229)
	at geny.Generator$Mapped.generate(Generator.scala:283)
	at geny.Generator$Filtered.generate(Generator.scala:276)
	at geny.Generator.foreach(Generator.scala:49)
	at geny.Generator.foreach$(Generator.scala:49)
	at geny.Generator$Filtered.foreach(Generator.scala:274)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2(MetalsLanguageServer.scala:2051)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspaceSources$2$adapted(MetalsLanguageServer.scala:2050)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspaceSources(MetalsLanguageServer.scala:2050)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$indexWorkspace$9(MetalsLanguageServer.scala:2228)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.indexWorkspace(MetalsLanguageServer.scala:2228)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$2(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.meta.internal.metals.TimerProvider.timedThunk(TimerProvider.scala:25)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$profiledIndexWorkspace$1(MetalsLanguageServer.scala:2128)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)
	at scala.util.Success.$anonfun$map$1(Try.scala:255)
	at scala.util.Success.map(Try.scala:213)
	at scala.concurrent.Future.$anonfun$map$1(Future.scala:292)
	at scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)
	at scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m
[0m2021.03.09 11:32:49 WARN  Could not find java sources in Some(/usr/lib/jvm/java-8-openjdk-amd64). Java symbols will not be available.[0m
package `com.revature.scala`

import org.apache.log4j._
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.SaveMode
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{
  StructType,
  StructField,
  BooleanType,
  StringType
}
import org.apache.spark.sql.Dataset

object GetS3Data {

  def main(args: Array[String]): Unit = {

    val spark = SparkSession
      .builder()
      .appName("Get S3 Data")
      //.config("spark.master", "local[*]")
      .config("spark.sql.warehouse.dir", "src/main/recources/warehouse")
      .getOrCreate()

    Logger.getLogger("org").setLevel(Level.WARN)

    val key = System.getenv(("AWS_ACCESS_KEY"))
    val secret = System.getenv(("AWS_SECRET_KEY"))

    spark.sparkContext.hadoopConfiguration.set("fs.s3a.access.key", key)
    spark.sparkContext.hadoopConfiguration.set("fs.s3a.secret.key", secret)

    //rddParser(spark)
    dfParser(spark)
    //urlIndex(spark)
    //jobExample(spark)

    spark.close()
  }

  def rddParser(spark: SparkSession): Unit = {

    val rdd = spark.sparkContext.textFile(
      "s3a://commoncrawl/crawl-data/CC-MAIN-2013-48/segments/1386163035819/warc/CC-MAIN-20131204131715-00000-ip-10-33-133-15.ec2.internal.warc.gz"
    )

    //rdd.take(2000).foreach(println)

    val flatRDD =
      rdd.flatMap(_.split(" ")).map((_, 1)).reduceByKey(_ + _).filter(_._2 > 5)

    flatRDD.take(2000).foreach(println)

    // rdd
    // .filter(line => line.contains("a href") && (line.contains("/job") || line.contains("/jobs") || line.contains("/job-listing")))
    // .take(200)
    // .distinct
    // .foreach(println)

    // val parsedRDD = rdd
    //   .flatMap(line =>
    //     line.split("""\s+""") match {
    //       case Array(href, _) => Some(href)
    //     }
    //   )
  }

  def dfParser(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "inferSchema" -> "true",
    //       "mode" -> "dropMalformed",
    //       "lineSep" -> """\r\n\r\n""",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610704847953.98/wet/CC-MAIN-20210128134124-20210128164124-00799.warc.wet.gz"
    //     )
    //   )
    //   .load()

    val jobsRegex = List(
      "jobs",
      "job-listing",
      "job-posting",
      "indeed.com/",
      "careers",
      "glassdoor.com/",
      "/employment"
    )

    val techJobs = List(
      "technology",
      "comput",
      "java",
      "python",
      "scala",
      "code",
      "coding",
      "programming",
      "backend",
      "frontend",
      "web-development",
      "website-development",
      "ruby",
      "sql",
      "html",
      "fullstack",
      "full-stack",
      "css",
      "software",
      "cybersecurity",
      "cryptography",
      "it-support",
      "it-specialist",
      "spark",
      "hive",
      "hql",
      "hadoop",
      "mapreduce",
      "hdfs",
      "c#",
      "sdk",
      "aws",
      "data",
      "apache",
      "kafka",
      "mongo",
      "c#",
      "programmer",
      "analytics"
    )

    val questionFilter = "low cod|no cod|low-cod|no-cod"

    lazy val commonCrawl = spark.read
      .option("lineSep", "WARC/1.0")
      .textFile(
        "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/wet/CC-MAIN-20210115134101-20210115164101-00182.warc.wet.gz"
      )
      .map(str => str.substring(str.indexOf("\n") + 1))
      .withColumn("Header", split($"value", "\r\n\r\n").getItem(0))
      .withColumn("Content", split($"value", "\r\n\r\n").getItem(1))
      .drop("value")
      .repartition(20)

    lazy val englishJobSites = commonCrawl
      .filter(
        $"Header" rlike ".*WARC-Target-URI:.*careers.*"
          or ($"Header" rlike ".*WARC-Target-URI:.*job-listing.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*jobs.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*employment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*indeed/.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*job-posting.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*glassdoor/.*")
          and ($"Header" contains "WARC-Identified-Content-Language: eng" and !($"Header" contains ","))
      )
      .repartition(6)
      .cache()

    lazy val techJobSites = englishJobSites
      .filter(
        $"Header" rlike ".*WARC-Target-URI:.*/jdk.*"
          or ($"Header" rlike ".*WARC-Target-URI:.*/technology.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/comput.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/java.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/python.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/scala.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/code.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/coding.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programming.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/backend.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/frontend.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/webdevelopment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/web-development.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/websitedevelopment.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/website-development.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/ruby.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/sql.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/html.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/fullstack.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/full-stack.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/css.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/software.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cyber.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/crypto.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/itsupport.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/it-support.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/itspecialist.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/it-specialist.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/spark.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hive.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hql.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hadoop.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/apache.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/mapreduce.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/hdfs.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/kafka.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cassandra.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/mongo.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programmer.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/programming.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/aws.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/athena.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/emr.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/s3.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/cloud.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/analytics.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/sdk.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/jvm.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/jre.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/byte.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/visual-studio.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/eclipse.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/intellij.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/visualstudio.*")
          or ($"Header" rlike ".*WARC-Target-URI:.*/vsc.*")
      )
      .repartition(2)
      .cache()

    techJobSites.show(5, false)

    lazy val lowCodeJobs = techJobSites
      .filter(lower($"Content") rlike questionFilter)
      .repartition(2)
      .cache()

    lazy val jobCount = englishJobSites.count.toDouble

    println(s"The total number of job related websites in the Common Crawl database is: $jobCount")

    lazy val techCount = techJobSites.count.toDouble

    println(s"The total number of tech related websites in the Common Crawl database is: $techCount")

    lazy val lowCodeCount = lowCodeJobs.count.toDouble

    println(f"The total number of low code websites in the Common Crawl database is: $lowCodeCount")

    lazy val techJobPercent = techCount / jobCount * 100

    println(f"The percentage of tech jobs to total jobs in the Common Crawl database is: $techJobPercent%.4f%%")

    lazy val lowCodePercent = lowCodeCount / techCount * 100

    println(f"The percentage of low code jobs to tech jobs is: $lowCodePercent%.4f%%")

    val warcSchema = StructType(
      Array(
        StructField(
          "Container",
          StructType(
            Array(
              StructField("Compressed", BooleanType, nullable = true),
              StructField("Filename", StringType, nullable = true),
              StructField(
                "Gzip-Metadata",
                StructType(
                  Array(
                    StructField("Deflate-Length", StringType, nullable = true),
                    StructField("Footer-Length", StringType, nullable = true),
                    StructField("Header-Length", StringType, nullable = true),
                    StructField("Inflated-CRC", StringType, nullable = true),
                    StructField("Inflated-Length", StringType, nullable = true)
                  )
                )
              )
            )
          )
        )
      )
    )

    val testSchema = StructType(
      Array(StructField("WARC-Target-URI", StringType, nullable = true))
    )

    case class WARC(
        Compressed: Boolean
    )

    // val df = spark.read
    //   .format("text")
    //   .options(
    //     Map(
    //       "compression" -> "gzip",
    //       "mode" -> "dropMalformed",
    //       "multiline" -> "true",
    //       "encoding" -> "UTF-16LE",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/segments/1610703495901.0/warc/CC-MAIN-20210115134101-20210115164101-00015.warc.gz",
    //       "inferSchema" -> "true"
    //     )
    //   )
    //   .load()

    // val splitDF =
    //   df.select(split($"value", "</html>").as("Websites")).drop("value")

    // val dropDFs = df
    //   .drop(
    //     ($"Container") and ($"Envelope.Format") and ($"Envelope.Payload-Metadata.Actual-Content-Length")
    //     and ($"Envelope.Payload-Metadata.Actual-Content-Type") and ($"Envelope.Payload-Metadata.Block-Digest")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Digest") and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Length")
    //     and ($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Entity-Trailing-Slop-Length")
    //   )
    //   .filter(
    //     $"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US"
    //   )

    // val parsedDF = df
    //   .select("Envelope.WARC-Header-Metadata")
    //   .filter(($"Envelope.Payload-Metadata.HTTP-Request-Metadata.Headers.Accept-Language" contains "en-US")
    //   and ($"Envelope.WARC-Header-Metadata.WARC-Target-URI" contains "/job-listing"))

    // parsedDF.show(200, false)

  }

  def urlIndex(spark: SparkSession): Unit = {

    import spark.implicits._
    val df = spark.read
      .format("parquet")
      .options(
        Map(
          "compression" -> "gzip",
          "mode" -> "dropMalformed",
          "inferSchema" -> "true",
          "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/crawl=CC-MAIN-2021-04/subset=warc/"
        )
      )
      .load()

    val jobsRegex = "/jobs|/job-listing|/job-posting"

    val techJobs = List(
      "/technology",
      "/computer",
      "/java",
      "/python",
      "/scala",
      "/code",
      "/coding",
      "/programming",
      "/backend",
      "/frontend",
      "/web-development",
      "/website-development",
      "/ruby",
      "/sql",
      "/html",
      "/fullstack",
      "/full-stack",
      "/css",
      "/software",
      "/cybersecurity",
      "/cryptography",
      "/it-support",
      "/it-specialist",
      "/spark",
      "/hive",
      "/hql",
      "/hadoop",
      "/mapreduce",
      "/hdfs",
      "/c#",
      "/sdk",
      "/aws",
      "/computing",
      "/data",
      "/apache",
      "/kafka",
      "/mongo"
    )

    val jobSiteIndex = df
      .filter(
        ($"content_languages" === "eng") and ($"content_charset" === "UTF-8") and (($"url_path"
          .rlike(jobsRegex)) and ($"url_path".rlike(techJobs.mkString("|"))))
      )
      .select(
        $"url",
        $"warc_filename",
        $"warc_record_offset",
        $"warc_record_length"
      )

    //jobSiteIndex.show(jobSiteIndex.count.toInt, false)

    jobSiteIndex
      .coalesce(1)
      .write
      .format("csv")
      .option("header", "true")
      .mode(SaveMode.Append)
      .save("TechJobsiteIndex")

    // val df = spark.read
    //   .format("json")
    //   .options(
    //     Map(
    //       "header" -> "false",
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "path" -> "s3a://commoncrawl/cc-index/table/cc-main/warc/"
    //     )
    //   )
    //   .load()

    // df.select()

    //   val techJobsDF = df
    //     .filter(
    //       $"_c0" contains "/jobs" and $"_c0" contains
    //       "tech|tech|computer|computer|cryptograpy|end|full|java|python|scala|spark|sql|C+|C#|unix"
    //     )
    //     .withColumnRenamed("_c0", "URI")
    //     .withColumnRenamed("_c1", "Path")
  }

  def jobExample(spark: SparkSession): Unit = {

    import spark.implicits._
    // val df = spark.read
    //   .format("parquet")
    //   .options(
    //     Map(
    //       "mode" -> "dropMalformed",
    //       "inferSchema" -> "true",
    //       "compression" -> "gzip",
    //       "path" -> "s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz"
    //     )
    //   )
    //   .load()

    // val exampleFormat = df
    //   .filter(
    //     ($"url_path" contains "jobs") and ($"content_languages" === "eng")
    //   )
    //   .select($"url_host_name", $"url_path" as "sample_path")
    //   .groupBy("url_host_name", "sample_path")
    //   .count()
    //   .orderBy($"count" desc)
    //   .as("n")

    // exampleFormat.show(200, false)

    val rdd = spark.sparkContext
      .textFile("s3a://commoncrawl/crawl-data/CC-MAIN-2021-04/wet.paths.gz")
      .coalesce(1, true)
      .saveAsTextFile("WETfiles")

  }
}

/** val regexSting = "volkswagen|vw"
  * val vwDF = carsDF.select(
  *   col("Name"),
  *   regexp_extract(col("Name"), regexString, 0).as("regex_extract")
  * ).where(col("regex_extract") =!= "").drop("regex_extract")
  *
  * vwDF.select(
  *   col("Name"),
  *   regexp_replace(col("Name"), regexString, "People's Car").as("regex_replace")
  * .show())
  */

[0m2021.03.09 11:32:50 INFO  time: indexed workspace in 6.09s[0m
[0m2021.03.09 11:32:50 INFO  compiling root (1 scala source)[0m
[0m2021.03.09 11:32:51 INFO  time: code lens generation in 9.09s[0m
java.io.IOException: Is a directory
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at scala.meta.internal.io.PlatformFileIO$.slurp(PlatformFileIO.scala:45)
	at scala.meta.internal.io.FileIO$.slurp(FileIO.scala:24)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1(MetalsLanguageServer.scala:1235)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$onChange$1$adapted(MetalsLanguageServer.scala:1234)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.meta.internal.metals.MetalsLanguageServer.onChange(MetalsLanguageServer.scala:1234)
	at scala.meta.internal.metals.MetalsLanguageServer.didChangeWatchedFiles(MetalsLanguageServer.scala:1213)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.MetalsLanguageServer.$anonfun$fileWatcher$1$adapted(MetalsLanguageServer.scala:181)
	at scala.meta.internal.metals.FileWatcher$$anon$1.onCreate(FileWatcher.scala:58)
	at com.swoval.files.CacheObservers.onCreate(CacheObservers.java:27)
	at com.swoval.files.FileCacheDirectoryTree$6.run(FileCacheDirectoryTree.java:510)
	at com.swoval.files.FileCacheDirectoryTree$4.run(FileCacheDirectoryTree.java:258)
	at com.swoval.files.Executor$PriorityRunnable.run(Executor.java:161)
	at com.swoval.files.Executor$ExecutorImpl$1.run(Executor.java:65)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
[0m2021.03.09 11:32:59 INFO  time: compiled root in 8.57s[0m
[0m2021.03.09 11:37:35 INFO  shutting down Metals[0m
[0m2021.03.09 11:37:36 INFO  Shut down connection with build server.[0m
No more data in the client stdin, exiting...
No more data in the client stdin, exiting...
